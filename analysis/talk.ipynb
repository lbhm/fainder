{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for the Talk\n",
    "\n",
    "**Note:** This notebook only contains copied and slightly adjusted code from the original analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import markers\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "from fainder.utils import load_input, configure_run\n",
    "from utils.plotting_defaults import autolabel_bars, parse_logs_wide, parse_logs_special\n",
    "\n",
    "configure_run(\"WARNING\")\n",
    "Path(\"talk\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(Path.home() / \".cache\" / \"matplotlib\" / \"tex.cache\")\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context=\"talk\",\n",
    "    style=\"ticks\",\n",
    "    palette=\"colorblind\",\n",
    "    color_codes=True,\n",
    "    font_scale=1.0,\n",
    "    rc={\n",
    "        \"axes.linewidth\": 1.5,  # 1.875\n",
    "        \"font.family\": \"Carlito\",  # free alternative to Calibri\n",
    "        # \"grid.linewidth\": 1.5,  # 1.5\n",
    "        # \"lines.linewidth\": 9,  # 9\n",
    "        # \"lines.markersize\": 12,  # 12\n",
    "        # \"patch.linewidth\": 1.5,  # 1.5\n",
    "        \"savefig.pad_inches\": 0.01,\n",
    "        \"savefig.transparent\": True,\n",
    "        \"svg.fonttype\": \"none\",\n",
    "        # \"xtick.major.pad\": 2.0,  # 3.5\n",
    "        \"xtick.major.size\": 10,  # 9\n",
    "        \"xtick.major.width\": 2.0,  # 1.875\n",
    "        # \"xtick.minor.pad\": 2.0,  # 3.4\n",
    "        \"xtick.minor.size\": 6,  # 6dd\n",
    "        \"xtick.minor.width\": 1.5,  # 1.5\n",
    "        # \"ytick.major.pad\": 2.0,  # 3.5\n",
    "        \"ytick.major.size\": 10,  # 9\n",
    "        \"ytick.major.width\": 2.0,  # 1.875\n",
    "        # \"ytick.minor.pad\": 2.0,  # 3.4\n",
    "        \"ytick.minor.size\": 6,  # 6\n",
    "        \"ytick.minor.width\": 1.5,  # 1.5\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_list = []\n",
    "for logfile in Path(\"../logs/runtime_benchmark/execution/\").iterdir():\n",
    "    config = logfile.stem.split(\"-\")\n",
    "    data = parse_logs_wide(logfile)\n",
    "\n",
    "    data[\"dataset\"] = config[0]\n",
    "    data[\"query_set\"] = config[1]\n",
    "    data[\"approach\"] = config[2]\n",
    "    data[\"execution\"] = config[3]\n",
    "\n",
    "    execution_list.append(data)\n",
    "\n",
    "execution = pd.DataFrame(\n",
    "    execution_list,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"query_set\",\n",
    "        \"approach\",\n",
    "        \"execution\",\n",
    "        \"query_collection_time\",\n",
    "    ],\n",
    ")\n",
    "execution = execution[\n",
    "    (execution[\"execution\"] == \"single\") | (execution[\"execution\"] == \"single_suppressed\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate([\"sportstables\", \"open_data_usa\", \"gittables\"]):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(2.4, 2.5), layout=\"constrained\")\n",
    "    data = (\n",
    "        execution[(execution[\"dataset\"] == dataset) & (execution[\"query_set\"] == \"collection\")]\n",
    "        .groupby([\"approach\", \"execution\"])\n",
    "        .agg({\"query_collection_time\": [\"mean\", \"std\"]})\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    ax.bar(\n",
    "        x=0,\n",
    "        height=data[3][0],\n",
    "        width=0.5,\n",
    "        # yerr=data[3][1],\n",
    "        color=sns.color_palette()[0],\n",
    "        edgecolor=\"black\",\n",
    "        label=r\"\\pscan{}\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        x=0.75,\n",
    "        height=data[0][0],\n",
    "        width=0.5,\n",
    "        # yerr=data[0][1],\n",
    "        color=sns.color_palette()[1],\n",
    "        edgecolor=\"black\",\n",
    "        label=r\"\\binsort{}\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        x=1.5,\n",
    "        height=data[4][0],\n",
    "        width=0.5,\n",
    "        # yerr=data[4][1],\n",
    "        color=sns.color_palette()[2],\n",
    "        edgecolor=\"black\",\n",
    "        hatch=\"////\",\n",
    "        label=r\"\\system{} w/ results\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        x=2.05,\n",
    "        height=data[5][0],\n",
    "        width=0.5,\n",
    "        # yerr=data[5][1],\n",
    "        color=sns.color_palette()[2],\n",
    "        edgecolor=\"black\",\n",
    "        hatch=\"oooo\",\n",
    "        label=r\"\\system{} w/o results\",\n",
    "    )\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yscale(\"log\")\n",
    "    if dataset == \"gittables\":\n",
    "        ax.set_ylim(top=ax.get_ylim()[1] * 2.25)\n",
    "    else:\n",
    "        ax.set_ylim(top=ax.get_ylim()[1] * 1.75)\n",
    "    autolabel_bars(ax, precision=3, decimal_precision=2)\n",
    "\n",
    "    plt.savefig(f\"talk/runtime_comparison_{dataset}.svg\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skyline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"sportstables\", \"open_data_usa\", \"gittables\"]\n",
    "queries = [\"low_selectivity\", \"mid_selectivity\", \"high_selectivity\"]\n",
    "\n",
    "baseline_list = []\n",
    "for dataset in datasets:\n",
    "    for query_set in queries:\n",
    "        for approach, metric_name in [\n",
    "            (\"pscan\", \"hist\"),\n",
    "            (\"ndist\", \"dist\"),\n",
    "            (\"binsort\", \"binsort\"),\n",
    "        ]:\n",
    "            acc_logs = load_input(\n",
    "                f\"../logs/accuracy_benchmark/baseline_comp/{dataset}-{approach}-{query_set}.zst\"\n",
    "            )\n",
    "            perf_logs = parse_logs_wide(\n",
    "                f\"../logs/accuracy_benchmark/baseline_comp/{dataset}-{approach}-{query_set}.log\"\n",
    "            )\n",
    "            baseline_list.append(\n",
    "                [\n",
    "                    dataset,\n",
    "                    approach,\n",
    "                    query_set,\n",
    "                    perf_logs[\"query_collection_time\"],\n",
    "                    acc_logs[f\"{metric_name}_time\"],\n",
    "                    np.mean(acc_logs[f\"{metric_name}_metrics\"][0]),\n",
    "                    np.mean(acc_logs[f\"{metric_name}_metrics\"][1]),\n",
    "                    np.mean(acc_logs[f\"{metric_name}_metrics\"][2]),\n",
    "                    np.mean(acc_logs[f\"{metric_name}_metrics\"][3]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Fainder rebinning and conversion\n",
    "        for i, index_mode in enumerate([\"rebinning\", \"conversion\"]):\n",
    "            acc_logs = load_input(\n",
    "                f\"../logs/accuracy_benchmark/baseline_comp/{dataset}-{index_mode}-{query_set}.zst\"\n",
    "            )\n",
    "            runtimes = parse_logs_special(\n",
    "                f\"../logs/accuracy_benchmark/baseline_comp/{dataset}-{index_mode}-{query_set}.log\"\n",
    "            )\n",
    "            baseline_list.append(\n",
    "                [\n",
    "                    dataset,\n",
    "                    f\"{index_mode}-precision\",\n",
    "                    query_set,\n",
    "                    runtimes[0],\n",
    "                    acc_logs[\"precision_mode_time\"],\n",
    "                    np.mean(acc_logs[\"precision_mode_metrics\"][0]),\n",
    "                    np.mean(acc_logs[\"precision_mode_metrics\"][1]),\n",
    "                    np.mean(acc_logs[\"precision_mode_metrics\"][2]),\n",
    "                    np.mean(acc_logs[\"precision_mode_metrics\"][3]),\n",
    "                ]\n",
    "            )\n",
    "            baseline_list.append(\n",
    "                [\n",
    "                    dataset,\n",
    "                    f\"{index_mode}-recall\",\n",
    "                    query_set,\n",
    "                    runtimes[1],\n",
    "                    acc_logs[\"recall_mode_time\"],\n",
    "                    np.mean(acc_logs[\"recall_mode_metrics\"][0]),\n",
    "                    np.mean(acc_logs[\"recall_mode_metrics\"][1]),\n",
    "                    np.mean(acc_logs[\"recall_mode_metrics\"][2]),\n",
    "                    np.mean(acc_logs[\"recall_mode_metrics\"][3]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Fainder exact\n",
    "        acc_logs = load_input(\n",
    "            f\"../logs/accuracy_benchmark/baseline_comp/{dataset}-exact-{query_set}.zst\"\n",
    "        )\n",
    "        baseline_list.append(\n",
    "            [\n",
    "                dataset,\n",
    "                \"exact\",\n",
    "                query_set,\n",
    "                acc_logs[\"precision_time\"] + acc_logs[\"recall_time\"] + acc_logs[\"iterative_time\"],\n",
    "                acc_logs[\"precision_time\"] + acc_logs[\"recall_time\"] + acc_logs[\"iterative_time\"],\n",
    "                1,  # Metrics not logged because approach is exact\n",
    "                1,\n",
    "                1,\n",
    "                None,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "baseline_comp = pd.DataFrame(\n",
    "    baseline_list,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"approach\",\n",
    "        \"queries\",\n",
    "        \"precise_time\",\n",
    "        \"total_time\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f1\",\n",
    "        \"pruning_factor\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_scatter(dataset: str, metric: str, queries: str) -> None:\n",
    "    _, ax = plt.subplots(1, 1, figsize=(2.7, 2.8), layout=\"constrained\")\n",
    "\n",
    "    if queries == \"all\":\n",
    "        data = baseline_comp[\n",
    "            (baseline_comp[\"dataset\"] == dataset)\n",
    "            & ~baseline_comp[\"approach\"].isin([\"conversion-precision\", \"rebinning-precision\"])\n",
    "        ]\n",
    "    else:\n",
    "        data = baseline_comp[\n",
    "            (baseline_comp[\"dataset\"] == dataset)\n",
    "            & (baseline_comp[\"queries\"] == queries)\n",
    "            & ~baseline_comp[\"approach\"].isin([\"conversion-precision\", \"rebinning-precision\"])\n",
    "        ]\n",
    "\n",
    "    data = (\n",
    "        data.groupby([\"approach\"])\n",
    "        .agg({\"precise_time\": \"mean\", metric: \"mean\"})\n",
    "        .reindex([\"pscan\", \"ndist\", \"binsort\", \"exact\", \"rebinning-recall\", \"conversion-recall\"])\n",
    "    )\n",
    "\n",
    "    if dataset == \"gittables\":\n",
    "        ax.scatter(\n",
    "            data[\"precise_time\"][:5],\n",
    "            data[metric][:5] * 100,\n",
    "            c=sns.color_palette()[:5],\n",
    "            clip_on=False,\n",
    "            marker=markers.CARETRIGHT,  # type: ignore\n",
    "        )\n",
    "        ax.scatter(\n",
    "            data[\"precise_time\"].iloc[5],\n",
    "            data[metric].iloc[5] * 100,\n",
    "            color=sns.color_palette()[8],\n",
    "            clip_on=False,\n",
    "            marker=markers.CARETLEFT,  # type: ignore\n",
    "        )\n",
    "    else:\n",
    "        ax.scatter(\n",
    "            data[\"precise_time\"],\n",
    "            data[metric] * 100,\n",
    "            c=sns.color_palette()[:5] + [sns.color_palette()[8]],\n",
    "            clip_on=False,\n",
    "            marker=markers.CARETRIGHT,  # type: ignore\n",
    "        )\n",
    "\n",
    "    ax.grid(True, which=\"major\", axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.3, color=\"gray\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    sns.despine()\n",
    "\n",
    "    plt.savefig(f\"talk/scatter_{metric}_{dataset}_{queries}.svg\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_baseline_scatter(\"sportstables\", \"f1\", \"all\")\n",
    "plot_baseline_scatter(\"open_data_usa\", \"f1\", \"all\")\n",
    "plot_baseline_scatter(\"gittables\", \"f1\", \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fainder Exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binsort yields the same results as profile-scan so we reuse the runtime measurements from the runtime benchmark for this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_list = []\n",
    "path = Path(\"../logs/runtime_benchmark/execution/\")\n",
    "for logfile in chain(\n",
    "    path.glob(\"*collection-binsort-single*\"), path.glob(\"*collection-iterative-single*\")\n",
    "):\n",
    "    config = logfile.stem.split(\"-\")\n",
    "    data = parse_logs_wide(logfile)\n",
    "\n",
    "    data[\"dataset\"] = config[0]\n",
    "    data[\"query_set\"] = config[1]\n",
    "    data[\"approach\"] = config[2]\n",
    "    data[\"execution\"] = config[3]\n",
    "    data[\"iteration\"] = config[4]\n",
    "\n",
    "    baseline_list.append(data)\n",
    "\n",
    "baselines = pd.DataFrame(\n",
    "    baseline_list,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        # \"query_set\",\n",
    "        \"approach\",\n",
    "        # \"execution\",\n",
    "        \"iteration\",\n",
    "        \"query_collection_time\",\n",
    "    ],\n",
    ")\n",
    "baselines.rename(columns={\"query_collection_time\": \"baseline_time\"}, inplace=True)\n",
    "baselines.replace({\"approach\": {\"iterative\": \"pscan\"}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_list = []\n",
    "for logfile in Path(\"../logs/exact_results/\").glob(\"*.zst\"):\n",
    "    config = logfile.stem.split(\"-\")\n",
    "    data = load_input(logfile)\n",
    "\n",
    "    assert len(config) == 3\n",
    "    data[\"dataset\"] = config[0]\n",
    "    data[\"approach\"] = config[1]\n",
    "    data[\"iteration\"] = config[2]\n",
    "    exact_list.append(data)\n",
    "\n",
    "exact = pd.DataFrame(\n",
    "    exact_list,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"approach\",\n",
    "        \"iteration\",\n",
    "        \"precision_time\",\n",
    "        \"recall_time\",\n",
    "        \"iterative_time\",\n",
    "        \"avg_reduction\",\n",
    "    ],\n",
    ")\n",
    "exact = exact.merge(baselines, on=[\"dataset\", \"approach\", \"iteration\"])\n",
    "exact[\"exact_time\"] = exact[\"precision_time\"] + exact[\"recall_time\"] + exact[\"iterative_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = exact.groupby([\"dataset\", \"approach\"]).mean(numeric_only=True)\n",
    "analysis[\"speedup\"] = analysis[\"baseline_time\"] / analysis[\"exact_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = []\n",
    "for dataset in [\"sportstables\", \"open_data_usa\", \"gittables\"]:\n",
    "    _, ax = plt.subplots(figsize=(2.7, 2.8), layout=\"constrained\")\n",
    "    colors = [sns.color_palette()[i] for i in range(4)]\n",
    "\n",
    "    for i, baseline in enumerate([\"pscan\", \"binsort\"]):\n",
    "        data = analysis.query(f\"dataset == '{dataset}' & approach == '{baseline}'\")\n",
    "        handles += ax.bar(\n",
    "            i * 0.75,\n",
    "            data[\"baseline_time\"],\n",
    "            width=0.5,\n",
    "            color=colors[i],\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "\n",
    "        bottom = 0\n",
    "        for j, time in [\n",
    "            (3, data[\"recall_time\"]),\n",
    "            (2, data[\"precision_time\"]),\n",
    "            (i, data[\"iterative_time\"]),\n",
    "        ]:\n",
    "            handles += ax.bar(\n",
    "                1.5 + i * 0.55,\n",
    "                time,\n",
    "                bottom=bottom,\n",
    "                width=0.5,\n",
    "                color=colors[j],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            bottom += time.item()\n",
    "\n",
    "        for time, x in [\n",
    "            (data[\"baseline_time\"].item(), i * 0.75),\n",
    "            (bottom, 1.5 + i * 0.55),\n",
    "        ]:\n",
    "            label = f\"{time:.0f}\" if time > 100 else f\"{time:.1f}\"\n",
    "            ax.annotate(\n",
    "                label,\n",
    "                xy=(x, time),  # type: ignore\n",
    "                xytext=(0, 1),\n",
    "                fontsize=mpl.rcParams[\"font.size\"] * 0.9,\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "\n",
    "    ax.set_xticks([0, 0.75, 1.775])\n",
    "    ax.set_xticklabels([\"Full\\nscan\", \"bin\\nsort\", \"Fainder\\nExact\"])\n",
    "    if dataset == \"gittables\":\n",
    "        ax.set_ylim(200, 60000)\n",
    "    else:\n",
    "        ax.set_ylim(\n",
    "            analysis.query(f\"dataset == '{dataset}' & approach == 'pscan'\")[\"recall_time\"].item()\n",
    "            / 2\n",
    "            % 10,\n",
    "            ax.get_ylim()[1] * 1.19,\n",
    "        )\n",
    "    ax.set_yscale(\"log\")\n",
    "    sns.despine()\n",
    "\n",
    "    plt.savefig(f\"talk/exact_{dataset}.svg\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Microbenchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_list = []\n",
    "for logfile in Path(f\"../logs/microbenchmarks/runtime/\").iterdir():\n",
    "    config = logfile.stem.split(\"-\")\n",
    "    data = parse_logs_wide(logfile)\n",
    "\n",
    "    data[\"dataset\"] = config[0]\n",
    "    data[\"index_type\"] = config[1]\n",
    "    data[\"parameter\"] = config[2][0]\n",
    "    data[\"parameter_value\"] = int(config[2][1:])\n",
    "    data[\"execution\"] = config[3]\n",
    "\n",
    "    runtime_list.append(data)\n",
    "\n",
    "runtime = pd.DataFrame(\n",
    "    runtime_list,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"index_type\",\n",
    "        \"parameter\",\n",
    "        \"parameter_value\",\n",
    "        \"execution\",\n",
    "        \"query_collection_time\",\n",
    "        \"avg_result_size\",\n",
    "    ],\n",
    ")\n",
    "runtime = (\n",
    "    runtime.groupby([\"dataset\", \"index_type\", \"parameter\", \"parameter_value\", \"execution\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_list = []\n",
    "for logfile in Path(\"../logs/microbenchmarks/indexing/\").iterdir():\n",
    "    config = logfile.stem.split(\"-\")\n",
    "    if config[1] != \"rebinning\":\n",
    "        continue\n",
    "    data = parse_logs_wide(logfile)\n",
    "\n",
    "    data[\"dataset\"] = config[0]\n",
    "    data[\"phase\"] = config[1]\n",
    "    data[\"parameter\"] = config[2][0]\n",
    "    data[\"parameter_value\"] = int(config[2][1:])\n",
    "\n",
    "    size_list.append(data)\n",
    "\n",
    "index_size = pd.DataFrame(\n",
    "    size_list,\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"phase\",\n",
    "        \"parameter\",\n",
    "        \"parameter_value\",\n",
    "        \"index_size\",\n",
    "    ],\n",
    ")\n",
    "index_size = (\n",
    "    index_size.groupby([\"dataset\", \"phase\", \"parameter\", \"parameter_value\"]).mean().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"precision\", \"recall\", \"f1\", \"pruning_factor\"]\n",
    "accuracy_list = []\n",
    "\n",
    "for logfile in Path(\"../logs/microbenchmarks/accuracy/\").iterdir():\n",
    "    logs = load_input(logfile)\n",
    "    config = logfile.stem.split(\"-\")\n",
    "\n",
    "    for mode, mode_data in [\n",
    "        (\"recall\", logs[\"recall_mode_metrics\"]),\n",
    "        (\"precision\", logs[\"precision_mode_metrics\"]),\n",
    "    ]:\n",
    "        for i, values in enumerate(mode_data):\n",
    "            for value in values:\n",
    "                accuracy_list.append(\n",
    "                    {\n",
    "                        \"dataset\": config[0],\n",
    "                        \"index_type\": config[1],\n",
    "                        \"parameter\": config[2][0],\n",
    "                        \"parameter_value\": int(config[2][1:]),\n",
    "                        \"index_mode\": mode,\n",
    "                        \"metric\": metrics[i],\n",
    "                        \"value\": value,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "accuracy = pd.DataFrame(accuracy_list)\n",
    "accuracy = (\n",
    "    accuracy.groupby(\n",
    "        [\"dataset\", \"index_type\", \"index_mode\", \"parameter\", \"parameter_value\", \"metric\"]\n",
    "    )\n",
    "    .agg({\"value\": \"mean\"})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = runtime[(runtime[\"dataset\"] == \"open_data_usa\") & (runtime[\"parameter\"] == \"k\")]\n",
    "i = index_size[\n",
    "    (index_size[\"dataset\"] == \"open_data_usa\")\n",
    "    & (index_size[\"parameter\"] == \"k\")\n",
    "    & (index_size[\"phase\"] == \"rebinning\")\n",
    "]\n",
    "a = accuracy[\n",
    "    (accuracy[\"dataset\"] == \"open_data_usa\")\n",
    "    & (accuracy[\"parameter\"] == \"k\")\n",
    "    & (accuracy[\"index_mode\"] == \"recall\")\n",
    "    & (accuracy[\"metric\"] == \"f1\")\n",
    "]\n",
    "\n",
    "_, ax1 = plt.subplots(figsize=(7.2, 2.8), layout=\"constrained\")\n",
    "\n",
    "ax2: Axes = ax1.twinx()  # type: ignore\n",
    "ax3: Axes = ax1.twinx()  # type: ignore\n",
    "\n",
    "# Offset the right spine of ax3 to not collide with ax2\n",
    "ax3.spines.right.set_position((\"axes\", 1.2))\n",
    "\n",
    "# Runtime\n",
    "ax1.plot(\n",
    "    r[(r[\"execution\"] == \"single\")][\"parameter_value\"],\n",
    "    r[(r[\"execution\"] == \"single\")][\"query_collection_time\"],\n",
    "    color=sns.color_palette()[0],\n",
    "    label=\"w/ results\",\n",
    ")\n",
    "ax1.plot(\n",
    "    r[(r[\"execution\"] == \"single_suppressed\")][\"parameter_value\"],\n",
    "    r[(r[\"execution\"] == \"single_suppressed\")][\"query_collection_time\"],\n",
    "    color=sns.color_palette()[0],\n",
    "    label=\"w/o results\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "# Index size\n",
    "ax2.plot(\n",
    "    i[\"parameter_value\"],\n",
    "    i[\"index_size\"],\n",
    "    color=sns.color_palette()[1],\n",
    "    label=\"Index size\",\n",
    ")\n",
    "\n",
    "# Accuracy\n",
    "ax3.plot(\n",
    "    a[(a[\"index_type\"] == \"rebinning\")][\"parameter_value\"],\n",
    "    a[(a[\"index_type\"] == \"rebinning\")][\"value\"] * 100,\n",
    "    color=sns.color_palette()[2],\n",
    "    label=\"Low mem.\",\n",
    ")\n",
    "ax3.plot(\n",
    "    a[(a[\"index_type\"] == \"conversion\")][\"parameter_value\"],\n",
    "    a[(a[\"index_type\"] == \"conversion\")][\"value\"] * 100,\n",
    "    color=sns.color_palette()[2],\n",
    "    label=\"Full rec.\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "ax1.set(xlabel=\"Number of clusters\", xlim=(1, 1000), ylabel=\"Time (s)\", yscale=\"log\")\n",
    "ax2.set(ylabel=\"Index size (MB)\", yscale=\"log\")\n",
    "ax3.set(ylabel=\"$F_1$ score (%)\", ylim=(0, 100))\n",
    "\n",
    "ax1.set_xticks([1, 100, 200, 400, 600, 800, 1000])\n",
    "ax1.set_yticks([0.1, 1])\n",
    "\n",
    "ax1.yaxis.label.set_color(sns.color_palette()[0])\n",
    "ax2.yaxis.label.set_color(sns.color_palette()[1])\n",
    "ax3.yaxis.label.set_color(sns.color_palette()[2])\n",
    "\n",
    "plt.savefig(\"talk/microbenchmarks_open_data_usa_k.svg\", bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
