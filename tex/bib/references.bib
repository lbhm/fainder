
@incollection{dastin_amazon_2022,
	location = {Boca Raton, {FL}},
	edition = {1},
	title = {Amazon Scraps Secret {AI} Recruiting Tool that Showed Bias against Women},
	isbn = {978-1-00-327829-0},
	url = {https://www.taylorfrancis.com/books/9781003278290/chapters/10.1201/9781003278290-44},
	pages = {296--299},
	booktitle = {Ethics of Data and Analytics},
	publisher = {Auerbach Publications},
	author = {Dastin, Jeffrey},
	editor = {Martin, Kirsten},
	urldate = {2024-07-26},
	date = {2022-03-16},
	langid = {english},
	doi = {10.1201/9781003278290-44},
}

@inproceedings{culotta_reducing_2014,
	location = {Boston, {MA}},
	title = {Reducing Sampling Bias in Social Media Data for County Health Inference},
	series = {{JSM} '14},
	abstract = {A number of recent studies have demonstrated the utility of social media data for inferring societal attributes such as public opinion and health. A commonly declared limitation of this methodology is the selection bias inherent in this approach – social media users are a non-representative sample of the population. This is exacerbated by ﬁltering steps that further limit the sample set in biased ways. Building on recent work in computational linguistics that infers demographic attributes of people based on their communications, we investigate methods to quantify and control for selection bias in social media studies. We present results estimating several county-level health statistics (e.g., obesity, diabetes, access to healthy foods) based on the Twitter activity of the top 100 counties in the U.S., and we compare strategies for reducing selection bias.},
	eventtitle = {{JSM} '14},
	booktitle = {Joint Statistical Meetings},
	publisher = {American Statistical Association},
	author = {Culotta, Aron},
	date = {2014},
	langid = {english},
}

@online{rose_are_2010,
	title = {Are Face-Detection Cameras Racist?},
	url = {https://time.com/archive/6906847/are-face-detection-cameras-racist/},
	abstract = {Why face-detection software on cameras and webcams made by {HP}, Nikon and Sony is being called out by consumers for failing to recognize black and Asian features and faces},
	titleaddon = {Time},
	author = {Rose, Adam},
	urldate = {2024-02-06},
	date = {2010-01-22},
	langid = {american},
}

@inproceedings{wang_retrieving_2021,
	location = {Virtual Event},
	title = {Retrieving Complex Tables with Multi-Granular Graph Representation Learning},
	isbn = {978-1-4503-8037-9},
	url = {https://dl.acm.org/doi/10.1145/3404835.3462909},
	doi = {10.1145/3404835.3462909},
	series = {{SIGIR} '21},
	abstract = {The task of natural language table retrieval ({NLTR}) seeks to retrieve semantically relevant tables based on natural language queries. Existing learning systems for this task often treat tables as plain text based on the assumption that tables are structured as dataframes. However, tables can have complex layouts which indicate diverse dependencies between subtable structures, such as nested headers. As a result, queries may refer to different spans of relevant content that is distributed across these structures. Moreover, such systems fail to generalize to novel scenarios beyond those seen in the training set. Prior methods are still distant from a generalizable solution to the {NLTR} problem, as they fall short in handling complex table layouts or queries over multiple granularities. To address these issues, we propose Graph-based Table Retrieval ({GTR}), a generalizable {NLTR} framework with multi-granular graph representation learning. In our framework, a table is first converted into a tabular graph, with cell nodes, row nodes and column nodes to capture content at different granularities. Then the tabular graph is input to a Graph Transformer model that can capture both table cell content and the layout structures. To enhance the robustness and generalizability of the model, we further incorporate a self-supervised pre-training task based on graph-context matching. Experimental results on two benchmarks show that our method leads to significant improvements over the current state-of-the-art systems. Further experiments demonstrate promising performance of our method on cross-dataset generalization, and enhanced capability of handling complex tables and fulfilling diverse query intents.},
	eventtitle = {{SIGIR} '21},
	pages = {1472--1482},
	booktitle = {44th International {ACM} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Wang, Fei and Sun, Kexuan and Chen, Muhao and Pujara, Jay and Szekely, Pedro},
	urldate = {2024-02-07},
	date = {2021-07-11},
	keywords = {graph transformer, pre-training, semantic retrieval, table retrieval},
}

@inproceedings{kato_test_2021,
	location = {Virtual Event, Canada},
	title = {A Test Collection for Ad-hoc Dataset Retrieval},
	isbn = {978-1-4503-8037-9},
	url = {https://dl.acm.org/doi/10.1145/3404835.3463261},
	doi = {10.1145/3404835.3463261},
	series = {{SIGIR} '21},
	abstract = {This paper introduces a new test collection for ad-hoc dataset retrieval, which have been developed through a shared task called Data Search in the fifteenth {NTCIR}. This test collection consists of dataset collections derived from the {US} and Japanese governments' open data sites (i.e., Data.gov and e-Stat), as well as English and Japanese topics for these collections. Organizing the shared task in {NTCIR}, we conducted relevance judgments for datasets retrieved by 74 search systems, and included them in the test collection. In addition to the detailed description of the test collection, we conducted in-depth analysis on the test collection, and revealed (1) what techniques were used and effective, (2) what topics were difficult, and (3) large topic variability in the dataset retrieval task.},
	eventtitle = {{SIGIR} '21},
	pages = {2450--2456},
	booktitle = {44th International {ACM} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Kato, Makoto P. and Ohshima, Hiroaki and Liu, Ying-Hsang and Chen, Hsin-Liang},
	urldate = {2023-05-08},
	date = {2021-07-11},
	keywords = {ad-hoc retrieval, dataset search, test collection},
}

@inproceedings{lin_acordar_2022,
	location = {Madrid, Spain},
	title = {{ACORDAR}: A Test Collection for Ad Hoc Content-Based ({RDF}) Dataset Retrieval},
	isbn = {978-1-4503-8732-3},
	url = {https://dl.acm.org/doi/10.1145/3477495.3531729},
	doi = {10.1145/3477495.3531729},
	series = {{SIGIR} '22},
	shorttitle = {{ACORDAR}},
	abstract = {Ad hoc dataset retrieval is a trending topic in {IR} research. Methods and systems are evolving from metadata-based to content-based ones which exploit the data itself for improving retrieval accuracy but thus far lack a specialized test collection. In this paper, we build and release the first test collection for ad hoc content-based dataset retrieval, where content-oriented dataset queries and content-based relevance judgments are annotated by human experts who are assisted with a dashboard designed specifically for comprehensively and conveniently browsing both the metadata and data of a dataset. We conduct extensive experiments on the test collection to analyze its difficulty and provide insights into the underlying task.},
	eventtitle = {{SIGIR} '22},
	pages = {2981--2991},
	booktitle = {45th International {ACM} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Lin, Tengteng and Chen, Qiaosheng and Cheng, Gong and Soylu, Ahmet and Ell, Basil and Zhao, Ruoqi and Shi, Qing and Wang, Xiaxia and Gu, Yu and Kharlamov, Evgeny},
	urldate = {2023-05-08},
	date = {2022-07-07},
	keywords = {ad hoc dataset retrieval, dataset browsing, dataset search, rdf, test collection},
}

@inproceedings{langenecker_sportstables_2023,
	location = {Dresden, Germany},
	title = {{SportsTables}: A New Corpus for Semantic Type Detection},
	isbn = {978-3-88579-725-8},
	url = {http://dl.gi.de/handle/20.500.12116/40377},
	doi = {10.18420/BTW2023-68},
	shorttitle = {{SportsTables}},
	abstract = {Table corpora such as {VizNet} or {TURL} which contain annotated semantic types per column are important to build machine learning models for the task of automatic semantic type detection. However, there is a huge discrepancy between corpora that are used for training and testing since real-world data lakes contain a huge fraction of numerical data which are not present in existing corpora. Hence, in this paper, we introduce a new corpus that contains a much higher proportion of numerical columns than existing corpora. To reflect the distribution in real-world data lakes, our corpus {SportsTables} has on average approx. 86\% numerical columns, posing new challenges to existing semantic type detection models which have mainly targeted non-numerical columns so far. To demonstrate this effect, we show the results of a first study using a state-of-the-art approach for semantic type detection on our new corpus and demonstrate significant performance differences in predicting semantic types for textual and numerical data.},
	eventtitle = {{BTW} '23},
	pages = {995--1008},
	booktitle = {{BTW}},
	publisher = {Gesellschaft für Informatik e.V.},
	author = {Langenecker, Sven and Sturm, Christoph and Schalles, Christian and Binnig, Carsten},
	urldate = {2023-11-28},
	date = {2023},
	langid = {english},
	note = {{ISBN}: 9783885797258
Publisher: Gesellschaft für Informatik e.V.},
	keywords = {Semantic Type Detection{\textbar}{\textbar}Column Annotated Corpora},
}

@inproceedings{herzig_open_2021,
	location = {Virtual Event},
	title = {Open Domain Question Answering over Tables via Dense Retrieval},
	url = {https://aclanthology.org/2021.naacl-main.43},
	doi = {10.18653/v1/2021.naacl-main.43},
	series = {{NAACL}-{HLT} '21},
	abstract = {Recent advances in open-domain {QA} have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain {QA} over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table {QA} dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end {QA} results from 33.8 to 37.7 exact match, over a {BERT} based retriever.},
	eventtitle = {{NAACL}-{HLT} '21},
	pages = {512--519},
	booktitle = {Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Herzig, Jonathan and Müller, Thomas and Krichene, Syrine and Eisenschlos, Julian},
	urldate = {2024-07-26},
	date = {2021-06},
}

@inproceedings{campello_density-based_2013,
	location = {Gold Coast, Australia},
	title = {Density-Based Clustering Based on Hierarchical Density Estimates},
	isbn = {978-3-642-37456-2},
	doi = {10.1007/978-3-642-37456-2_14},
	series = {{PAKDD} '13},
	abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a “flat” partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
	eventtitle = {{PAKDD} '13},
	pages = {160--172},
	booktitle = {Pacific-Asia Conference on Knowledge Discovery and Data Mining},
	author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
	editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
	date = {2013},
	langid = {english},
	keywords = {Cluster Tree, Core Object, Density Threshold, Hierarchical Cluster Method, Minimum Span Tree},
}

@inproceedings{bogatu_voyager_2022,
	location = {Virtual Event, United Kingdom},
	title = {Voyager: Data Discovery and Integration for Data Science},
	url = {https://openproceedings.org/2022/conf/edbt/paper-103.pdf},
	series = {{EDBT} '22},
	shorttitle = {Voyager},
	abstract = {Data discovery and integration have grown to become two important research �elds in both academic and commercial domains, mainly fueled by the ever increasing availability of datasets that are stored by organisations without their conceptual meaning or relationships being explicitly known. These tasks can be carried out in di�erent settings and for di�erent purposes; here we focus on the collection of tasks performed by data scientists to acquire the knowledge needed when deciding what analyses to perform on client data. In this paper, we focus on support for three processes often encountered in practice by data scientists: data identi�cation, data understanding and relationship discovery. We describe our practical experience with each of these processes and the means by which we assist data scientists in performing them. We have been informed by real–life use–cases in identifying the tasks carried out routinely by data scientists at Peak {AI}. The paper reports the design decisions made in the development of a system to support data discovery and integration, and reports on an evaluation that investigates both usability and task e�ciency.},
	eventtitle = {{EDBT} '22},
	pages = {537--548},
	booktitle = {25th International Conference on Extending Database Technology},
	publisher = {{OpenProceedings}.org},
	author = {Bogatu, Alex and Paton, Norman W. and Douthwaite, Mark and Freitas, André},
	urldate = {2022-08-11},
	date = {2022},
	langid = {english},
	keywords = {Database Technology},
}

@inproceedings{behar_optimal_2020,
	location = {Copenhagen, Denmark},
	title = {Optimal Histograms with Outliers},
	url = {https://openproceedings.org/2020/conf/edbt/paper_93.pdf},
	doi = {10.5441/002/EDBT.2020.17},
	series = {{EDBT} '20},
	abstract = {Histograms are a well studied and simple way to summarize data. As such, they are used extensively in a variety of applications that require estimates of data frequency values. Significant previous work has studied the problem of finding optimal histograms with respect to an error measure. In this paper we study the classic problem of finding an optimal histogram for a dataset, with a new twist: The histogram must contain at least 𝑛 − 𝑘 of the 𝑛 data points. The 𝑘 excluded data points are considered outliers. We consider two notions of excluding data items, by allowing arbitrary items to be excluded, or only removing items while retaining a consistent histogram. Polynomial algorithms are presented for these problems. Significant experimentation demonstrates that our algorithms work well in practice to reduce the histogram error.},
	eventtitle = {{EDBT} '20},
	pages = {181--192},
	booktitle = {23rd International Conference on Extending Database Technology},
	publisher = {{OpenProceedings}.org},
	author = {Behar, Rachel and Cohen, Sara},
	urldate = {2024-02-26},
	date = {2020},
	langid = {english},
	keywords = {Database Technology},
}

@inproceedings{esmailoghli_cocoa_2021,
	location = {Nicosia, Cyprus},
	title = {{COCOA}: {COrrelation} {COefficient}-Aware Data Augmentation},
	url = {https://doi.org/10.5441/002/edbt.2021.30},
	doi = {10.5441/002/edbt.2021.30},
	series = {{EDBT} '21},
	abstract = {Calculating correlation coefficients is one of the most used measures in data science. Although linear correlations are fast and easy to calculate, they lack robustness and effectiveness in the existence of non-linear associations. Rank-based coefficients such as Spearman’s are more suitable. However, rank-based measures first require to sort the values and obtain the ranks, making their calculation super-linear. One of the use-cases that is affected by this is data enrichment for Machine Learning ({ML}) through feature extraction from large databases. Finding the most promising features from millions of candidates to increase the {ML} accuracy requires billions of correlation calculations. In this paper, we introduce an index structure that ensures rank-based correlation calculation in a linear time. Our solution accelerates the correlation calculation up to 500 times in the data enrichment setting.},
	eventtitle = {{EDBT} '21},
	pages = {331--336},
	booktitle = {24th International Conference on Extending Database Technology},
	publisher = {{OpenProceedings}.org},
	author = {Esmailoghli, Mahdi and Quiané-Ruiz, Jorge-Arnulfo and Abedjan, Ziawasch},
	date = {2021},
}

@inproceedings{derakhshan_continuous_2019,
	location = {Lisbon, Portugal},
	title = {Continuous Deployment of Machine Learning Pipelines},
	series = {{EDBT} '22},
	abstract = {Today machine learning is entering many business and scienti c applications. The life cycle of machine learning applications consists of data preprocessing for transforming the raw data into features, training a model using the features, and deploying the model for answering prediction queries. In order to guarantee accurate predictions, one has to continuously monitor and update the deployed model and pipeline. Current deployment platforms update the model using online learning methods. When online learning alone is not adequate to guarantee the prediction accuracy, some deployment platforms provide a mechanism for automatic or manual retraining of the model. While the online training is fast, the retraining of the model is time-consuming and adds extra overhead and complexity to the process of deployment. We propose a novel continuous deployment approach for updating the deployed model using a combination of the incoming realtime data and the historical data. We utilize sampling techniques to include the historical data in the training process, thus eliminating the need for retraining the deployed model. We also o er online statistics computation and dynamic materialization of the preprocessed features, which further reduces the total training and data preprocessing time. In our experiments, we design and deploy two pipelines and models to process two real-world datasets. The experiments show that continuous deployment reduces the total training cost up to 15 times while providing the same level of quality when compared to the state-of-the-art deployment approaches.},
	eventtitle = {{EDBT} '22},
	pages = {12},
	booktitle = {22nd International Conference on Extending Database Technology},
	author = {Derakhshan, Behrouz and Mahdiraji, Alireza Rezaei and Rabl, Tilmann and Markl, Volker},
	date = {2019-03},
	langid = {english},
}

@inproceedings{flores_effective_2021,
	location = {Nicosia, Cyprus},
	title = {Effective and Scalable Data Discovery with {NextiaJD}},
	url = {https://openproceedings.org/2021/conf/edbt/p184.pdf},
	doi = {10.5441/002/EDBT.2021.85},
	series = {{EDBT} '21},
	abstract = {We present {NextiaJD}, a data discovery system with high predictive performance and computational efficiency. {NextiaJD} aids data scientists in the discovery of datasets that can be crossed. To that end, it proposes a ranking of candidate pairs according to their join quality, which is based on a novel similarity measure that considers both containment and cardinality proportions between candidate attributes. To do so, {NextiaJD} adopts a learning approach relying on profiles. These are succinct and informative representations of the schemata and data values of datasets that capture their underlying characteristics. {NextiaJD}’s features are fully integrated into Apache Spark and benefits from it to parallelize the profiling and discovery processes. The on-site demonstration will showcase how {NextiaJD} can effectively support large-scale data discovery tasks with a large set of datasets the audience will be able to play with.},
	eventtitle = {{EDBT} '21},
	pages = {690--693},
	booktitle = {24th International Conference on Extending Database Technology},
	publisher = {{OpenProceedings}.org},
	author = {Flores, Javier and Nadal, Sergi and Romero, Oscar},
	urldate = {2022-09-01},
	date = {2021},
	langid = {english},
	keywords = {Database Technology},
}

@inproceedings{mishra_interactive_2009,
	location = {Saint Petersburg, Russia},
	title = {Interactive Query Refinement},
	isbn = {978-1-60558-422-5},
	url = {http://portal.acm.org/citation.cfm?doid=1516360.1516459},
	doi = {10.1145/1516360.1516459},
	series = {{EDBT} '09},
	abstract = {We investigate the problem of reﬁning {SQL} queries to satisfy cardinality constraints on the query result. This has applications to the many/few answers problems often faced by database users. We formalize the problem of query reﬁnement and propose a framework to support it in a database system. We introduce an interactive model of reﬁnement that incorporates user feedback to best capture user preferences. Our techniques are designed to handle queries having range and equality predicates on numerical and categorical attributes. We present an experimental evaluation of our framework implemented in an open source data manager and demonstrate the feasibility and practical utility of our approach.},
	eventtitle = {{EDBT} '09},
	pages = {862--873},
	booktitle = {12th International Conference on Extending Database Technology},
	publisher = {{ACM} Press},
	author = {Mishra, Chaitanya and Koudas, Nick},
	urldate = {2022-12-19},
	date = {2009},
	langid = {english},
}

@inproceedings{damme_lightweight_2017,
	location = {Venice, Italy},
	title = {Lightweight Data Compression Algorithms: An Experimental Survey},
	url = {https://openproceedings.org/2017/conf/edbt/paper-146.pdf},
	series = {{EDBT} '17},
	shorttitle = {Lightweight Data Compression Algorithms},
	abstract = {Lightweight data compression algorithms are frequently applied in in-memory database systems to tackle the growing gap between processor speed and main memory bandwidth. In recent years, the vectorization of basic techniques such as delta coding and null suppression has considerably enlarged the corpus of available algorithms. As a result, today there is a large number of algorithms to choose from, while diﬀerent algorithms are tailored to diﬀerent data characteristics. However, a comparative evaluation of these algorithms under diﬀerent data characteristics has never been suﬃciently conducted in the literature. To close this gap, we conducted an exhaustive experimental survey by evaluating several state-of-the-art compression algorithms as well as cascades of basic techniques. We systematically investigated the inﬂuence of the data properties on the performance and the compression rates. The evaluated algorithms are based on publicly available implementations as well as our own vectorized reimplementations. We summarize our experimental ﬁndings leading to several new insights and to the conclusion, that there is no single-best algorithm.},
	eventtitle = {{EDBT} '17},
	booktitle = {20th International Conference on Extending Database Technology},
	publisher = {{OpenProceedings}.org},
	author = {Damme, Patrick and Habich, Dirk and Hildebrandt, Juliana and Lehner, Wolfgang},
	urldate = {2021-11-22},
	date = {2017},
	langid = {english},
	keywords = {Database Technology},
}

@inproceedings{flores_towards_2021,
	location = {Nicosia, Cyprus},
	title = {Towards Scalable Data Discovery},
	url = {https://openproceedings.org/2021/conf/edbt/p61.pdf},
	doi = {10.5441/002/edbt.2021.47},
	series = {{EDBT} '21},
	abstract = {We study the problem of discovering joinable datasets at scale. We approach the problem from a learning perspective relying on profiles. These are succinct representations that capture the underlying characteristics of the schemata and data values of datasets, which can be efficiently extracted in a distributed and parallel fashion. Profiles are then compared, to predict the quality of a join operation among a pair of attributes from different datasets. In contrast to the state-of-the-art, we define a novel notion of join quality that relies on a metric considering both the containment and cardinality proportion between join candidate attributes. We implement our approach in a system called {NextiaJD}, and present experiments to show the predictive performance and computational efficiency of our method. Our experiments show that {NextiaJD} obtains similar predictive performance to that of hashbased methods, yet we are able to scale-up to larger volumes of data. Also, {NextiaJD} generates a considerably less amount of false positives, which is a desirable feature at scale.},
	eventtitle = {{EDBT} '21},
	pages = {433--438},
	booktitle = {24th International Conference on Extending Database Technology},
	publisher = {{OpenProceedings}.org},
	author = {Flores, Javier and Nadal, Sergi and Romero, Oscar},
	urldate = {2022-09-01},
	date = {2021},
	langid = {english},
	keywords = {Database Technology},
}

@inproceedings{farber_recommending_2021,
	location = {Virtual Event, Australia},
	title = {Recommending Datasets for Scientific Problem Descriptions},
	isbn = {978-1-4503-8446-9},
	url = {https://dl.acm.org/doi/10.1145/3459637.3482166},
	doi = {10.1145/3459637.3482166},
	series = {{CIKM} '21},
	abstract = {The steadily rising number of datasets is making it increasingly difficult for researchers and practitioners to be aware of all datasets, particularly of the most relevant datasets for a given research problem. To this end, dataset search engines have been proposed. However, they are based on user's keywords and, thus, have difficulty determining precisely fitting datasets for complex research problems. In this paper, we propose a system that recommends suitable datasets based on a given research problem description. The recommendation task is designed as a domain-specific text classification task. As shown in a comprehensive offline evaluation using various state-of-the-art models, as well as 88,000 paper abstracts and 265,000 citation contexts as research problem descriptions, we obtain an F1-score of 0.75. In an additional user study, we show that users in real-world settings are 88\% satisfied in all test cases. We therefore see promising future directions for dataset recommendation.},
	eventtitle = {{CIKM} '21},
	pages = {3014--3018},
	booktitle = {30th {ACM} International Conference on Information \& Knowledge Management},
	publisher = {{ACM} Press},
	author = {Färber, Michael and Leisinger, Ann-Kathrin},
	urldate = {2023-05-04},
	date = {2021-10-30},
	keywords = {datasets, machine learning, recommendation, text classification},
}

@inproceedings{yahya_robust_2013,
	location = {San Francisco, {CA}},
	title = {Robust Question Answering over the Web of Linked Data},
	isbn = {978-1-4503-2263-8},
	url = {http://dl.acm.org/citation.cfm?doid=2505515.2505677},
	doi = {10.1145/2505515.2505677},
	series = {{CIKM} '13},
	abstract = {Knowledge bases and the Web of Linked Data have become important assets for search, recommendation, and analytics. Natural-language questions are a user-friendly mode of tapping this wealth of knowledge and data. However, question answering technology does not work robustly in this setting as questions have to be translated into structured queries and users have to be careful in phrasing their questions. This paper advocates a new approach that allows questions to be partially translated into relaxed queries, covering the essential but not necessarily all aspects of the user’s input. To compensate for the omissions, we exploit textual sources associated with entities and relational facts. Our system translates user questions into an extended form of structured {SPARQL} queries, with text predicates attached to triple patterns. Our solution is based on a novel optimization model, cast into an integer linear program, for joint decomposition and disambiguation of the user question. We demonstrate the quality of our methods through experiments with the {QALD} benchmark.},
	eventtitle = {{CIKM} '13},
	pages = {1107--1116},
	booktitle = {22nd {ACM} International Conference on Information \& Knowledge Management},
	publisher = {{ACM} Press},
	author = {Yahya, Mohamed and Berberich, Klaus and Elbassuoni, Shady and Weikum, Gerhard},
	urldate = {2023-03-13},
	date = {2013},
	langid = {english},
}

@inproceedings{baunsgaard_federated_2022,
	location = {Atlanta, {GA}},
	title = {Federated Data Preparation, Learning, and Debugging in Apache {SystemDS}},
	isbn = {978-1-4503-9236-5},
	url = {https://dl.acm.org/doi/10.1145/3511808.3557162},
	doi = {10.1145/3511808.3557162},
	series = {{CIKM} '22},
	abstract = {Federated learning allows training machine learning ({ML}) models without central consolidation of the raw data. Variants of such federated learning systems enable privacy-preserving {ML}, and address data ownership and/or sharing constraints. However, existing work mostly adopt data-parallel parameter-server architectures for mini-batch training, require manual construction of federated runtime plans, and largely ignore the broad variety of data preparation, {ML} algorithms, and model debugging. Over the last years, we extended Apache {SystemDS} by an additional federated runtime backend for federated linear-algebra programs, federated parameter servers, and federated data preparation. In this paper, we share the system-level compiler and runtime integration, new features such as multi-tenant federated learning, selected federated primitives, multi-key homomorphic encryption, and our monitoring infrastructure. Our demonstrator showcases how composite {ML} pipelines can be compiled into federated runtime plans with low overhead.},
	eventtitle = {{CIKM} '22},
	pages = {4813--4817},
	booktitle = {31st {ACM} International Conference on Information \& Knowledge Management},
	publisher = {{ACM} Press},
	author = {Baunsgaard, Sebastian and Boehm, Matthias and Innerebner, Kevin and Kehayov, Mito and Lackner, Florian and Ovcharenko, Olga and Phani, Arnab and Rieger, Tobias and Weissteiner, David and Wrede, Sebastian Benjamin},
	urldate = {2023-03-20},
	date = {2022},
	keywords = {federated learning, federated raw data, monitoring},
}

@inproceedings{zhang_dsdd_2021,
	location = {Virtual Event, Australia},
	title = {{DSDD}: Domain-Specific Dataset Discovery on the Web},
	isbn = {978-1-4503-8446-9},
	url = {https://doi.org/10.1145/3459637.3482427},
	doi = {10.1145/3459637.3482427},
	series = {{CIKM} '21},
	abstract = {With the push for transparency and open data, many datasets and data repositories are becoming available on the Web. This opens new opportunities for data-driven exploration, from empowering analysts to answer new questions and obtain insights to improving predictive models through data augmentation. But as datasets are spread over a plethora of Web sites, finding data that are relevant for a given task is difficult. In this paper, we take a first step towards the construction of domain-specific data lakes. We propose an end-to-end dataset discovery system, targeted at domain experts, which given a small set of keywords, automatically finds potentially relevant datasets on the Web. The system makes use of search engines to hop across Web sites, uses online learning to incrementally build a model to recognize sites that contain datasets, utilizes a set of discovery actions to broaden the search, and applies a multi-armed bandit based algorithm to balance the trade-offs of different discovery actions. We report the results of an extensive experimental evaluation over multiple domains, and demonstrate that our strategy is effective and outperforms state-of-the-art content discovery methods.},
	eventtitle = {{CIKM} '21},
	pages = {2527--2536},
	booktitle = {30th {ACM} International Conference on Information \& Knowledge Management},
	publisher = {{ACM} Press},
	author = {Zhang, Haoxiang and Santos, Aécio and Freire, Juliana},
	date = {2021},
	note = {event-place:},
	keywords = {domain-specific dataset discovery, focused crawling, meta search, multi-armed bandit, online learning},
}

@inproceedings{chen_towards_2019,
	location = {Beijing, China},
	title = {Towards More Usable Dataset Search: From Query Characterization to Snippet Generation},
	isbn = {978-1-4503-6976-3},
	url = {https://dl.acm.org/doi/10.1145/3357384.3358096},
	doi = {10.1145/3357384.3358096},
	series = {{CIKM} '19},
	shorttitle = {Towards More Usable Dataset Search},
	abstract = {Reusing published datasets on the Web is of great interest to researchers and developers. Their data needs may be met by submitting queries to a dataset search engine to retrieve relevant datasets. In this ongoing work towards developing a more usable dataset search engine, we characterize real data needs by annotating the semantics of 1,947 queries using a novel fine-grained scheme, to provide implications for enhancing dataset search. Based on the findings, we present a query-centered framework for dataset search, and explore the implementation of snippet generation and evaluate it with a preliminary user study.},
	eventtitle = {{CIKM} '19},
	pages = {2445--2448},
	booktitle = {28th {ACM} International Conference on Information and Knowledge Management},
	publisher = {{ACM} Press},
	author = {Chen, Jinchi and Wang, Xiaxia and Cheng, Gong and Kharlamov, Evgeny and Qu, Yuzhong},
	urldate = {2023-05-08},
	date = {2019-11-03},
	keywords = {data needs, dataset search, query annotation, snippet generation},
}

@inproceedings{noy_google_2019,
	location = {San Francisco, {CA}},
	title = {Google Dataset Search: Building a Search Engine for Datasets in an Open Web Ecosystem},
	isbn = {978-1-4503-6674-8},
	url = {https://doi.org/10.1145/3308558.3313685},
	doi = {10.1145/3308558.3313685},
	series = {{WWW} '19},
	abstract = {There are thousands of data repositories on the Web, providing access to millions of datasets. National and regional governments, scientific publishers and consortia, commercial data providers, and others publish data for fields ranging from social science to life science to high-energy physics to climate science and more. Access to this data is critical to facilitating reproducibility of research results, enabling scientists to build on others’ work, and providing data journalists easier access to information and its provenance. In this paper, we discuss Google Dataset Search, a dataset-discovery tool that provides search capabilities over potentially all datasets published on the Web. The approach relies on an open ecosystem, where dataset owners and providers publish semantically enhanced metadata on their own sites. We then aggregate, normalize, and reconcile this metadata, providing a search engine that lets users find datasets in the “long tail” of the Web. In this paper, we discuss both social and technical challenges in building this type of tool, and the lessons that we learned from this experience.},
	eventtitle = {{WWW} '19},
	pages = {1365--1375},
	booktitle = {World Wide Web Conference},
	author = {Noy, Natasha and Burgess, Matthew and Brickley, Dan},
	date = {2019},
	langid = {english},
}

@inproceedings{gong_store_2023,
	location = {Austin, {TX}},
	title = {To Store or Not? Online Data Selection for Federated Learning with Limited Storage},
	isbn = {978-1-4503-9416-1},
	url = {https://dl.acm.org/doi/10.1145/3543507.3583426},
	doi = {10.1145/3543507.3583426},
	series = {{WWW} '23},
	shorttitle = {To Store or Not?},
	abstract = {Machine learning models have been deployed in mobile networks to deal with massive data from different layers to enable automated network management and intelligence on devices. To overcome high communication cost and severe privacy concerns of centralized machine learning, federated learning ({FL}) has been proposed to achieve distributed machine learning among networked devices. While the computation and communication limitation has been widely studied, the impact of on-device storage on the performance of {FL} is still not explored. Without an effective data selection policy to filter the massive streaming data on devices, classical {FL} can suffer from much longer model training time (4 ×) and significant inference accuracy reduction (7\%), observed in our experiments. In this work, we take the first step to consider the online data selection for {FL} with limited on-device storage. We first define a new data valuation metric for data evaluation and selection in {FL} with theoretical guarantees for speeding up model convergence and enhancing final model accuracy, simultaneously. We further design {ODE}, a framework of Online Data {sElection} for {FL}, to coordinate networked devices to store valuable data samples. Experimental results on one industrial dataset and three public datasets show the remarkable advantages of {ODE} over the state-of-the-art approaches. Particularly, on the industrial dataset, {ODE} achieves as high as 2.5 × speedup of training time and 6\% increase in inference accuracy, and is robust to various factors in practical environments.},
	eventtitle = {{WWW} '23},
	pages = {3044--3055},
	booktitle = {{ACM} Web Conference},
	publisher = {Association for Computing Machinery},
	author = {Gong, Chen and Zheng, Zhenzhe and Wu, Fan and Shao, Yunfeng and Li, Bingshuai and Chen, Guihai},
	urldate = {2023-05-25},
	date = {2023-04-30},
	keywords = {Data Selection, Federated Learning, Limited On-Device Storage},
}

@inproceedings{zhang_ad_2018,
	location = {Geneva, Switzerland},
	title = {Ad Hoc Table Retrieval using Semantic Similarity},
	isbn = {978-1-4503-5639-8},
	url = {https://dl.acm.org/doi/10.1145/3178876.3186067},
	doi = {10.1145/3178876.3186067},
	series = {{WWW} '18},
	abstract = {We introduce and address the problem of ad hoc table retrieval: answering a keyword query with a ranked list of tables. This task is not only interesting on its own account, but is also being used as a core component in many other table-based information access scenarios, such as table completion or table mining. The main novel contribution of this work is a method for performing semantic matching between queries and tables. Specifically, we (i) represent queries and tables in multiple semantic spaces (both discrete sparse and continuous dense vector representations) and (ii) introduce various similarity measures for matching those semantic representations. We consider all possible combinations of semantic representations and similarity measures and use these as features in a supervised learning model. Using a purpose-built test collection based on Wikipedia tables, we demonstrate significant and substantial improvements over a state-of-the-art baseline.},
	eventtitle = {{WWW} '18},
	pages = {1553--1562},
	booktitle = {World Wide Web Conference},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Zhang, Shuo and Balog, Krisztian},
	urldate = {2023-05-05},
	date = {2018-04-10},
	keywords = {semantic matching, semantic representations, semantic similarity, table retrieval, table search},
}

@inproceedings{rezig_towards_2021,
	location = {Virtual Event, Japan},
	title = {Towards Data Discovery by Example},
	volume = {12633},
	isbn = {978-3-030-71055-2},
	doi = {10.1007/978-3-030-71055-2_6},
	series = {Lecture Notes in Computer Science},
	abstract = {Data scientists today have to query an avalanche of multi-source data (e.g., data lakes, company databases) for diverse analytical tasks. Data discovery is labor-intensive as users have to find the right tables, and the combination thereof to answer their queries. Data discovery systems automatically find and link (e.g., joins) tables across various sources to aid users in finding the data they need. In this paper, we outline our ongoing efforts to build a data discovery by example system, {DICE}, that iteratively searches for new tables guided by user-provided data examples. Additionally, {DICE} asks users to validate results to improve the discovery process over multiple iterations.},
	eventtitle = {Poly/{DMAH}@{VLDB} '20},
	pages = {66--71},
	booktitle = {Workshop on Heterogeneous Data Management, Polystores, and Analytics for Healthcare},
	publisher = {Springer International Publishing},
	author = {Rezig, El Kindi and Vanterpool, Allan and Gadepally, Vijay and Price, Benjamin and Cafarella, Michael and Stonebraker, Michael},
	date = {2021},
}

@inproceedings{mo_ppfl_2021,
	location = {Virtual Event, Wisconsin},
	title = {{PPFL}: Privacy-preserving Federated Learning with Trusted Execution Environments},
	isbn = {978-1-4503-8443-8},
	url = {https://dl.acm.org/doi/10.1145/3458864.3466628},
	doi = {10.1145/3458864.3466628},
	shorttitle = {{PPFL}},
	abstract = {We propose and implement a Privacy-preserving Federated Learning (������������������ ������) framework for mobile systems to limit privacy leakages in federated learning. Leveraging the widespread presence of Trusted Execution Environments ({TEEs}) in high-end and mobile devices, we utilize {TEEs} on clients for local training, and on servers for secure aggregation, so that model/gradient updates are hidden from adversaries. Challenged by the limited memory size of current {TEEs}, we leverage greedy layer-wise training to train each model’s layer inside the trusted area until its convergence. The performance evaluation of our implementation shows that ������������������ ������ can significantly improve privacy while incurring small system overheads at the client-side. In particular, ������������������ ������ can successfully defend the trained model against data reconstruction, property inference, and membership inference attacks. Furthermore, it can achieve comparable model utility with fewer communication rounds (0.54×) and a similar amount of network traffic (1.002×) compared to the standard federated learning of a complete model. This is achieved while only introducing up to ∼15\% {CPU} time, ∼18\% memory usage, and ∼21\% energy consumption overhead in ������������������ ������’s client-side.},
	eventtitle = {{MobiSys} '21},
	pages = {94--108},
	booktitle = {19th Annual International Conference on Mobile Systems, Applications, and Services},
	publisher = {{ACM}},
	author = {Mo, Fan and Haddadi, Hamed and Katevas, Kleomenis and Marin, Eduard and Perino, Diego and Kourtellis, Nicolas},
	urldate = {2021-06-30},
	date = {2021-06-24},
	langid = {english},
}

@inproceedings{yang_flop_2021,
	location = {Virtual Event, Singapore},
	title = {{FLOP}: Federated Learning on Medical Datasets using Partial Networks},
	url = {http://arxiv.org/abs/2102.05218},
	doi = {10.1145/3447548.3467185},
	shorttitle = {{FLOP}},
	abstract = {The outbreak of {COVID}-19 Disease due to the novel coronavirus has caused a shortage of medical resources. To aid and accelerate the diagnosis process, automatic diagnosis of {COVID}-19 via deep learning models has recently been explored by researchers across the world. While different data-driven deep learning models have been developed to mitigate the diagnosis of {COVID}-19, the data itself is still scarce due to patient privacy concerns. Federated Learning ({FL}) is a natural solution because it allows different organizations to cooperatively learn an effective deep learning model without sharing raw data. However, recent studies show that {FL} still lacks privacy protection and may cause data leakage. We investigate this challenging problem by proposing a simple yet effective algorithm, named Federated Learning on Medical Datasets using Partial Networks ({FLOP}), that shares only a partial model between the server and clients. Extensive experiments on benchmark data and real-world healthcare tasks show that our approach achieves comparable or better performance while reducing the privacy and security risks. Of particular interest, we conduct experiments on the {COVID}-19 dataset and find that our {FLOP} algorithm can allow different hospitals to collaboratively and effectively train a partially shared model without sharing local patients’ data.},
	eventtitle = {{KDD} '21},
	pages = {3845--3853},
	booktitle = {27th {ACM} {SIGKDD} Conference on Knowledge Discovery \& Data Mining},
	author = {Yang, Qian and Zhang, Jianyi and Hao, Weituo and Spell, Gregory and Carin, Lawrence},
	urldate = {2021-09-29},
	date = {2021-08-14},
	langid = {english},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{li_fedrs_2021,
	location = {Virtual Event, Singapore},
	title = {{FedRS}: Federated Learning with Restricted Softmax for Label Distribution Non-{IID} Data},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467254},
	doi = {10.1145/3447548.3467254},
	shorttitle = {{FedRS}},
	abstract = {Federated Learning ({FL}) aims to generate a global shared model via collaborating decentralized clients with privacy considerations. Unlike standard distributed optimization, {FL} takes multiple optimization steps on local clients and then aggregates the model updates via a parameter server. Although this significantly reduces communication costs, the non-iid property across heterogeneous devices could make the local update diverge a lot, posing a fundamental challenge to aggregation. In this paper, we focus on a special kind of non-iid scene, i.e., label distribution skew, where each client can only access a partial set of the whole class set. Considering top layers of neural networks are more task-specific, we advocate that the last classification layer is more vulnerable to the shift of label distribution. Hence, we in-depth study the classifier layer and point out that the standard softmax will encounter several problems caused by missing classes. As an alternative, we propose “Restricted Softmax" to limit the update of missing classes’ weights during the local procedure. Our proposed {FedRS} is very easy to implement with only a few lines of code. We investigate our methods on both public datasets and a real-world service awareness application. Abundant experimental results verify the superiorities of our methods.},
	eventtitle = {{KDD} '21},
	pages = {995--1005},
	booktitle = {27th {ACM} {SIGKDD} Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Li, Xin-Chun and Zhan, De-Chuan},
	urldate = {2022-02-01},
	date = {2021-08-14},
	langid = {english},
}

@inproceedings{wu_federated_2021,
	location = {Virtual Event, Israel},
	title = {Federated Deep Knowledge Tracing},
	isbn = {978-1-4503-8297-7},
	url = {https://dl.acm.org/doi/10.1145/3437963.3441747},
	doi = {10.1145/3437963.3441747},
	abstract = {Knowledge tracing is a fundamental task in intelligent education for tracking the knowledge states of students on necessary concepts. In recent years, Deep Knowledge Tracing ({DKT}) utilizes recurrent neural networks to model student learning sequences. This approach has achieved significant success and has been widely used in many educational applications. However, in practical scenarios, it tends to suffer from the following critical problems due to data isolation: 1) Data scarcity. Educational data, which is usually distributed across different silos (e.g., schools), is difficult to gather. 2) Different data quality. Students in different silos have different learning schedules, which results in unbalanced learning records, meaning that it is necessary to evaluate the learning data quality independently for different silos. 3) Data incomparability. It is difficult to compare the knowledge states of students with different learning processes from different silos. Inspired by federated learning, in this paper, we propose a novel Federated Deep Knowledge Tracing ({FDKT}) framework to collectively train high-quality {DKT} models for multiple silos. In this framework, each client takes charge of training a distributed {DKT} model and evaluating data quality by leveraging its own local data, while a center server is responsible for aggregating models and updating the parameters for all the clients. In particular, in the client part, we evaluate data quality incorporating different education measurement theories, and we construct two quality-oriented implementations based on {FDKT}, i.e., {FDKTCTT} and {FDKTIRT}-where the means of data quality evaluation follow Classical Test Theory and Item Response Theory, respectively. Moreover, in the server part, we adopt hierarchical model interpolation to uptake local effects for model personalization. Extensive experiments on real-world datasets demonstrate the effectiveness and superiority of the {FDKT} framework.},
	eventtitle = {{WSDM} '21},
	pages = {662--670},
	booktitle = {14th {ACM} International Conference on Web Search and Data Mining},
	publisher = {{ACM}},
	author = {Wu, Jinze and Huang, Zhenya and Liu, Qi and Lian, Defu and Wang, Hao and Chen, Enhong and Ma, Haiping and Wang, Shijin},
	urldate = {2021-09-30},
	date = {2021-03-08},
	langid = {english},
}

@inproceedings{hong_federated_2021,
	location = {Virtual Event, Singapore},
	title = {Federated Adversarial Debiasing for Fair and Transferable Representations},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467281},
	doi = {10.1145/3447548.3467281},
	abstract = {Federated learning is a distributed learning framework that is communication efficient and provides protection over participating users’ raw training data. One outstanding challenge of federate learning comes from the users’ heterogeneity, and learning from such data may yield biased and unfair models for minority groups. While adversarial learning is commonly used in centralized learning for mitigating bias, there are significant barriers when extending it to the federated framework. In this work, we study these barriers and address them by proposing a novel approach Federated Adversarial {DEbiasing} ({FADE}). {FADE} does not require users’ sensitive group information for debiasing and offers users the freedom to optout from the adversarial component when privacy or computational costs become a concern. We show that ideally, {FADE} can attain the same global optimality as the one by the centralized algorithm. We then analyze when its convergence may fail in practice and propose a simple yet effective method to address the problem. Finally, we demonstrate the effectiveness of the proposed framework through extensive empirical studies, including the problem settings of unsupervised domain adaptation and fair learning. Our codes and pretrained models are available at: https://github.com/illidanlab/{FADE}.},
	eventtitle = {{KDD} '21},
	pages = {617--627},
	booktitle = {27th {ACM} {SIGKDD} Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Hong, Junyuan and Zhu, Zhuangdi and Yu, Shuyang and Wang, Zhangyang and Dodge, Hiroko H. and Zhou, Jiayu},
	urldate = {2022-02-01},
	date = {2021-08-14},
	langid = {english},
}

@inproceedings{yu_fed2_2021,
	location = {Virtual Event, Singapore},
	title = {Fed2: Feature-Aligned Federated Learning},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467309},
	doi = {10.1145/3447548.3467309},
	shorttitle = {Fed2},
	abstract = {Federated learning learns from scattered data by fusing collaborative models from local nodes. However, conventional coordinatebased model averaging by {FedAvg} ignored the random information encoded per parameter and may suffer from structural feature misalignment. In this work, we propose 𝐹𝑒𝑑2, a feature-aligned federated learning framework to resolve this issue by establishing a firm structure-feature alignment across the collaborative models. 𝐹𝑒𝑑2 is composed of two major designs: First, we design a feature-oriented model structure adaptation method to ensure explicit feature allocation in different neural network structures. Applying the structure adaptation to collaborative models, matchable structures with similar feature information can be initialized at the very early training stage. During the federated learning process, we then propose a feature paired averaging scheme to guarantee aligned feature distribution and maintain no feature fusion conflicts under either {IID} or non-{IID} scenarios. Eventually, 𝐹𝑒𝑑2 could effectively enhance the federated learning convergence performance under extensive homoand heterogeneous settings, providing excellent convergence speed, accuracy, and computation/communication efficiency.},
	eventtitle = {{KDD} '21},
	pages = {2066--2074},
	booktitle = {27th {ACM} {SIGKDD} Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Yu, Fuxun and Zhang, Weishan and Qin, Zhuwei and Xu, Zirui and Wang, Di and Liu, Chenchen and Tian, Zhi and Chen, Xiang},
	urldate = {2021-09-29},
	date = {2021-08-14},
	langid = {english},
}

@inproceedings{li_fair_2020,
	location = {Virtual Event, Ethiopia},
	title = {Fair Resource Allocation in Federated Learning},
	url = {https://openreview.net/forum?id=ByexElSYDr},
	eventtitle = {{ICLR} '20},
	booktitle = {International Conference on Learning Representations},
	author = {Li, Tian and Sanjabi, Maziar and Beirami, Ahmad and Smith, Virginia},
	date = {2020},
}

@inproceedings{baunsgaard_exdra_2021,
	location = {Virtual Event, China},
	title = {{ExDRa}: Exploratory Data Science on Federated Raw Data},
	isbn = {978-1-4503-8343-1},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457549},
	doi = {10.1145/3448016.3457549},
	series = {{SIGMOD} '21},
	shorttitle = {{ExDRa}},
	abstract = {Data science workflows are largely exploratory, dealing with underspecified objectives, open-ended problems, and unknown business value. Therefore, little investment is made in systematic acquisition, integration, and pre-processing of data. This lack of infrastructure results in redundant manual effort and computation. Furthermore, central data consolidation is not always technically or economically desirable or even feasible (e.g., due to privacy, and/or data ownership). The {ExDRa} system aims to provide system infrastructure for this exploratory data science process on federated and heterogeneous, raw data sources. Technical focus areas include (1) ad-hoc and federated data integration on raw data, (2) data organization and reuse of intermediates, and (3) optimization of the data science lifecycle, under awareness of partially accessible data. In this paper, we describe use cases, the overall system architecture, selected features of {SystemDS}’ new federated backend (for federated linear algebra programs, federated parameter servers, and federated data preparation), as well as promising initial results. Beyond existing work on federated learning, {ExDRa} focuses on enterprise federated {ML} and related data pre-processing challenges. In this context, federated {ML} has the potential to create a more fine-grained spectrum of data ownership and thus, even new markets.},
	eventtitle = {{SIGMOD} '21},
	pages = {2450--2463},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Baunsgaard, Sebastian and Boehm, Matthias and Chaudhary, Ankit and Derakhshan, Behrouz and Geißelsöder, Stefan and Grulich, Philipp M. and Hildebrand, Michael and Innerebner, Kevin and Markl, Volker and Neubauer, Claus and Osterburg, Sarah and Ovcharenko, Olga and Redyuk, Sergey and Rieger, Tobias and Rezaei Mahdiraji, Alireza and Wrede, Sebastian Benjamin and Zeuch, Steffen},
	urldate = {2021-08-23},
	date = {2021-06-09},
	langid = {english},
}

@inproceedings{kumar_automation_2021,
	location = {Virtual Event, China},
	title = {Automation of Data Prep, {ML}, and Data Science: New Cure or Snake Oil?},
	isbn = {978-1-4503-8343-1},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457537},
	doi = {10.1145/3448016.3457537},
	series = {{SIGMOD} '21},
	shorttitle = {Automation of Data Prep, {ML}, and Data Science},
	abstract = {As machine learning ({ML}), artificial intelligence ({AI}), and Data Science grow in practical importance, a large part of the {ML}/{AI} software industry claims to have built tools and platforms to automate the entire workflow of {ML}. That includes vexing problems of data preparation (prep), studied intensively by the database ({DB}) community for decades, with basically no resolution so far. Such claims by the {ML}/{AI} industry face a stunning lack of scientific scrutiny from the {DB} and {ML} research worlds, largely due to the lack of meaningful, large, and objective benchmarks. As such tools rapidly gain adoption among enterprises and other customers, this panel will debate whether the new {ML}/{AI} industry is basically selling “snake oil” to such users, how to evolve away from the status quo by instituting meaningful new benchmarks, creating new partnerships between industry and academia for this, and other pressing questions in this important arena. We aim to spur vigorous conversations that will hopefully lead to genuine new cures for an age-old affliction in Data Science.},
	eventtitle = {{SIGMOD} '21},
	pages = {2878--2880},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Kumar, Arun},
	urldate = {2021-11-11},
	date = {2021-06-09},
	langid = {english},
}

@inproceedings{zhang_asysqn_2021,
	location = {Virtual Event, Singapore},
	title = {{AsySQN}: Faster Vertical Federated Learning Algorithms with Better Computation Resource Utilization},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467169},
	doi = {10.1145/3447548.3467169},
	shorttitle = {{AsySQN}},
	abstract = {Vertical federated learning ({VFL}) is an effective paradigm of training the emerging cross-organizational (e.g., different corporations, companies and organizations) collaborative learning with privacy preserving. Stochastic gradient descent ({SGD}) methods are the popular choices for training {VFL} models because of the low per-iteration computation. However, existing {SGD}-based {VFL} algorithms are communication-expensive due to a large number of communication rounds. Meanwhile, most existing {VFL} algorithms use synchronous computation which seriously hamper the computation resource utilization in real-world applications. To address the challenges of communication and computation resource utilization, we propose an asynchronous stochastic quasi-Newton ({AsySQN}) framework for {VFL}, under which three algorithms, i.e. {AsySQN}-{SGD}, -{SVRG} and -{SAGA}, are proposed. The proposed {AsySQN}-type algorithms making descent steps scaled by approximate (without calculating the inverse Hessian matrix explicitly) Hessian information convergence much faster than {SGD}-based methods in practice and thus can dramatically reduce the number of communication rounds. Moreover, the adopted asynchronous computation can make better use of the computation resource. We theoretically prove the convergence rates of our proposed algorithms for strongly convex problems. Extensive numerical experiments on real-word datasets demonstrate the lower communication costs and better computation resource utilization of our algorithms compared with state-of-the-art {VFL} algorithms.},
	eventtitle = {{KDD} '21},
	pages = {3917--3927},
	booktitle = {27th {ACM} {SIGKDD} Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Zhang, Qingsong and Gu, Bin and Deng, Cheng and Gu, Songxiang and Bo, Liefeng and Pei, Jian and Huang, Heng},
	urldate = {2022-01-24},
	date = {2021-08-14},
	langid = {english},
}

@inproceedings{isenko_where_2022,
	location = {Philadelphia, {PA}},
	title = {Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines},
	isbn = {978-1-4503-9249-5},
	url = {https://dl.acm.org/doi/10.1145/3514221.3517848},
	doi = {10.1145/3514221.3517848},
	series = {{SIGMOD} '22},
	shorttitle = {Where Is My Training Bottleneck?},
	abstract = {Preprocessing pipelines in deep learning aim to provide sufficient data throughput to keep the training processes busy. Maximizing resource utilization is becoming more challenging as the throughput of training processes increases with hardware innovations (e.g., faster {GPUs}, {TPUs}, and inter-connects) and advanced parallelization techniques that yield better scalability. At the same time, the amount of training data needed in order to train increasingly complex models is growing. As a consequence of this development, data preprocessing and provisioning are becoming a severe bottleneck in end-to-end deep learning pipelines.},
	eventtitle = {{SIGMOD} '22},
	pages = {1825--1839},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Isenko, Alexander and Mayer, Ruben and Jedele, Jeffrey and Jacobsen, Hans-Arno},
	urldate = {2022-06-15},
	date = {2022-06-10},
	langid = {english},
}

@inproceedings{alotaibi_towards_2019,
	location = {Amsterdam, Netherlands},
	title = {Towards Scalable Hybrid Stores: Constraint-Based Rewriting to the Rescue},
	isbn = {978-1-4503-5643-5},
	url = {https://dl.acm.org/doi/10.1145/3299869.3319895},
	doi = {10.1145/3299869.3319895},
	series = {{SIGMOD} '19},
	shorttitle = {Towards Scalable Hybrid Stores},
	abstract = {Big data applications routinely involve diverse datasets: relations flat or nested, complex-structure graphs, documents, poorly structured logs, or even text data. To handle the data, application designers usually rely on several data stores used side-by-side, each capable of handling one or a few data models, and each very efficient for some, but not all, kinds of processing on the data. A current limitation is that applications are written taking into account which part of the data is stored in which store and how. This fails to take advantage of (i) possible redundancy, when the same data may be accessible (with different performance) from distinct data stores; (ii) partial query results (in the style of materialized views) which may be available in the stores.},
	eventtitle = {{SIGMOD} '19},
	pages = {1660--1677},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Alotaibi, Rana and Bursztyn, Damian and Deutsch, Alin and Manolescu, Ioana and Zampetakis, Stamatis},
	urldate = {2022-05-18},
	date = {2019-06-25},
	langid = {english},
}

@inproceedings{chen_towards_2019-1,
	location = {Amsterdam, Netherlands},
	title = {Towards Model-based Pricing for Machine Learning in a Data Marketplace},
	isbn = {978-1-4503-5643-5},
	url = {http://dl.acm.org/citation.cfm?doid=3299869.3300078},
	doi = {10.1145/3299869.3300078},
	series = {{SIGMOD} '19},
	abstract = {Data analytics using machine learning ({ML}) has become ubiquitous in science, business intelligence, journalism and many other domains. While a lot of work focuses on reducing the training cost, inference runtime and storage cost of {ML} models, little work studies how to reduce the cost of data acquisition, which potentially leads to a loss of sellers’ revenue and buyers’ affordability and efficiency. In this paper, we propose a model-based pricing ({MBP}) framework, which instead of pricing the data, directly prices {ML} model instances. We first formally describe the desired properties of the {MBP} framework, with a focus on avoiding arbitrage. Next, we show a concrete realization of the {MBP} framework via a noise injection approach, which provably satisfies the desired formal properties. Based on the proposed framework, we then provide algorithmic solutions on how the seller can assign prices to models under different market scenarios (such as to maximize revenue). Finally, we conduct extensive experiments, which validate that the {MBP} framework can provide high revenue to the seller, high affordability to the buyer, and also operate on low runtime cost.},
	eventtitle = {{SIGMOD} '19},
	pages = {1535--1552},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Chen, Lingjiao and Koutris, Paraschos and Kumar, Arun},
	urldate = {2020-08-31},
	date = {2019},
	langid = {english},
}

@inproceedings{shah_towards_2021,
	location = {Virtual Event, China},
	title = {Towards Benchmarking Feature Type Inference for {AutoML} Platforms},
	isbn = {978-1-4503-8343-1},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457274},
	doi = {10.1145/3448016.3457274},
	series = {{SIGMOD} '21},
	abstract = {The paradigm of {AutoML} has created an opportunity to enable {ML} for the masses. Emerging industrial-scale cloud {AutoML} platforms aim to automate the end-to-end {ML} workflow. While many works have looked into automated feature engineering, model selection, or hyper-parameter search in {AutoML}, little work has studied a crucial step that serves as an entry point to this workflow: {ML} feature type inference. The semantic gap between attribute types (e.g., strings, numbers) in databases/files and {ML} feature types (e.g., Numeric, Categorical) necessitates type inference. In this work, we formalize and standardize this task by creating the first ever benchmark labeled dataset, which we use to objectively evaluate existing {AutoML} tools. Our dataset has 9921 examples and a 9-class label vocabulary. Our labeled data also offers an alternative approach to automate this task than existing rule-based or syntax-based approaches: use {ML} itself to predict feature types. We collate a benchmark suite of 30 classification and regression tasks to assess the importance of type inference for downstream models. Empirical comparison on our labeled data shows that an {ML}-based approach delivers a lift of an average 14\% and up to 38\% in accuracy for identifying feature types compared to prominent industrial tools. Our downstream benchmark suite reveals that the {ML}-based approach outperforms existing industrial-strength tools for 47 out of 60 downstream models. We release our labeled dataset, models, and downstream benchmarks in a public repository with a leaderboard.},
	eventtitle = {{SIGMOD} '21},
	pages = {1584--1596},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Shah, Vraj and Lacanlale, Jonathan and Kumar, Premanand and Yang, Kevin and Kumar, Arun},
	urldate = {2022-08-09},
	date = {2021-06-09},
	langid = {english},
}

@inproceedings{smith_machine_2020,
	location = {Portland, {OR}},
	title = {The Machine Learning Bazaar: Harnessing the {ML} Ecosystem for Effective System Development},
	url = {http://arxiv.org/abs/1905.08942},
	doi = {10.1145/3318464.3386146},
	series = {{SIGMOD} '20},
	shorttitle = {The Machine Learning Bazaar},
	abstract = {As machine learning is applied more widely, data scientists often struggle to find or create end-to-end machine learning systems for specific tasks. The proliferation of libraries and frameworks and the complexity of the tasks have led to the emergence of “pipeline jungles” — brittle, ad hoc {ML} systems. To address these problems, we introduce the Machine Learning Bazaar, a new framework for developing machine learning and automated machine learning software systems. First, we introduce {ML} primitives, a unified {API} and specification for data processing and {ML} components from different software libraries. Next, we compose primitives into usable {ML} pipelines, abstracting away glue code, data flow, and data storage. We further pair these pipelines with a hierarchy of {AutoML} strategies — Bayesian optimization and bandit learning. We use these components to create a general-purpose, multi-task, end-to-end {AutoML} system that provides solutions to a variety of data modalities (image, text, graph, tabular, relational, etc.) and problem types (classification, regression, anomaly detection, graph matching, etc.). We demonstrate 5 real-world use cases and 2 case studies of our approach. Finally, we present an evaluation suite of 456 real-world {ML} tasks and describe the characteristics of 2.5 million pipelines searched over this task suite.},
	eventtitle = {{SIGMOD} '20},
	pages = {785--800},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Smith, Micah J. and Sala, Carles and Kanter, James Max and Veeramachaneni, Kalyan},
	urldate = {2020-08-31},
	date = {2020-06-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.08942},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Machine Learning},
}

@inproceedings{mishra_stretch_2008,
	location = {Vancouver, Canada},
	title = {Stretch 'n' Shrink: Resizing Queries to User Preferences},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?doid=1376616.1376741},
	doi = {10.1145/1376616.1376741},
	series = {{SIGMOD} '08},
	shorttitle = {Stretch 'n' shrink},
	abstract = {We present Stretch ‘n’ Shrink, a query design framework that explicitly takes into account user preferences about the desired answer size, and subsequently modiﬁes the query with user feedback to meet this target. Our system has been prototyped inside an open source data manager, and requires minimal modiﬁcations to the database engine.},
	eventtitle = {{SIGMOD} '08},
	pages = {1227--1230},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Mishra, Chaitanya and Koudas, Nick},
	urldate = {2022-12-22},
	date = {2008},
	langid = {english},
}

@inproceedings{armbrust_spark_2015,
	location = {Melbourne, Australia},
	title = {Spark {SQL}: Relational Data Processing in Spark},
	isbn = {978-1-4503-2758-9},
	url = {https://dl.acm.org/doi/10.1145/2723372.2742797},
	doi = {10.1145/2723372.2742797},
	series = {{SIGMOD} '15},
	shorttitle = {Spark {SQL}},
	abstract = {Spark {SQL} is a new module in Apache Spark that integrates relational processing with Spark’s functional programming {API}. Built on our experience with Shark, Spark {SQL} lets Spark programmers leverage the beneﬁts of relational processing (e.g., declarative queries and optimized storage), and lets {SQL} users call complex analytics libraries in Spark (e.g., machine learning). Compared to previous systems, Spark {SQL} makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative {DataFrame} {API} that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and deﬁne extension points. Using Catalyst, we have built a variety of features (e.g., schema inference for {JSON}, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark {SQL} as an evolution of both {SQL}-on-Spark and of Spark itself, offering richer {APIs} and optimizations while keeping the beneﬁts of the Spark programming model.},
	eventtitle = {{SIGMOD} '15},
	pages = {1383--1394},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Armbrust, Michael and Xin, Reynold S. and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K. and Meng, Xiangrui and Kaftan, Tomer and Franklin, Michael J. and Ghodsi, Ali and Zaharia, Matei},
	urldate = {2022-05-16},
	date = {2015-05-27},
	langid = {english},
}

@inproceedings{siddiqui_shapesearch_2020,
	location = {Portland, {OR}},
	title = {{ShapeSearch}: A Flexible and Efficient System for Shape-based Exploration of Trendlines},
	isbn = {978-1-4503-6735-6},
	url = {https://dl.acm.org/doi/10.1145/3318464.3389722},
	doi = {10.1145/3318464.3389722},
	series = {{SIGMOD} '20},
	shorttitle = {{ShapeSearch}},
	eventtitle = {{SIGMOD} '20},
	pages = {51--65},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Siddiqui, Tarique and Luh, Paul and Wang, Zesheng and Karahalios, Karrie and Parameswaran, Aditya},
	urldate = {2023-07-14},
	date = {2020-06-11},
	langid = {english},
}

@inproceedings{lutz_pump_2020,
	location = {Portland, {OR}},
	title = {Pump Up the Volume: Processing Large Data on {GPUs} with Fast Interconnects},
	isbn = {978-1-4503-6735-6},
	url = {https://dl.acm.org/doi/10.1145/3318464.3389705},
	doi = {10.1145/3318464.3389705},
	series = {{SIGMOD} '20},
	shorttitle = {Pump Up the Volume},
	abstract = {{GPUs} have long been discussed as accelerators for database query processing because of their high processing power and memory bandwidth. However, two main challenges limit the utility of {GPUs} for large-scale data processing: (1) the onboard memory capacity is too small to store large data sets, yet (2) the interconnect bandwidth to {CPU} main-memory is insufficient for ad hoc data transfers. As a result, {GPU}-based systems and algorithms run into a transfer bottleneck and do not scale to large data sets. In practice, {CPUs} process large-scale data faster than {GPUs} with current technology. In this paper, we investigate how a fast interconnect can resolve these scalability limitations using the example of {NVLink} 2.0. {NVLink} 2.0 is a new interconnect technology that links dedicated {GPUs} to a {CPU}. The high bandwidth of {NVLink} 2.0 enables us to overcome the transfer bottleneck and to efficiently process large data sets stored in main-memory on {GPUs}. We perform an in-depth analysis of {NVLink} 2.0 and show how we can scale a no-partitioning hash join beyond the limits of {GPU} memory. Our evaluation shows speedups of up to 18× over {PCI}-e 3.0 and up to 7.3× over an optimized {CPU} implementation. Fast {GPU} interconnects thus enable {GPUs} to efficiently accelerate query processing.},
	eventtitle = {{SIGMOD} '20},
	pages = {1633--1649},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Lutz, Clemens and Breß, Sebastian and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker},
	urldate = {2022-02-10},
	date = {2020-06-11},
	langid = {english},
}

@inproceedings{castro_fernandez_protecting_2022,
	location = {Philadelphia, {PA}},
	title = {Protecting Data Markets from Strategic Buyers},
	isbn = {978-1-4503-9249-5},
	url = {https://dl.acm.org/doi/10.1145/3514221.3517855},
	doi = {10.1145/3514221.3517855},
	series = {{SIGMOD} '22},
	abstract = {The growing adoption of data analytics platforms and machine learning-based solutions for decision-makers creates a significant demand for datasets, which explains the appearance of data markets. In a well-functioning data market, sellers share data in exchange for money, and buyers pay for datasets that help them solve problems. The market raises sufficient money to compensate sellers and incentivize them to keep sharing datasets. This low-friction matching of sellers and buyers distributes the value of data among participants. But designing online data markets is challenging because they must account for the strategic behavior of participants. In this paper, we introduce techniques to protect data markets from strategic participants, even when the asset traded is data. We combine those techniques into a pricing algorithm specifically designed to trade data. The evaluation includes a user study and extensive simulations. Together, the evaluation demonstrates how participants strategize and the effectiveness of our techniques.},
	eventtitle = {{SIGMOD} '22},
	pages = {1755--1769},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Castro Fernandez, Raul},
	urldate = {2022-08-10},
	date = {2022-06-10},
	langid = {english},
}

@inproceedings{nargesian_organizing_2020,
	location = {Portland, {OR}},
	title = {Organizing Data Lakes for Navigation},
	isbn = {978-1-4503-6735-6},
	url = {https://dl.acm.org/doi/10.1145/3318464.3380605},
	doi = {10.1145/3318464.3380605},
	series = {{SIGMOD} '20},
	abstract = {We consider the problem of creating an e�ective navigation structure over a data lake. We de�ne an organization as a navigation graph that contains nodes representing sets of attributes within a data lake and edges indicating subset relationships among nodes. We propose the data lake organization problem as the problem of� nding an organization that allows a user to most e�ectively navigate a data lake. We present a new probabilistic model of how users interact with an organization and propose an approximate algorithm for the data lake organization problem. We show the e�ectiveness of the algorithm on both a real data lake containing data from open data portals and on a benchmark that contains rich metadata emulating the observed characteristics of real data lakes. Through a formal user study, we show that navigation can help users� nd relevant tables that cannot be found by keyword search.},
	eventtitle = {{SIGMOD} '20},
	pages = {1939--1950},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Nargesian, Fatemeh and Pu, Ken Q. and Zhu, Erkang and Ghadiri Bashardoost, Bahar and Miller, Renée J.},
	urldate = {2022-08-17},
	date = {2020-06-11},
	langid = {english},
}

@inproceedings{soliman_orca_2014,
	location = {Snowbird, {UT}},
	title = {Orca: A Modular Query Optimizer Architecture for Big Data},
	isbn = {978-1-4503-2376-5},
	url = {https://dl.acm.org/doi/10.1145/2588555.2595637},
	doi = {10.1145/2588555.2595637},
	series = {{SIGMOD} '14},
	shorttitle = {Orca},
	abstract = {The performance of analytical query processing in data management systems depends primarily on the capabilities of the system’s query optimizer. Increased data volumes and heightened interest in processing complex analytical queries have prompted Pivotal to build a new query optimizer.},
	eventtitle = {{SIGMOD} '14},
	pages = {337--348},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Soliman, Mohamed A. and Antova, Lyublena and Raghavan, Venkatesh and El-Helw, Amr and Gu, Zhongxian and Shen, Entong and Caragea, George C. and Garcia-Alvarado, Carlos and Rahman, Foyzur and Petropoulos, Michalis and Waas, Florian and Narayanan, Sivaramakrishnan and Krikellas, Konstantinos and Baldwin, Rhonda},
	urldate = {2022-05-17},
	date = {2014-06-18},
	langid = {english},
}

@inproceedings{derakhshan_optimizing_2020,
	location = {Portland, {OR}},
	title = {Optimizing Machine Learning Workloads in Collaborative Environments},
	isbn = {978-1-4503-6735-6},
	url = {https://dl.acm.org/doi/10.1145/3318464.3389715},
	doi = {10.1145/3318464.3389715},
	series = {{SIGMOD} '20},
	abstract = {Effective collaboration among data scientists results in highquality and efficient machine learning ({ML}) workloads. In a collaborative environment, such as Kaggle or Google Colabratory, users typically re-execute or modify published scripts to recreate or improve the result. This introduces many redundant data processing and model training operations. Reusing the data generated by the redundant operations leads to the more efficient execution of future workloads. However, existing collaborative environments lack a data management component for storing and reusing the result of previously executed operations.},
	eventtitle = {{SIGMOD} '20},
	pages = {1701--1716},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Derakhshan, Behrouz and Rezaei Mahdiraji, Alireza and Abedjan, Ziawasch and Rabl, Tilmann and Markl, Volker},
	urldate = {2022-02-09},
	date = {2020-06-11},
	langid = {english},
}

@inproceedings{habich_morphstore_2019,
	location = {Amsterdam, Netherlands},
	title = {{MorphStore} - In-Memory Query Processing based on Morphing Compressed Intermediates {LIVE}},
	isbn = {978-1-4503-5643-5},
	url = {https://dl.acm.org/doi/10.1145/3299869.3320234},
	doi = {10.1145/3299869.3320234},
	series = {{SIGMOD}'19},
	abstract = {In this demo, we present {MorphStore}, an in-memory column store with a novel compression-aware query processing concept. Basically, compression using lightweight integer compression algorithms already plays an important role in existing in-memory column stores, but mainly for base data. The continuous handling of compression from the base data to the intermediate results during query processing has already been discussed, but not investigated in detail since the computational e ort for compression as well as decompression is often assumed to exceed the bene ts of a reduced transfer cost between {CPU} and main memory. However, this argument increasingly loses its validity as we are going to show in our demo. Generally, our novel compression-aware query processing concept is characterized by the fact that we are able to speed up the query execution by morphing compressed intermediate results from one scheme to another scheme to dynamically adapt to the changing data characteristics during query processing. Our morphing decisions are made using a cost-based approach.},
	eventtitle = {{SIGMOD}'19},
	pages = {1917--1920},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Habich, Dirk and Damme, Patrick and Ungethüm, Annett and Pietrzyk, Johannes and Krause, Alexander and Hildebrandt, Juliana and Lehner, Wolfgang},
	urldate = {2021-11-22},
	date = {2019-06-25},
	langid = {english},
}

@inproceedings{mouratidis_marrying_2021,
	location = {Virtual Event, China},
	title = {Marrying Top-k with Skyline Queries: Relaxing the Preference Input while Producing Output of Controllable Size},
	isbn = {978-1-4503-8343-1},
	url = {https://dl.acm.org/doi/10.1145/3448016.3457299},
	doi = {10.1145/3448016.3457299},
	series = {{SIGMOD} '21},
	shorttitle = {Marrying Top-k with Skyline Queries},
	abstract = {The two most common paradigms to identify records of preference in a multi-objective setting rely either on dominance (e.g., the skyline operator) or on a utility function defined over the records’ attributes (typically, using a top-𝑘 query). Despite their proliferation, each of them has its own palpable drawbacks. Motivated by these drawbacks, we identify three hard requirements for practical decision support, namely, personalization, controllable output size, and flexibility in preference specification. With these requirements as a guide, we combine elements from both paradigms and propose two new operators, {ORD} and {ORU}. We perform a qualitative study to demonstrate how they work, and evaluate their performance against adaptations of previous work that mimic their output.},
	eventtitle = {{SIGMOD} '21},
	pages = {1317--1330},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Mouratidis, Kyriakos and Li, Keming and Tang, Bo},
	urldate = {2023-10-25},
	date = {2021-06-09},
	langid = {english},
}

@inproceedings{zhu_josie_2019,
	location = {Amsterdam, Netherlands},
	title = {{JOSIE}: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes},
	isbn = {978-1-4503-5643-5},
	url = {https://dl.acm.org/doi/10.1145/3299869.3300065},
	doi = {10.1145/3299869.3300065},
	series = {{SIGMOD} '19},
	shorttitle = {{JOSIE}},
	abstract = {We present a new solution for finding joinable tables in massive data lakes: given a table and one join column, find tables that can be joined with the given table on the largest number of distinct values. The problem can be formulated as an overlap set similarity search problem by considering columns as sets and matching values as intersection between sets. Although set similarity search is well-studied in the field of approximate string search (e.g., fuzzy keyword search), the solutions are designed for and evaluated over sets of relatively small size (average set size rarely much over 100 and maximum set size in the low thousands) with modest dictionary sizes (the total number of distinct values in all sets is only a few million). We observe that modern data lakes typically have massive set sizes (with maximum set sizes that may be tens of millions) and dictionaries that include hundreds of millions of distinct values. Our new algorithm, {JOSIE} (Joining Search using Intersection Estimation) minimizes the cost of set reads and inverted index probes used in finding the top-k sets. We show that {JOSIE} completely out performs the state-of-the-art overlap set similarity search techniques on data lakes. More surprising, we also consider state-of-the-art approximate algorithm and show that our new exact search algorithm performs almost as well, and even in some cases better, on real data lakes.},
	eventtitle = {{SIGMOD} '19},
	pages = {847--864},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Zhu, Erkang and Deng, Dong and Nargesian, Fatemeh and Miller, Renée J.},
	urldate = {2023-07-06},
	date = {2019-06-25},
	keywords = {data discovery, data lakes, joinable table search, set similarity search},
}

@inproceedings{mottin_iqr_2014,
	location = {Snowbird, {UT}},
	title = {{IQR}: An Interactive Query Relaxation System for the Empty-Answer Problem},
	isbn = {978-1-4503-2376-5},
	url = {https://dl.acm.org/doi/10.1145/2588555.2594512},
	doi = {10.1145/2588555.2594512},
	series = {{SIGMOD} '14},
	shorttitle = {{IQR}},
	abstract = {We present {IQR}, a system that demonstrates optimization based interactive relaxations for queries that return an empty answer. Given an empty answer, {IQR} dynamically suggests one relaxation of the original query conditions at a time to the user, based on certain optimization objectives, and the user responds by either accepting or declining the relaxation, until the user arrives at a non-empty answer, or a non-empty answer is impossible to achieve with any further relaxations. The relaxation suggestions hinge on a probabilistic framework that takes into account the probability of the user accepting a suggested relaxation, as well as how much that relaxation serves towards the optimization objective. {IQR} accepts a wide variety of optimization objectives - user centric objectives, such as, minimizing the number of user interactions (i.e., e↵ort) or returning relevant results, as well as seller centric objectives, such as, maximizing proﬁt. {IQR} o↵ers principled exact and approximate solutions for generating relaxations that are demonstrated using multiple, large real datasets.},
	eventtitle = {{SIGMOD} '14},
	pages = {1095--1098},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Mottin, Davide and Marascu, Alice and Basu Roy, Senjuti and Das, Gautam and Palpanas, Themis and Velegrakis, Yannis},
	urldate = {2022-12-07},
	date = {2014-06-18},
	langid = {english},
}

@inproceedings{halevy_goods_2016,
	location = {San Francisco, {CA}},
	title = {Goods: Organizing Google's Datasets},
	isbn = {978-1-4503-3531-7},
	url = {http://dl.acm.org/citation.cfm?doid=2882903.2903730},
	doi = {10.1145/2882903.2903730},
	series = {{SIGMOD} '16},
	shorttitle = {Goods},
	abstract = {Enterprises increasingly rely on structured datasets to run their businesses. These datasets take a variety of forms, such as structured ﬁles, databases, spreadsheets, or even services that provide access to the data. The datasets often reside in diﬀerent storage systems, may vary in their formats, may change every day. In this paper, we present Goods, a project to rethink how we organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them. Goods extracts metadata ranging from salient information about each dataset (owners, timestamps, schema) to relationships among datasets, such as similarity and provenance. It then exposes this metadata through services that allow engineers to ﬁnd datasets within the company, to monitor datasets, to annotate them in order to enable others to use their datasets, and to analyze relationships between them. We discuss the technical challenges that we had to overcome in order to crawl and infer the metadata for billions of datasets, to maintain the consistency of our metadata catalog at scale, and to expose the metadata to users. We believe that many of the lessons that we learned are applicable to building large-scale enterprise-level datamanagement systems in general.},
	eventtitle = {{SIGMOD} '16},
	pages = {795--806},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Halevy, Alon and Korn, Flip and Noy, Natalya F. and Olston, Christopher and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong},
	urldate = {2020-08-31},
	date = {2016},
	langid = {english},
}

@inproceedings{zhang_finding_2020,
	location = {Portland, {OR}},
	title = {Finding Related Tables in Data Lakes for Interactive Data Science},
	isbn = {978-1-4503-6735-6},
	url = {https://dl.acm.org/doi/10.1145/3318464.3389726},
	doi = {10.1145/3318464.3389726},
	series = {{SIGMOD} '20},
	abstract = {Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.},
	eventtitle = {{SIGMOD} '20},
	pages = {1951--1966},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Zhang, Yi and Ives, Zachary G.},
	urldate = {2022-08-17},
	date = {2020-06-11},
	langid = {english},
}

@inproceedings{haas_extensible_1989,
	location = {Portland, {OR}},
	title = {Extensible Query Processing in Starburst},
	isbn = {0-89791-317-5},
	url = {https://doi.org/10.1145/67544.66962},
	doi = {10.1145/67544.66962},
	series = {{SIGMOD} '89},
	abstract = {Today's {DBMSs} are unable to support the increasing demands of the various applications that would like to use a {DBMS}. Each kind of application poses new requirements for the {DBMS}. The Starburst project at {IBM}'s Almaden Research Center aims to extend relational {DBMS} technology to bridge this gap between applications and the {DBMS}. While providing a full function relational system to enable sharing across applications, Starburst will also allow (sophisticated) programmers to add many kinds of extensions to the base system's capabilities, including language extensions (e.g., new datatypes and operations), data management extensions (e.g., new access and storage methods) and internal processing extensions (e.g., new join methods and new query transformations). To support these features, the database query language processor must be very powerful and highly extensible. Starburst's language processor features a powerful query language, rule-based optimization and query rewrite, and an execution system based on an extended relational algebra. In this paper, we describe the design of Starburst's query language processor and discuss the ways in which the language processor can be extended to achieve Starburst's goals.},
	eventtitle = {{SIGMOD} '89},
	pages = {377--388},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Haas, Laura M. and Freytag, Johann C. and Lohman, Guy M. and Pirahesh, Hamid},
	date = {1989},
}

@inproceedings{an_avoiding_2022,
	location = {Philadelphia, {PA}},
	title = {Avoiding Read Stalls on Flash Storage},
	isbn = {978-1-4503-9249-5},
	url = {https://dl.acm.org/doi/10.1145/3514221.3526126},
	doi = {10.1145/3514221.3526126},
	series = {{SIGMOD} '22},
	abstract = {When a dirty victim page is selected for replacement upon page miss, the buffer manager has to first flush the dirty victim to the storage before reading the missing page. This conventional read-after-write ({RAW}) protocol, while working well on hard disks, causes the problem of read stall on flash storage with asymmetric read-write speed and parallelism; because of the resource conflict for a buffer frame between write and read operations, a page-missing process has to wait for the slow write to complete to secure a clean frame for the missing page. This strict write-then-read serialization under-utilizes {CPU} and storage, worsening transaction throughput and latency.},
	eventtitle = {{SIGMOD} '22},
	pages = {1404--1417},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {An, Mijin and Song, In-Yeong and Song, Yong-Ho and Lee, Sang-Won},
	urldate = {2022-06-15},
	date = {2022-06-10},
	langid = {english},
}

@inproceedings{camacho-rodriguez_apache_2019,
	location = {Amsterdam, Netherlands},
	title = {Apache Hive: From {MapReduce} to Enterprise-grade Big Data Warehousing},
	isbn = {978-1-4503-5643-5},
	url = {https://dl.acm.org/doi/10.1145/3299869.3314045},
	doi = {10.1145/3299869.3314045},
	series = {{SIGMOD} '19},
	shorttitle = {Apache Hive},
	abstract = {Apache Hive is an open-source relational database system for analytic big-data workloads. In this paper we describe the key innovations on the journey from batch tool to fully fledged enterprise data warehousing system. We present a hybrid architecture that combines traditional {MPP} techniques with more recent big data and cloud concepts to achieve the scale and performance required by today’s analytic applications. We explore the system by detailing enhancements along four main axis: Transactions, optimizer, runtime, and federation. We then provide experimental results to demonstrate the performance of the system for typical workloads and conclude with a look at the community roadmap.},
	eventtitle = {{SIGMOD} '19},
	pages = {1773--1786},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Camacho-Rodríguez, Jesús and Chauhan, Ashutosh and Gates, Alan and Koifman, Eugene and O'Malley, Owen and Garg, Vineet and Haindrich, Zoltan and Shelukhin, Sergey and Jayachandran, Prasanth and Seth, Siddharth and Jaiswal, Deepak and Bouguerra, Slim and Bangarwa, Nishant and Hariappan, Sankar and Agarwal, Anishek and Dere, Jason and Dai, Daniel and Nair, Thejas and Dembla, Nita and Vijayaraghavan, Gopal and Hagleitner, Günther},
	urldate = {2022-05-10},
	date = {2019-06-25},
	langid = {english},
}

@inproceedings{begoli_apache_2018,
	location = {Houston, {TX}},
	title = {Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources},
	isbn = {978-1-4503-4703-7},
	url = {https://dl.acm.org/doi/10.1145/3183713.3190662},
	doi = {10.1145/3183713.3190662},
	series = {{SIGMOD} '18},
	shorttitle = {Apache Calcite},
	abstract = {Apache Calcite is a foundational software framework that provides query processing, optimization, and query language support to many popular open-source data processing systems such as Apache Hive, Apache Storm, Apache Flink, Druid, and {MapD}. The goal of this paper is to formally introduce Calcite to the broader research community, briefly present its history, and describe its architecture, features, functionality, and patterns for adoption. Calcite’s architecture consists of a modular and extensible query optimizer with hundreds of built-in optimization rules, a query processor capable of processing a variety of query languages, an adapter architecture designed for extensibility, and support for heterogeneous data models and stores (relational, semi-structured, streaming, and geospatial). This flexible, embeddable, and extensible architecture is what makes Calcite an attractive choice for adoption in big-data frameworks. It is an active project that continues to introduce support for the new types of data sources, query languages, and approaches to query processing and optimization.},
	eventtitle = {{SIGMOD} '18},
	pages = {221--230},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Begoli, Edmon and Camacho-Rodríguez, Jesús and Hyde, Julian and Mior, Michael J. and Lemire, Daniel},
	urldate = {2022-05-16},
	date = {2018-05-27},
	langid = {english},
}

@inproceedings{kolev_cloudmdsql_2016,
	location = {San Francisco, {CA}},
	title = {The {CloudMdsQL} Multistore System},
	isbn = {978-1-4503-3531-7},
	url = {https://dl.acm.org/doi/10.1145/2882903.2899400},
	doi = {10.1145/2882903.2899400},
	series = {{SIGMOD} '16},
	abstract = {The blooming of different cloud data management infrastructures has turned multistore systems to a major topic in the nowadays cloud landscape. In this demonstration, we present a Cloud Multidatastore Query Language ({CloudMdsQL}), and its query engine. {CloudMdsQL} is a functional {SQL}-like language, capable of querying multiple heterogeneous data stores (relational and {NoSQL}) within a single query that may contain embedded invocations to each data store’s native query interface. The major innovation is that a {CloudMdsQL} query can exploit the full power of local data stores, by simply allowing some local data store native queries (e.g. a breadth-first search query against a graph database) to be called as functions, and at the same time be optimized.},
	eventtitle = {{SIGMOD} '16},
	pages = {2113--2116},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Kolev, Boyan and Bondiombouy, Carlyna and Valduriez, Patrick and Jimenez-Peris, Ricardo and Pau, Raquel and Pereira, José},
	urldate = {2022-05-18},
	date = {2016-06-26},
	langid = {english},
}

@inproceedings{olston_pig_2008,
	location = {Vancouver, Canada},
	title = {Pig Latin: A Not-so-Foreign Language for Data Processing},
	isbn = {978-1-60558-102-6},
	url = {http://portal.acm.org/citation.cfm?doid=1376616.1376726},
	doi = {10.1145/1376616.1376726},
	series = {{SIGMOD} '08},
	shorttitle = {Pig latin},
	abstract = {There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, oﬀer a solution, but are usually prohibitively expensive at this scale. Besides, many of the people who analyze this data are entrenched procedural programmers, who ﬁnd the declarative, {SQL} style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse.},
	eventtitle = {{SIGMOD} '08},
	pages = {1099--1110},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Olston, Christopher and Reed, Benjamin and Srivastava, Utkarsh and Kumar, Ravi and Tomkins, Andrew},
	urldate = {2022-05-18},
	date = {2008},
	langid = {english},
}

@inproceedings{kanne_histograms_2010,
	location = {Indianapolis, {IN}},
	title = {Histograms Reloaded: The Merits of Bucket Diversity},
	isbn = {978-1-4503-0032-2},
	url = {https://dl.acm.org/doi/10.1145/1807167.1807239},
	doi = {10.1145/1807167.1807239},
	series = {{SIGMOD} '10},
	shorttitle = {Histograms Reloaded},
	abstract = {Virtually all histograms store for each bucket the number of distinct values it contains and their average frequency. In this paper, we question this paradigm. We start out by investigating the estimation precision of three commercial database systems which also follow the above paradigm. It turns out that huge errors are quite common. We then introduce new bucket types and investigate their accuracy when building optimal histograms with them. The results are ambiguous. There is no clear winner among the bucket types. At this point, we (1) switch to heterogeneous histograms, where different buckets of the same histogram possibly are of different types, and (2) design more bucket types. The nice consequence of introducing heterogeneous histograms is that we can guarantee decent upper error bounds while at the same time heterogeneous histograms require far less space than homogeneous histograms.},
	eventtitle = {{SIGMOD} '10},
	pages = {663--674},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Kanne, Carl-Christian and Moerkotte, Guido},
	urldate = {2023-08-16},
	date = {2010-06-06},
	keywords = {cardinality estimation, histograms},
}

@inproceedings{galhotra_dataprism_2022,
	location = {Philadelphia, {PA}},
	title = {{DataPrism}: Exposing Disconnect between Data and Systems},
	isbn = {978-1-4503-9249-5},
	url = {https://doi.org/10.1145/3514221.3517864},
	doi = {10.1145/3514221.3517864},
	series = {{SIGMOD} '22},
	shorttitle = {{DataPrism}},
	abstract = {As data is a central component of many modern systems, the cause of a system malfunction may reside in the data, and, specifically, particular properties of data. E.g., a health-monitoring system that is designed under the assumption that weight is reported in lbs will malfunction when encountering weight reported in kilograms. Like software debugging, which aims to find bugs in the source code or runtime conditions, our goal is to debug data to identify potential sources of disconnect between the assumptions about some data and systems that operate on that data. We propose {DataPrism}, a framework to identify data properties (profiles) that are the root causes of performance degradation or failure of a data-driven system. Such identification is necessary to repair data and resolve the disconnect between data and systems. Our technique is based on causal reasoning through interventions: when a system malfunctions for a dataset, {DataPrism} alters the data profiles and observes changes in the system's behavior due to the alteration. Unlike statistical observational analysis that reports mere correlations, {DataPrism} reports causally verified root causes -- in terms of data profiles -- of the system malfunction. We empirically evaluate {DataPrism} on seven real-world and several synthetic data-driven systems that fail on certain datasets due to a diverse set of reasons. In all cases, {DataPrism} identifies the root causes precisely while requiring orders of magnitude fewer interventions than prior techniques.},
	eventtitle = {{SIGMOD} '22},
	pages = {217--231},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Galhotra, Sainyam and Fariha, Anna and Lourenço, Raoni and Freire, Juliana and Meliou, Alexandra and Srivastava, Divesh},
	urldate = {2024-07-02},
	date = {2022-06-11},
}

@inproceedings{lenzerini_data_2002,
	location = {New York, {NY}},
	title = {Data Integration: A Theoretical Perspective},
	isbn = {1-58113-507-6},
	url = {https://doi.org/10.1145/543613.543644},
	doi = {10.1145/543613.543644},
	abstract = {Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.},
	eventtitle = {{PODS} '02},
	pages = {233--246},
	booktitle = {21st {ACM} Symposium on Principles of Database Systems},
	publisher = {{ACM} Press},
	author = {Lenzerini, Maurizio},
	date = {2002},
	note = {event-place: Madison, Wisconsin},
}

@inproceedings{castro_fernandez_demo_2017,
	location = {Chicago, {IL}},
	title = {A Demo of the Data Civilizer System},
	isbn = {978-1-4503-4197-4},
	url = {https://dl.acm.org/doi/10.1145/3035918.3058740},
	doi = {10.1145/3035918.3058740},
	series = {{SIGMOD} '17},
	abstract = {Finding relevant data for a speciﬁc task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80\%) of their eﬀort is spent ﬁnding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate Data Civilizer to ease the pain faced in analyzing data “in the wild”. Data Civilizer is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.},
	eventtitle = {{SIGMOD} '17},
	pages = {1639--1642},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Castro Fernandez, Raul and Deng, Dong and Mansour, Essam and Qahtan, Abdulhakim A. and Tao, Wenbo and Abedjan, Ziawasch and Elmagarmid, Ahmed and Ilyas, Ihab F. and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
	urldate = {2022-08-10},
	date = {2017-05-09},
	langid = {english},
}

@article{bernstein_vision_2000,
	title = {A Vision for Management of Complex Models},
	volume = {29},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/369275.369289},
	doi = {10.1145/369275.369289},
	abstract = {Many problems encountered when building applications of database systems involve the manipulation of models. By "model," we mean a complex structure that represents a design artifact, such as a relational schema, objectoriented interface, {UML} model, {\textasciitilde} r {\textasciitilde} {DTD}, web-site schema, semantic network, complex document, or software configuration. Many uses of models involve managing changes in models and transformations of data from one model into another. These uses require an explicit representation of "mappings" between models. We propose to make database systems easier to use for these applications by making "model" and "model mapping" first-class objects with special operations that simplify their use. We call this capability model management.},
	pages = {55--63},
	number = {4},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Bernstein, Phillip A. and Halevy, Alon Y. and Pottinger, Rachel A.},
	urldate = {2022-04-11},
	date = {2000-12},
	langid = {english},
}

@inproceedings{kaoudi_cost-based_2017,
	location = {Chicago, {IL}},
	title = {A Cost-based Optimizer for Gradient Descent Optimization},
	isbn = {978-1-4503-4197-4},
	url = {http://dl.acm.org/citation.cfm?doid=3035918.3064042},
	doi = {10.1145/3035918.3064042},
	series = {{SIGMOD} '17},
	abstract = {As the use of machine learning ({ML}) permeates into diverse application domains, there is an urgent need to support a declarative framework for {ML}. Ideally, a user will specify an {ML} task in a high-level and easy-to-use language and the framework will invoke the appropriate algorithms and system conﬁgurations to execute it. An important observation towards designing such a framework is that many {ML} tasks can be expressed as mathematical optimization problems, which take a speciﬁc form. Furthermore, these optimization problems can be eﬃciently solved using variations of the gradient descent ({GD}) algorithm. Thus, to decouple a user speciﬁcation of an {ML} task from its execution, a key component is a {GD} optimizer. We propose a cost-based {GD} optimizer that selects the best {GD} plan for a given {ML} task. To build our optimizer, we introduce a set of abstract operators for expressing {GD} algorithms and propose a novel approach to estimate the number of iterations a {GD} algorithm requires to converge. Extensive experiments on real and synthetic datasets show that our optimizer not only chooses the best {GD} plan but also allows for optimizations that achieve orders of magnitude performance speed-up.},
	eventtitle = {{SIGMOD} '17},
	pages = {977--992},
	booktitle = {{ACM} International Conference on Management of Data},
	publisher = {{ACM} Press},
	author = {Kaoudi, Zoi and Quiane-Ruiz, Jorge-Arnulfo and Thirumuruganathan, Saravanan and Chawla, Sanjay and Agrawal, Divy},
	urldate = {2020-09-07},
	date = {2017},
	langid = {english},
}

@inproceedings{gong_ver_2023,
	location = {Anaheim, {CA}},
	title = {Ver: View Discovery in the Wild},
	url = {https://ieeexplore.ieee.org/abstract/document/10184768},
	doi = {10.1109/ICDE55515.2023.00045},
	series = {{ICDE} '23},
	shorttitle = {Ver},
	abstract = {We present Ver1, a data discovery system that identifies project-join views over large repositories of tables that do not contain join path information, and even when input queries are inaccurate. Ver implements a reference architecture to solve both the technical (scale and search) and human (semantic ambiguity, navigating a large number of results) problems of view discovery. We demonstrate users find the view they want when using Ver with a user study and we demonstrate its performance with large-scale end-to-end experiments on real-world datasets containing tens of millions of join paths.},
	eventtitle = {{ICDE} '23},
	pages = {503--516},
	booktitle = {39th {IEEE} International Conference on Data Engineering},
	author = {Gong, Yue and Zhu, Zhiru and Galhotra, Sainyam and Fernandez, Raul Castro},
	urldate = {2024-01-18},
	date = {2023-04},
	note = {{ISSN}: 2375-026X},
}

@inproceedings{koutras_valentine_2021,
	title = {Valentine: Evaluating Matching Techniques for Dataset Discovery},
	doi = {10.1109/ICDE51399.2021.00047},
	series = {{ICDE} '21},
	shorttitle = {Valentine},
	abstract = {Data scientists today search large data lakes to discover and integrate datasets. In order to bring together disparate data sources, dataset discovery methods rely on some form of schema matching: the process of establishing correspondences between datasets. Traditionally, schema matching has been used to find matching pairs of columns between a source and a target schema. However, the use of schema matching in dataset discovery methods differs from its original use. Nowadays schema matching serves as a building block for indicating and ranking inter-dataset relationships. Surprisingly, although a discovery method’s success relies highly on the quality of the underlying matching algorithms, the latest discovery methods employ existing schema matching algorithms in an ad-hoc fashion due to the lack of openly-available datasets with ground truth, reference method implementations, and evaluation metrics.In this paper, we aim to rectify the problem of evaluating the effectiveness and efficiency of schema matching methods for the specific needs of dataset discovery. To this end, we propose Valentine, an extensible open-source experiment suite to execute and organize large-scale automated matching experiments on tabular data. Valentine includes implementations of seminal schema matching methods that we either implemented from scratch (due to absence of open source code) or imported from open repositories. The contributions of Valentine are: i) the definition of four schema matching scenarios as encountered in dataset discovery methods, ii) a principled dataset fabrication process tailored to the scope of dataset discovery methods and iii) the most comprehensive evaluation of schema matching techniques to date, offering insight on the strengths and weaknesses of existing techniques, that can serve as a guide for employing schema matching in future dataset discovery methods.},
	eventtitle = {{ICDE} '21},
	pages = {468--479},
	booktitle = {37th {IEEE} International Conference on Data Engineering},
	author = {Koutras, Christos and Siachamis, George and Ionescu, Andra and Psarakis, Kyriakos and Brons, Jerry and Fragkoulis, Marios and Lofi, Christoph and Bonifati, Angela and Katsifodimos, Asterios},
	date = {2021-04},
	note = {{ISSN}: 2375-026X},
	keywords = {Conferences, Data engineering, Fabrication, Lakes, Open source software},
}

@inproceedings{behme_art_2023,
	location = {Anaheim, {CA}, {USA}},
	title = {The Art of Losing to Win: Using Lossy Image Compression to Improve Data Loading in Deep Learning Pipelines},
	doi = {10.1109/ICDE55515.2023.00077},
	series = {{ICDE} '23},
	shorttitle = {The Art of Losing to Win},
	abstract = {Training deep learning ({DL}) models often takes a significant amount of time and is thus typically performed on expensive {GPUs} to speed up the process. However, data loading has recently been identified as one of the main performance bottlenecks in {DL}, resulting in {GPU} under-utilization. Looking forward, the combination of larger datasets and faster {GPUs} will exacerbate the problem. The data management community has started to address this by proposing data loading optimization techniques, including lossy image compression. While lossy compression is a conceptually promising approach for mitigating data loading bottlenecks in {DL}, there is only limited understanding of its efficacy in terms of impact on model throughput and accuracy. In this paper, we present an extensive experimental analysis of lossy image compression as a means to improve the performance of neural network training. We find that lossy compression can improve both throughput and accuracy of {DL} pipelines if resources such as time or storage capacity are limited. Furthermore, the choice of compression quality and codec are important hyperparameters when training deep neural networks.},
	eventtitle = {{ICDE} '23},
	pages = {936--949},
	booktitle = {39th {IEEE} International Conference on Data Engineering},
	author = {Behme, Lennart and Thirumuruganathan, Saravanan and Mahdiraji, Alireza Rezaei and Quiané-Ruiz, Jorge-Arnulfo and Markl, Volker},
	date = {2023},
}

@inproceedings{castro_fernandez_seeping_2018,
	location = {Paris, France},
	title = {Seeping Semantics: Linking Datasets Using Word Embeddings for Data Discovery},
	doi = {10.1109/ICDE.2018.00093},
	series = {{ICDE} '18},
	shorttitle = {Seeping Semantics},
	abstract = {Employees that spend more time finding relevant data than analyzing it suffer from a data discovery problem. The large volume of data in enterprises, and sometimes the lack of knowledge of the schemas aggravates this problem. Similar to how we navigate the Web, we propose to identify semantic links that assist analysts in their discovery tasks. These links relate tables to each other, to facilitate navigating the schemas. They also relate data to external data sources, such as ontologies and dictionaries, to help explain the schema meaning. We materialize the links in an enterprise knowledge graph, where they become available to analysts. The main challenge is how to find pairs of objects that are semantically related. We propose {SEMPROP}, a {DAG} of different components that find links based on syntactic and semantic similarities. {SEMPROP} is commanded by a semantic matcher which leverages word embeddings to find objects that are semantically related. We introduce coherent group, a technique to combine word embeddings that works better than other state of the art combination alternatives. We implement {SEMPROP} as part of Aurum, a data discovery system we are building, and conduct user studies, real deployments and a quantitative evaluation to understand the benefits of links for data discovery tasks, as well as the benefits of {SEMPROP} and coherent groups to find those links.},
	eventtitle = {{ICDE} '18},
	pages = {989--1000},
	booktitle = {34th {IEEE} International Conference on Data Engineering},
	author = {Castro Fernandez, Raul and Mansour, Essam and Qahtan, Abdulhakim A. and Elmagarmid, Ahmed and Ilyas, Ihab and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
	date = {2018-04},
	note = {{ISSN}: 2375-026X},
	keywords = {Databases, Drugs, Ontologies, Proteins, Semantics, Syntactics, Task analysis, data discovery, word embeddings},
}

@inproceedings{sethi_presto_2019,
	location = {Macao, China},
	title = {Presto: {SQL} on Everything},
	doi = {10.1109/ICDE.2019.00196},
	shorttitle = {Presto},
	abstract = {Presto is an open source distributed query engine that supports much of the {SQL} analytics workload at Facebook. Presto is designed to be adaptive, flexible, and extensible. It supports a wide variety of use cases with diverse characteristics. These range from user-facing reporting applications with sub-second latency requirements to multi-hour {ETL} jobs that aggregate or join terabytes of data. Presto's Connector {API} allows plugins to provide a high performance I/O interface to dozens of data sources, including Hadoop data warehouses, {RDBMSs}, {NoSQL} systems, and stream processing systems. In this paper, we outline a selection of use cases that Presto supports at Facebook. We then describe its architecture and implementation, and call out features and performance optimizations that enable it to support these use cases. Finally, we present performance results that demonstrate the impact of our main design decisions.},
	eventtitle = {{ICDE} '19},
	pages = {1802--1813},
	booktitle = {35th {IEEE} International Conference on Data Engineering},
	author = {Sethi, Raghav and Traverso, Martin and Sundstrom, Dain and Phillips, David and Xie, Wenlei and Sun, Yutian and Yegitbasi, Nezih and Jin, Haozhun and Hwang, Eric and Shingte, Nileema and Berner, Christopher},
	date = {2019-04},
	note = {{ISSN}: 2375-026X},
	keywords = {Connectors, Data warehouses, Engines, Facebook, Optimization, {SQL}, Structured Query Language, Tools, big data, data warehouse, query engine},
}

@inproceedings{kruse_optimizing_2019,
	location = {Macao, Macao},
	title = {Optimizing Cross-Platform Data Movement},
	isbn = {978-1-5386-7474-1},
	url = {https://ieeexplore.ieee.org/document/8731354/},
	doi = {10.1109/ICDE.2019.00162},
	series = {{ICDE} '19},
	abstract = {Data analytics are moving beyond the limits of a single data processing platform. A cross-platform query optimizer is necessary to enable applications to run their tasks over multiple platforms efﬁciently and in a platform-agnostic manner. For the optimizer to be effective, it must consider data movement costs across different data processing platforms. In this paper, we present the graph-based data movement strategy used by {RHEEM}, our open-source cross-platform system. In particular, we (i) model the data movement problem as a new graph problem, which we prove to be {NP}-hard, and (ii) propose a novel graph exploration algorithm, which allows {RHEEM} to discover multiple hidden opportunities for cross-platform data processing.},
	eventtitle = {{ICDE} '19},
	pages = {1642--1645},
	booktitle = {35th {IEEE} International Conference on Data Engineering},
	publisher = {{IEEE}},
	author = {Kruse, Sebastian and Kaoudi, Zoi and Quiane-Ruiz, Jorge-Arnulfo and Chawla, Sanjay and Naumann, Felix and Contreras-Rojas, Bertty},
	urldate = {2020-08-25},
	date = {2019-04},
	langid = {english},
}

@inproceedings{santos_sketch-based_2022,
	title = {A Sketch-based Index for Correlated Dataset Search},
	doi = {10.1109/ICDE53745.2022.00264},
	series = {{ICDE} '22},
	abstract = {Dataset search is emerging as a critical capability in both research and industry: it has spurred many novel applications, ranging from the enrichment of analyses of real-world phenomena to the improvement of machine learning models. Recent research in this field has explored a new class of data-driven queries: queries consist of datasets and retrieve, from a large collection, related datasets. In this paper, we study a specific type of data-driven query that supports relational data augmentation through numerical data relationships: given an input query table, find the top-k tables that are both joinable with it and contain columns that are correlated with a column in the query. We propose a novel hashing scheme that allows the construction of a sketch-based index to support efficient correlated table search. We show that our proposed approach is effective and efficient, and achieves better trade-offs that significantly improve both the ranking accuracy and recall compared to the state-of-the-art solutions.},
	eventtitle = {{ICDE} '22},
	pages = {2928--2941},
	booktitle = {38th {IEEE} International Conference on Data Engineering},
	author = {Santos, Aécio and Bessa, Aline and Musco, Christopher and Freire, Juliana},
	date = {2022-05},
	note = {{ISSN}: 2375-026X},
	keywords = {Correlation, Data visualization, Harmonic analysis, Industries, Machine learning, Market research, Search engines, dataset search, sketching, table search},
}

@inproceedings{wang_efficient_2021,
	location = {Chania, Greece},
	title = {An Efficient Approach for Cross-Silo Federated Learning to Rank},
	doi = {10.1109/ICDE51399.2021.00102},
	series = {{ICDE} '21},
	abstract = {Traditional learning-to-rank ({LTR}) models are usually trained in a centralized approach based upon a large amount of data. However, with the increasing awareness of data privacy, it is harder to collect data from multiple owners as before, and the resultant data isolation problem makes the performance of learned {LTR} models severely compromised. Inspired by the recent progress in federated learning, we propose a novel framework named Cross-Silo Federated Learning-to-Rank ({CS}-F-{LTR}), where the efficiency issue becomes the major bottleneck. To deal with the challenge, we first devise a privacy-preserving cross-party term frequency querying scheme based on sketching algorithms and differential privacy. To further improve the overall efficiency, we propose a new structure named reverse top-K sketch ({RTK}-Sketch) which significantly accelerates the feature generation process while holding theoretical guarantees on accuracy loss. Extensive experiments conducted on public datasets verify the effectiveness and efficiency of the proposed approach.},
	eventtitle = {{ICDE} '21},
	pages = {1128--1139},
	booktitle = {37th {IEEE} International Conference on Data Engineering},
	author = {Wang, Yansheng and Tong, Yongxin and Shi, Dingyuan and Xu, Ke},
	date = {2021-04},
	keywords = {Collaborative work, Conferences, Data engineering, Data models, Differential privacy, Privacy, n/a},
}

@inproceedings{redyuk_automated_2019,
	location = {Macao, Macao},
	title = {Automated Documentation of End-to-End Experiments in Data Science},
	isbn = {978-1-5386-7474-1},
	url = {https://ieeexplore.ieee.org/document/8731587/},
	doi = {10.1109/ICDE.2019.00243},
	series = {{ICDE} '19},
	abstract = {Reproducibility plays a crucial role in experimentation. However, the modern research ecosystem and the underlying frameworks are constantly evolving and thereby making it extremely difﬁcult to reliably reproduce scientiﬁc artifacts such as data, algorithms, trained models and visualizations. We therefore aim to design a novel system for assisting data scientists with rigorous end-to-end documentation of dataoriented experiments. Capturing data lineage, metadata, and other artifacts helps reproducing and sharing experimental results. We summarize this challenge as automated documentation of data science experiments. We aim at reducing manual overhead for experimenting researchers, and intend to create a novel approach in dataﬂow and metadata tracking based on the analysis of the experiment source code. The envisioned system will accelerate the research process in general, and enable capturing ﬁne-grained meta information by deriving a declarative representation of data science experiments.},
	eventtitle = {{ICDE} '19},
	pages = {2076--2080},
	booktitle = {35th {IEEE} International Conference on Data Engineering},
	publisher = {{IEEE}},
	author = {Redyuk, Sergey},
	urldate = {2020-08-31},
	date = {2019-04},
	langid = {english},
}

@inproceedings{li_efficient_2021,
	location = {Chania, Greece},
	title = {Efficient Federated-Learning Model Debugging},
	doi = {10.1109/ICDE51399.2021.00039},
	series = {{ICDE} '21},
	abstract = {Federated learning ({FL}) enables large amounts of participants to construct a global learning model, while storing training data privately at each client device. A fundamental issue in this framework is the susceptibility to the erroneous training data. This problem is especially challenging due to the invisibility of clients' local training data and training process, as well as the resource constraints of a large number of mobile and edge devices. In this paper, we try to tackle this challenging issue by introducing the first {FL} debugging framework, {FLDebugger}, for mitigating test error caused by erroneous training data. The pro-posed solution traces the global model's bugs (test errors), jointly through the training log and the underlying learning algorithm, back to first identify the clients and subsequently their training samples that are most responsible for the errors. In addition, we devise an influence-based participant selection strategy to fix bugs as well as to accelerate the convergence of model retraining. The performance of the identification algorithm is evaluated via extensive experiments on a real {AIoT} system (50 clients, including 20 edge computers, 20 laptops and 10 desktops) and in larger-scale simulated environments. The evaluation results attest to that our framework achieves accurate and efficient identification of negatively influential clients and samples, and significantly improves the model performance by fixing bugs.},
	eventtitle = {{ICDE} '21},
	pages = {372--383},
	booktitle = {37th {IEEE} International Conference on Data Engineering},
	author = {Li, Anran and Zhang, Lan and Wang, Junhao and Tan, Juntao and Han, Feng and Qin, Yaxuan and Freris, Nikolaos M. and Li, Xiang-Yang},
	date = {2021-04},
	keywords = {Adaptation models, Computational modeling, Computer bugs, Debugging, Federated learning, Influence function, Toxicology, Training, Training data},
}

@inproceedings{luo_feature_2021,
	location = {Chania, Greece},
	title = {Feature Inference Attack on Model Predictions in Vertical Federated Learning},
	doi = {10.1109/ICDE51399.2021.00023},
	series = {{ICDE} '21},
	abstract = {Federated learning ({FL}) is an emerging paradigm for facilitating multiple organizations' data collaboration without revealing their private data to each other. Recently, vertical {FL}, where the participating organizations hold the same set of samples but with disjoint features and only one organization owns the labels, has received increased attention. This paper presents several feature inference attack methods to investigate the potential privacy leakages in the model prediction stage of vertical {FL}. The attack methods consider the most stringent setting that the adversary controls only the trained vertical {FL} model and the model predictions, relying on no background information of the attack target's data distribution. We first propose two specific attacks on the logistic regression ({LR}) and decision tree ({DT}) models, according to individual prediction output. We further design a general attack method based on multiple prediction outputs accumulated by the adversary to handle complex models, such as neural networks ({NN}) and random forest ({RF}) models. Experimental evaluations demonstrate the effectiveness of the proposed attacks and highlight the need for designing private mechanisms to protect the prediction outputs in vertical {FL}.},
	eventtitle = {{ICDE} '21},
	pages = {181--192},
	booktitle = {37th {IEEE} International Conference on Data Engineering},
	author = {Luo, Xinjian and Wu, Yuncheng and Xiao, Xiaokui and Ooi, Beng Chin},
	date = {2021-04},
	keywords = {Collaborative work, Data models, Data privacy, Organizations, Prediction algorithms, Predictive models, Radio frequency, feature inference attack, model prediction, privacy preservation, vertical federated learning},
}

@inproceedings{kaoudi_ml-based_2020,
	location = {Virtual Event},
	title = {{ML}-based Cross-Platform Query Optimization},
	isbn = {978-1-72812-903-7},
	url = {https://ieeexplore.ieee.org/document/9101757/},
	doi = {10.1109/ICDE48307.2020.00132},
	series = {{ICDE} '20},
	abstract = {Cost-based optimization is widely known to suffer from a major weakness: administrators spend a signiﬁcant amount of time to tune the associated cost models. This problem only gets exacerbated in cross-platform settings as there are many more parameters that need to be tuned. In the era of machine learning ({ML}), the ﬁrst step to remedy this problem is to replace the cost model of the optimizer with an {ML} model. However, such a solution brings in two major challenges. First, the optimizer has to transform a query plan to a vector million times during plan enumeration incurring a very high overhead. Second, a lot of training data is required to effectively train the {ML} model. We overcome these challenges in Robopt, a novel vector-based optimizer we have built for Rheem, a cross-platform system. Robopt not only uses an {ML} model to prune the search space but also bases the entire plan enumeration on a set of algebraic operations that operate on vectors, which are a natural ﬁt to the {ML} model. This leads to both speed-up and scale-up of the enumeration process by exploiting modern {CPUs} via vectorization. We also accompany Robopt with a scalable training data generator for building its {ML} model. Our evaluation shows that (i) the vector-based approach is more efﬁcient and scalable than simply using an {ML} model and (ii) Robopt matches and, in some cases, improves Rheem’s cost-based optimizer in choosing good plans without requiring any tuning effort.},
	eventtitle = {{ICDE} '20},
	pages = {1489--1500},
	booktitle = {36th {IEEE} International Conference on Data Engineering},
	publisher = {{IEEE}},
	author = {Kaoudi, Zoi and Quiane-Ruiz, Jorge-Arnulfo and Contreras-Rojas, Bertty and Pardo-Meza, Rodrigo and Troudi, Anis and Chawla, Sanjay},
	urldate = {2020-08-25},
	date = {2020-04},
	langid = {english},
}

@inproceedings{stonebraker_one_2005,
	location = {Tokyo, Japan},
	title = {"One Size Fits All": An Idea Whose Time Has Come and Gone},
	doi = {10.1109/ICDE.2005.1},
	series = {{ICDE} '05},
	shorttitle = {"One size fits all"},
	abstract = {The last 25 years of commercial {DBMS} development can be summed up in a single phrase: "one size fits all". This phrase refers to the fact that the traditional {DBMS} architecture (originally designed and optimized for business data processing) has been used to support many data-centric applications with widely varying characteristics and requirements. In this paper, we argue that this concept is no longer applicable to the database market, and that the commercial world will fracture into a collection of independent database engines, some of which may be unified by a common front-end parser. We use examples from the stream-processing market and the data-warehouse market to bolster our claims. We also briefly discuss other markets for which the traditional architecture is a poor fit and argue for a critical rethinking of the current factoring of systems services into products.},
	eventtitle = {{ICDE} '05},
	pages = {2--11},
	booktitle = {21st {IEEE} International Conference on Data Engineering},
	author = {Stonebraker, Michael and Çetintemel, Uĝur},
	date = {2005-04},
	keywords = {Artificial intelligence, Computer architecture, Computer science, Costs, Data warehouses, Databases, Delay, Engines, Laboratories, Prototypes},
}

@inproceedings{galhotra_metam_2023,
	location = {Anaheim, {CA}},
	title = {Metam: Goal-Oriented Data Discovery},
	url = {https://ieeexplore.ieee.org/document/10184845},
	doi = {10.1109/ICDE55515.2023.00213},
	series = {{ICDE} '23},
	shorttitle = {Metam},
	abstract = {Data is a central component of machine learning and causal inference tasks. The availability of large amounts of data from sources such as open data repositories, data lakes and data marketplaces creates an opportunity to augment data and boost those tasks’ performance. However, augmentation techniques rely on a user manually discovering and shortlisting useful candidate augmentations. Existing solutions do not leverage the synergy between discovery and augmentation, thus underexploiting data.In this paper, we introduce Metam, a novel goal-oriented framework that queries the downstream task with a candidate dataset, forming a feedback loop that automatically steers the discovery and augmentation process. To select candidates efficiently, Metam leverages properties of the: i) data, ii) utility function, and iii) solution set size. We show Metam’s theoretical guarantees and demonstrate those empirically on a broad set of tasks. All in all, we demonstrate the promise of goal-oriented data discovery to modern data science applications.},
	eventtitle = {{ICDE} '23},
	pages = {2780--2793},
	booktitle = {39th {IEEE} International Conference on Data Engineering},
	author = {Galhotra, Sainyam and Gong, Yue and Fernandez, Raul Castro},
	urldate = {2024-02-03},
	date = {2023-04},
	note = {{ISSN}: 2375-026X},
	keywords = {Big Data applications, Complexity theory, Data engineering, Data science, Feedback loop, Machine learning, Task analysis},
}

@inproceedings{chaudhuri_generalization_1990,
	title = {Generalization and a Framework for Query Modification},
	doi = {10.1109/ICDE.1990.113463},
	series = {{ICDE} '90},
	abstract = {The rigidity and limited expressiveness of relational queries often require that a query be iteratively modified. An initial query is posed, and once it is discovered that the answer does not meet the additional constraints, which are not expressed in the relational query, it is necessary to modify the query in a way such that those constraints are satisfied. The aim of this work is to capture this iterative process by extending the query model. Extended queries, which express additional constraints on the answer set and designate some of the conditions in the relational query as flexible, are defined. The query modification operators modify flexible constraints to satisfy an extended query. The query modification operation, generalization, is described. The conditions under which generalization is applicable are identified. Rules of generalization are proposed, and an algorithm for picking a minimal generalization is suggested.{\textless}{\textgreater}},
	eventtitle = {{ICDE} '90},
	pages = {138--145},
	booktitle = {6th {IEEE} International Conference on Data Engineering},
	author = {Chaudhuri, Surajit},
	date = {1990-02},
	keywords = {Data engineering, Engineering drawings, Information retrieval, Marine vehicles, Object oriented databases, Object oriented modeling, Pressing, Query processing, Relational databases},
}

@inproceedings{bogatu_dataset_2020,
	location = {Dallas, {TX}},
	title = {Dataset Discovery in Data Lakes},
	isbn = {978-1-72812-903-7},
	url = {https://ieeexplore.ieee.org/document/9101607/},
	doi = {10.1109/ICDE48307.2020.00067},
	series = {{ICDE} '20},
	abstract = {Data analytics stands to beneﬁt from the increasing availability of datasets that are held without their conceptual relationships being explicitly known. When collected, these datasets form a data lake from which, by processes like data wrangling, speciﬁc target datasets can be constructed that enable value–adding analytics. Given the potential vastness of such data lakes, the issue arises of how to pull out of the lake those datasets that might contribute to wrangling out a given target. We refer to this as the problem of dataset discovery in data lakes and this paper contributes an effective and efﬁcient solution to it. Our approach uses features of the values in a dataset to construct hash–based indexes that map those features into a uniform distance space. This makes it possible to deﬁne similarity distances between features and to take those distances as measurements of relatedness w.r.t. a target table. Given the latter (and exemplar tuples), our approach returns the most related tables in the lake. We provide a detailed description of the approach and report on empirical results for two forms of relatedness (unionability and joinability) comparing them with prior work, where pertinent, and showing signiﬁcant improvements in all of precision, recall, target coverage, indexing and discovery times.},
	eventtitle = {{ICDE} '20},
	pages = {709--720},
	booktitle = {36th {IEEE} International Conference on Data Engineering},
	publisher = {{IEEE}},
	author = {Bogatu, Alex and Fernandes, Alvaro A. A. and Paton, Norman W. and Konstantinou, Nikolaos},
	urldate = {2020-08-31},
	date = {2020-04},
	langid = {english},
}

@inproceedings{kaoudi_cross-platform_2018,
	location = {Paris, France},
	title = {Cross-Platform Data Processing: Use Cases and Challenges},
	doi = {10.1109/ICDE.2018.00223},
	series = {{ICDE} '18},
	shorttitle = {Cross-Platform Data Processing},
	abstract = {There is a zoo of data processing platforms which help users and organizations to extract value out of their data. Although each of these platforms excels in specific aspects, users typically end up running their data analytics on suboptimal platforms. This is not only because choosing the right platform among the myriad of big data platforms is a daunting task, but also due to the fact that today's data analytics are moving beyond the limits of a single platform. Thus, there is an urgent need for cross-platform data processing, i.e., using more than one data processing platform to perform a data analytics task. Despite the need, achieving this is still a dreadful process where developers have to get intimate with many systems and write ad hoc scripts for integrating them. This tutorial is motivated by this need. We will discuss the importance of supporting cross-platform data processing in a systematic way as well as the current efforts to achieve that. In particular, we will introduce a classification of the different cases where an application needs or benefits from cross-platform data processing and the challenges of each case. Along with this classification, we will also present the efforts known up to date to support cross-platform data processing. We will conclude this tutorial with a discussion of several important open problems.},
	eventtitle = {{ICDE} '18},
	pages = {1723--1726},
	booktitle = {34th {IEEE} International Conference on Data Engineering},
	author = {Kaoudi, Zoi and Quiane-Ruiz, Jorge-Arnulfo},
	date = {2018-04},
	note = {{ISSN}: 2375-026X},
	keywords = {Big Data, Data analysis, Organizations, Sparks, Task analysis, Tutorials, cross platform, data processing, polystores, query processing},
}

@inproceedings{mansour_building_2018,
	location = {Paris, France},
	title = {Building Data Civilizer Pipelines with an Advanced Workflow Engine},
	doi = {10.1109/ICDE.2018.00184},
	series = {{ICDE} '18},
	abstract = {In order for an enterprise to gain insight into its internal business and the changing outside environment, it is essential to provide the relevant data for in-depth analysis. Enterprise data is usually scattered across departments and geographic regions and is often inconsistent. Data scientists spend the majority of their time finding, preparing, integrating, and cleaning relevant data sets. Data Civilizer is an end-to-end data preparation system. In this paper, we present the complete system, focusing on our new workflow engine, a superior system for entity matching and consolidation, and new cleaning tools. Our workflow engine allows data scientists to author, execute and retrofit data preparation pipelines of different data discovery and cleaning services. Our end-to-end demo scenario is based on data from the {MIT} data warehouse and e-commerce data sets.},
	eventtitle = {{ICDE} '18},
	pages = {1593--1596},
	booktitle = {34th {IEEE} International Conference on Data Engineering},
	publisher = {{IEEE}},
	author = {Mansour, Essam and Deng, Dong and Fernandez, Raul Castro and Qahtan, Abdulhakim A. and Tao, Wenbo and Abedjan, Ziawasch and Elmagarmid, Ahmed and Ilyas, Ihab F. and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
	date = {2018-04},
	note = {{ISSN}: 2375-026X},
	keywords = {Buildings, Cleaning, Data Cleaning, Data Discovery, Data Integration, Electrocardiography, Engines, Pipelines, Task analysis, Tools},
}

@inproceedings{castro_fernandez_aurum_2018,
	location = {Paris, France},
	title = {Aurum: A Data Discovery System},
	isbn = {978-1-5386-5520-7},
	url = {https://ieeexplore.ieee.org/document/8509315/},
	doi = {10.1109/ICDE.2018.00094},
	series = {{ICDE} '18},
	shorttitle = {Aurum},
	abstract = {Organizations face a data discovery problem when their analysts spend more time looking for relevant data than analyzing it. This problem has become commonplace in modern organizations as: i) data is stored across multiple storage systems, from databases to data lakes, to the cloud; ii) data scientists do not operate within the limits of well-deﬁned schemas or a small number of data sources—instead, to answer complex questions they must access data spread across thousands of data sources. To address this problem, we capture relationships between datasets in an enterprise knowledge graph ({EKG}), which helps users to navigate among disparate sources. The contribution of this paper is {AURUM}, a system to build, maintain and query the {EKG}. To build the {EKG}, we introduce a Two-step process which scales to large datasets and requires only one-pass over the data, avoiding overloading the source systems. To maintain the {EKG} without re-reading all data every time, we introduce a resource-efﬁcient sampling signature ({RESS}) method which works by only using a small sample of the data. Finally, to query the {EKG}, we introduce a collection of composable primitives, thus allowing users to deﬁne many different types of discovery queries. We describe our experience using {AURUM} in three corporate scenarios and do a performance evaluation of each component.},
	eventtitle = {{ICDE} '18},
	pages = {1001--1012},
	booktitle = {34th {IEEE} International Conference on Data Engineering},
	publisher = {{IEEE}},
	author = {Castro Fernandez, Raul and Abedjan, Ziawasch and Koko, Famien and Yuan, Gina and Madden, Samuel and Stonebraker, Michael},
	urldate = {2020-08-31},
	date = {2018-04},
	langid = {english},
}

@inproceedings{asudeh_assessing_2019,
	title = {Assessing and Remedying Coverage for a Given Dataset},
	doi = {10.1109/ICDE.2019.00056},
	series = {{ICDE} '19},
	abstract = {Data analysis impacts virtually every aspect of our society today. Often, this analysis is performed on an existing dataset, possibly collected through a process that the data scientists had limited control over. The existing data analyzed may not include the complete universe, but it is expected to cover the diversity of items in the universe. Lack of adequate coverage in the dataset can result in undesirable outcomes such as biased decisions and algorithmic racism, as well as creating vulnerabilities such as opening up room for adversarial attacks. In this paper, we assess the coverage of a given dataset over multiple categorical attributes. We first provide efficient techniques for traversing the combinatorial explosion of value combinations to identify any regions of attribute space not adequately covered by the data. Then, we determine the least amount of additional data that must be obtained to resolve this lack of adequate coverage. We confirm the value of our proposal through both theoretical analyses and comprehensive experiments on real data.},
	eventtitle = {{ICDE} '19},
	pages = {554--565},
	booktitle = {35th {IEEE} International Conference on Data Engineering},
	author = {Asudeh, Abolfazl and Jin, Zhongjun and Jagadish, Hosagrahar V.},
	date = {2019-04},
	note = {{ISSN}: 2375-026X},
	keywords = {Bias, Data science, Discrimination, Disparate Prediction, Fairness, Google, Input Data Assessment, Pattern matching, Process control, Software algorithms, Tools, Training data},
}

@online{open_knowledge_foundation_ckan_2022,
	title = {{CKAN}},
	url = {http://ckan.org/},
	abstract = {{CKAN} is an open-source {DMS} (data management system) for powering data hubs and data portals. {CKAN} makes it easy to publish, share and use data.},
	author = {{Open Knowledge Foundation}},
	urldate = {2022-08-28},
	date = {2022},
}

@article{gong_nexus_2024,
	title = {Nexus: Correlation Discovery over Collections of Spatio-Temporal Tabular Data},
	volume = {2},
	url = {https://doi.org/10.1145/3654957},
	doi = {10.1145/3654957},
	shorttitle = {Nexus},
	abstract = {Causal analysis is essential for gaining insights into complex real-world processes and making informed decisions. However, performing accurate causal analysis on observational data is generally infeasible, and therefore, domain experts start exploration with the identification of correlations. The increased availability of data from open government websites, organizations, and scientific studies presents an opportunity to harness observational datasets in assisting domain experts during this exploratory phase.In this work, we introduce Nexus, a system designed to align large repositories of spatio-temporal datasets and identify correlations, facilitating the exploration of causal relationships. Nexus addresses the challenges of aligning tabular datasets across space and time, handling missing data, and identifying correlations deemed "interesting". Empirical evaluation on Chicago Open Data and United Nations datasets demonstrates the effectiveness of Nexus in exposing interesting correlations, many of which have undergone extensive scrutiny by social scientists.},
	pages = {154:1--154:28},
	number = {3},
	journaltitle = {Proceedings of the {ACM} on Management of Data},
	shortjournal = {Proc. {ACM} Manag. Data},
	author = {Gong, Yue and Galhotra, Sainyam and Castro Fernandez, Raul},
	urldate = {2024-07-02},
	date = {2024-05-30},
}

@inproceedings{kuchnik_plumber_2022,
	location = {Santa Clara, {CA}},
	title = {Plumber: Diagnosing and Removing Performance Bottlenecks in Machine Learning Data Pipelines},
	abstract = {Input pipelines, which ingest and transform input data, are an essential part of training Machine Learning ({ML}) models. However, it is challenging to implement efﬁcient input pipelines, as it requires reasoning about parallelism, asynchrony, and variability in ﬁne-grained proﬁling information. Our analysis of over two million {ML} jobs in Google datacenters reveals that a signiﬁcant fraction of model training jobs could beneﬁt from faster input data pipelines. At the same time, our analysis indicates that most jobs do not saturate host hardware, pointing in the direction of software-based bottlenecks. Motivated by these ﬁndings, we propose Plumber, a tool for ﬁnding bottlenecks in {ML} input pipelines. Plumber uses an extensible and interpretable operational analysis analytical model to automatically tune parallelism, prefetching, and caching under host resource constraints. Across ﬁve representative {ML} pipelines, Plumber obtains speedups of up to 47× for misconﬁgured pipelines. By automating caching, Plumber obtains end-to-end speedups of over 50\% compared to state-of-the-art tuners.},
	eventtitle = {{MLSys} '22},
	booktitle = {5th {MLSys} Conference},
	author = {Kuchnik, Michael and Klimovic, Ana and Šimša, Jirí and Smith, Virginia and Amvrosiadis, George},
	date = {2022},
	langid = {english},
}

@inproceedings{hulsebos_it_2024,
	location = {Santiago, Chile},
	title = {It Took Longer than I was Expecting: Why is Dataset Search Still so Hard?},
	isbn = {9798400706936},
	url = {https://doi.org/10.1145/3665939.3665959},
	doi = {10.1145/3665939.3665959},
	series = {{HILDA}  24},
	shorttitle = {It Took Longer than I was Expecting},
	abstract = {Dataset search is a long-standing problem across both industry and academia. While most industry tools focus on identifying one or more datasets matching a user-specified query, most recent academic papers focus on the subsequent problems of join and union discovery for a given dataset. In this paper, we take a step back and ask: is the task of identifying an initial dataset really a solved problem? Are join and union discovery indeed the most pressing problems to work on? To answer these questions, we survey 89 data professionals and surface the objectives, processes, and tools used to search for structured datasets, along with the challenges faced when using existing systems. We uncover characteristics of data content and metadata that are most important for data professionals during search, such as granularity and data freshness. Informed by our analysis, we argue that dataset search is not yet a solved problem, but is, in fact, difficult to solve. To move the needle in the right direction, we distill four desiderata for future dataset search systems: iterative interfaces, hybrid querying, task-driven search and result diversity.},
	eventtitle = {{HILDA} '24},
	pages = {1--4},
	booktitle = {2024 Workshop on Human-In-the-Loop Data Analytics},
	publisher = {Association for Computing Machinery},
	author = {Hulsebos, Madelon and Lin, Wenjing and Shankar, Shreya and Parameswaran, Aditya},
	urldate = {2024-06-27},
	date = {2024-06-18},
}

@inproceedings{akhtar_croissant_2024,
	location = {Santiago, Chile},
	title = {Croissant: A Metadata Format for {ML}-Ready Datasets},
	isbn = {9798400706110},
	url = {https://dl.acm.org/doi/10.1145/3650203.3663326},
	doi = {10.1145/3650203.3663326},
	shorttitle = {Croissant},
	abstract = {Data is a critical resource for Machine Learning ({ML}), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by {ML} tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in {ML} data management and responsible {AI}. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular {ML} frameworks.},
	eventtitle = {{DEEM} '24},
	pages = {1--6},
	booktitle = {8th Workshop on Data Management for End-to-End Machine Learning},
	publisher = {{ACM}},
	author = {Akhtar, Mubashara and Benjelloun, Omar and Conforti, Costanza and Gijsbers, Pieter and Giner-Miguelez, Joan and Jain, Nitisha and Kuchnik, Michael and Lhoest, Quentin and Marcenac, Pierre and Maskey, Manil and Mattson, Peter and Oala, Luis and Ruyssen, Pierre and Shinde, Rajat and Simperl, Elena and Thomas, Goeffry and Tykhonov, Slava and Vanschoren, Joaquin and Van Der Velde, Jos and Vogler, Steffen and Wu, Carole-Jean},
	urldate = {2024-06-27},
	date = {2024-06-09},
	langid = {english},
}

@article{rahm_survey_2001,
	title = {A Survey of Approaches to Automatic Schema Matching},
	volume = {10},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s007780100057},
	doi = {10.1007/s007780100057},
	abstract = {Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.},
	pages = {334--350},
	number = {4},
	journaltitle = {The {VLDB} Journal},
	shortjournal = {The {VLDB} Journal},
	author = {Rahm, Erhard and Bernstein, Philip A.},
	urldate = {2024-05-28},
	date = {2001-12-01},
	langid = {english},
	keywords = {Key words: Schema matching – Schema integration – Graph matching – Model management – Machine learning},
}

@article{halevy_answering_2001,
	title = {Answering Queries Using Views: A Survey},
	volume = {10},
	rights = {http://www.springer.com/tdm},
	issn = {10668888},
	url = {http://link.springer.com/10.1007/s007780100054},
	doi = {10.1007/s007780100054},
	shorttitle = {Answering queries using views},
	abstract = {The problem of answering queries using views is to ﬁnd efﬁcient methods of answering a query using a set of previously deﬁned materialized views over the database, rather than accessing the database relations. The problem has recently received signiﬁcant attention because of its relevance to a wide variety of data management problems. In query optimization, ﬁnding a rewriting of a query using a set of materialized views can yield a more efﬁcient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, ﬁnding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.},
	pages = {270--294},
	number = {4},
	journaltitle = {The {VLDB} Journal},
	author = {Halevy, Alon Y.},
	urldate = {2024-05-28},
	date = {2001-12},
	langid = {english},
}

@inproceedings{ullman_information_1997,
	location = {Delphi, Greece},
	title = {Information Integration Using Logical Views},
	isbn = {978-3-540-49682-3},
	doi = {10.1007/3-540-62222-5_34},
	abstract = {A number of ideas concerning information-integration tools can be thought of as constructing answers to queries using views that represent the capabilities of information sources. We review the formal basis of these techniques, which are closely related to containment algorithms for conjunctive queries and/or Datalog programs. Then we compare the approaches taken by {AT}\&T Labs' “Information Manifold” and the Stanford “Tsimmis” project in these terms.},
	eventtitle = {{ICDT} '97},
	pages = {19--40},
	booktitle = {6th International Conference on Database Theory},
	publisher = {Springer},
	author = {Ullman, Jeffrey D.},
	editor = {Afrati, Foto and Kolaitis, Phokion},
	date = {1997},
	langid = {english},
}

@inproceedings{levy_querying_1996,
	location = {San Francisco, {CA}, {USA}},
	title = {Querying Heterogeneous Information Sources Using Source Descriptions},
	isbn = {978-1-55860-382-0},
	series = {{VLDB} '96},
	eventtitle = {{VLDB} '96},
	pages = {251--262},
	booktitle = {22nd International Conference on Very Large Data Bases},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Levy, Alon Y. and Rajaraman, Anand and Ordille, Joann J.},
	urldate = {2024-05-28},
	date = {1996-09-03},
}

@online{hugging_face_hugging_2024,
	title = {Hugging Face Datasets},
	url = {https://huggingface.co/datasets},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	author = {{Hugging Face}},
	urldate = {2024-05-15},
	date = {2024-05-09},
}

@article{li_ranking_2010,
	title = {Ranking Continuous Probabilistic Datasets},
	volume = {3},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/1920841.1920923},
	doi = {10.14778/1920841.1920923},
	abstract = {Ranking is a fundamental operation in data analysis and decision support, and plays an even more crucial role if the dataset being explored exhibits uncertainty. This has led to much work in understanding how to rank uncertain datasets in recent years. In this paper, we address the problem of ranking when the tuple scores are uncertain, and the uncertainty is captured using continuous probability distributions (e.g. Gaussian distributions). We present a comprehensive solution to compute the values of a parameterized ranking function ({PRF}) [18] for arbitrary continuous probability distributions (and thus rank the uncertain dataset); {PRF} can be used to simulate or approximate many other ranking functions proposed in prior work. We develop exact polynomial time algorithms for some continuous probability distribution classes, and efficient approximation schemes with provable guarantees for arbitrary probability distributions. Our algorithms can also be used for exact or approximate evaluation of k-nearest neighbor queries over uncertain objects, whose positions are modeled using continuous probability distributions. Our experimental evaluation over several datasets illustrates the effectiveness of our approach at efficiently ranking uncertain datasets with continuous attribute uncertainty.},
	pages = {638--649},
	number = {1},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Li, Jian and Deshpande, Amol},
	urldate = {2024-05-02},
	date = {2010-09-01},
}

@misc{chiang_chatbot_2024,
	title = {Chatbot Arena: An Open Platform for Evaluating {LLMs} by Human Preference},
	url = {http://arxiv.org/abs/2403.04132},
	doi = {10.48550/arXiv.2403.04132},
	shorttitle = {Chatbot Arena},
	abstract = {Large Language Models ({LLMs}) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating {LLMs} based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced {LLM} leaderboards, widely cited by leading {LLM} developers and companies. Our demo is publicly available at {\textbackslash}url\{https://chat.lmsys.org\}.},
	number = {{arXiv}:2403.04132},
	publisher = {{arXiv}},
	author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
	urldate = {2024-04-25},
	date = {2024-03-06},
	eprinttype = {arxiv},
	eprint = {2403.04132 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{chai_selective_2022,
	title = {Selective data acquisition in the wild for model charging},
	volume = {15},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3523210.3523223},
	doi = {10.14778/3523210.3523223},
	abstract = {The lack of sufficient labeled data is a key bottleneck for practitioners in many real-world supervised machine learning ({ML}) tasks. In this paper, we study a new problem, namely selective data acquisition in the wild for model charging: given a supervised {ML} task and data in the wild (e.g., enterprise data warehouses, online data repositories, data markets, and so on), the problem is to select labeled data points from the data in the wild as additional train data that can help the {ML} task. It consists of two steps (Fig. 1). The first step is to discover relevant datasets (e.g., tables with similar relational schema), which will result in a set of candidate datasets. Because these candidate datasets come from different sources and may follow different distributions, not all data points they contain can help. The second step is to select which data points from these candidate datasets should be used. We build an end-to-end solution. For step 1, we piggyback off-the-shelf data discovery tools. Technically, our focus is on step 2, for which we propose a solution framework called {AutoData}. It first clusters all data points from candidate datasets such that each cluster contains similar data points from different sources. It then iteratively picks which cluster to use, samples data points (i.e., a mini-batch) from the picked cluster, evaluates the mini-batch, and then revises the search criteria by learning from the feedback (i.e., reward) based on the evaluation. We propose a multi-armed bandit based solution and a Deep Q Networks-based reinforcement learning solution. Experiments using both relational and image datasets show the effectiveness of our solutions.},
	pages = {1466--1478},
	number = {7},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Chai, Chengliang and Liu, Jiabin and Tang, Nan and Li, Guoliang and Luo, Yuyu},
	urldate = {2024-04-22},
	date = {2022-03-01},
}

@article{nargesian_tailoring_2021,
	title = {Tailoring data source distributions for fairness-aware data integration},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3476249.3476299},
	doi = {10.14778/3476249.3476299},
	abstract = {Data scientists often develop data sets for analysis by drawing upon sources of data available to them. A major challenge is to ensure that the data set used for analysis has an appropriate representation of relevant (demographic) groups: it meets desired distribution requirements. Whether data is collected through some experiment or obtained from some data provider, the data from any single source may not meet the desired distribution requirements. Therefore, a union of data from multiple sources is often required. In this paper, we study how to acquire such data in the most cost effective manner, for typical cost functions observed in practice. We present an optimal solution for binary groups when the underlying distributions of data sources are known and all data sources have equal costs. For the generic case with unequal costs, we design an approximation algorithm that performs well in practice. When the underlying distributions are unknown, we develop an exploration-exploitation based strategy with a reward function that captures the cost and approximations of group distributions in each data source. Besides theoretical analysis, we conduct comprehensive experiments that confirm the effectiveness of our algorithms.},
	pages = {2519--2532},
	number = {11},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Nargesian, Fatemeh and Asudeh, Abolfazl and Jagadish, H. V.},
	urldate = {2024-04-17},
	date = {2021-07-01},
}

@article{greenacre_importance_2016,
	title = {The Importance of Selection Bias in Internet Surveys},
	volume = {6},
	issn = {2161-718X, 2161-7198},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ojs.2016.63035},
	doi = {10.4236/ojs.2016.63035},
	pages = {397--404},
	number = {3},
	journaltitle = {Open Journal of Statistics},
	shortjournal = {{OJS}},
	author = {Greenacre, Zerrin Asan},
	urldate = {2024-02-13},
	date = {2016},
}

@book{cormen_introduction_2022,
	location = {Cambridge, {MA}},
	title = {Introduction to Algorithms},
	isbn = {978-0-262-36750-9},
	abstract = {"The leading introductory textbook and reference on algorithms"-- Provided by publisher},
	publisher = {{MIT} Press},
	author = {Cormen, Thomas H. and Leiserson, Charles Eric and Rivest, Ronald L. and Stein, Clifford},
	date = {2022},
	note = {{OCLC}: 1305060400},
}

@book{manning_introduction_2008,
	location = {Cambridge, {MA}},
	title = {Introduction to Information Retrieval},
	isbn = {978-0-521-86571-5},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	date = {2008},
}

@report{otto_gaia-x_2021,
	title = {{GAIA}-X and {IDS}},
	url = {https://zenodo.org/records/5675897},
	abstract = {This position paper gives a first overview of the architectures of {GAIA}-X and International Data Spaces. This is followed by a discussion on how the two infrastructures are or can be aligned to one infrastructure. We only focus on the alignment of {IDS} and {GAIA}-X without any discussions about other initiatives like Plattform Industrie 4.0, {SWIPO} (Switching Cloud Providers and Porting Data), or similar. This current paper also does not specifically address synergies with the Once-Only technical system developed by the European Commission for the public sector, which will re-use mature solutions from the {CEF} Building Blocks and the {ISA}² Core Vocabularies. From our perspectives, these initiatives are also highly relevant and are partly linked to both, {IDS} and {GAIA}-X initiatives, but an integration would go beyond the scope of this paper.},
	institution = {Zenodo},
	author = {Otto, Boris},
	urldate = {2024-02-28},
	date = {2021-01-01},
	doi = {10.5281/zenodo.5675897},
	keywords = {Certification, Connectors, Data Economy, Data Ecosystems, Data Exchange, Data Sharing, Data Sovereignty, Data Spaces, {GAIA}-X, {IDS} Certification, International Data Spaces},
}

@article{braud_road_2021,
	title = {The Road to European Digital Sovereignty with Gaia-X and {IDSA}},
	volume = {35},
	issn = {1558-156X},
	url = {https://ieeexplore.ieee.org/abstract/document/9387709},
	doi = {10.1109/MNET.2021.9387709},
	abstract = {Digitalization is chewing the world with strong economic and social impacts. Recently, the management of the {COVID} 19 crisis highlighted the power of digital tools and their impact on stakes such as privacy, surveillance, transparency, and censorship. How nations deal with massive digitalization and master the technologies, and applications that are deployed and used on their soils by their companies and citizens is vividly raised by the U.S. ban on Huawei enforced by the “clean network” strategy. This ban could have set the ground for a technology war between “2 blocks: the digital democracies and the techno-authoritarian regimes” [1], [2].},
	pages = {4--5},
	number = {2},
	journaltitle = {{IEEE} Network},
	author = {Braud, Arnaud and Fromentoux, Gaël and Radier, Benoit and Le Grand, Olivier},
	urldate = {2024-02-28},
	date = {2021-03},
	note = {Conference Name: {IEEE} Network},
}

@article{idreos_periodic_2018,
	title = {The Periodic Table of Data Structures},
	volume = {41},
	abstract = {We describe the vision of being able to reason about the design space of data structures. We break this down into two questions: 1) Can we know all data structures that is possible to design? 2) Can we compute the performance of arbitrary designs on a given hardware and workload without having to implement the design or even access the target hardware? If those challenges are possible, then an array of exciting opportunities would become feasible such as interactive what-if design to improve the productivity of data systems researchers and engineers, and informed decision making in industrial settings with regards to critical hardware/workload/data structure design issues. Then, even fully automated discovery of new data structure designs becomes possible. Furthermore, the structure of the design space itself provides numerous insights and opportunities such as the existence of design continuums that can lead to data systems with deep adaptivity, and a new understanding of the possible performance tradeoffs. Given the universal presence of data structures at the very core of any data-driven ﬁeld across all sciences and industries, reasoning about their design can have signiﬁcant beneﬁts, making it more feasible (easier, faster and cheaper) to adopt tailored state-of-the-art storage solutions. And this effect is going to become increasingly more critical as data keeps growing, hardware keeps changing and more applications/ﬁelds realize the transformative power and potential of data analytics. This paper presents this vision and surveys ﬁrst steps that demonstrate its feasibility.},
	pages = {64--75},
	number = {3},
	journaltitle = {Bulletin of the {IEEE} Computer Society Technical Committee on Data Engineering},
	shortjournal = {{IEEE} Data Eng. Bull.},
	author = {Idreos, Stratos and Zoumpatianos, Kostas and Athanassoulis, Manos and Dayan, Niv and Hentschel, Brian and Kester, Michael S. and Guo, Demi and Maas, Lukas and Qin, Wilson and Wasay, Abdul and Sun, Yiyou},
	date = {2018},
}

@article{cormode_synopses_2011,
	title = {Synopses for Massive Data: Samples, Histograms, Wavelets, Sketches},
	volume = {4},
	issn = {1931-7883},
	url = {http://dx.doi.org/10.1561/1900000004},
	doi = {10.1561/1900000004},
	shorttitle = {Synopses for Massive Data},
	abstract = {Methods for Approximate Query Processing ({AQP}) are essential for dealing with massive data. They are often the only means of providing interactive response times when exploring massive datasets, and are also needed to handle high speed data streams. These methods proceed by computing a lossy, compact synopsis of the data, and then executing the query of interest against the synopsis rather than the entire dataset. We describe basic principles and recent developments in {AQP}. We focus on four key synopses: random samples, histograms, wavelets, and sketches. We consider issues such as accuracy, space and time eﬃciency, optimality, practicality, range of applicability, error bounds on query answers, and incremental maintenance. We also discuss the tradeoﬀs between the diﬀerent synopsis types.},
	pages = {1--294},
	number = {1},
	journaltitle = {Foundations and Trends® in Databases},
	shortjournal = {{FNT} in Databases},
	author = {Cormode, Graham and Garofalakis, Minos and Haas, Peter J. and Jermaine, Chris},
	date = {2011},
}

@article{abedjan_detecting_2016,
	title = {Detecting Data Errors: Where Are We and What Needs To Be Done?},
	volume = {9},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2994509.2994518},
	doi = {10.14778/2994509.2994518},
	shorttitle = {Detecting data errors},
	abstract = {Data cleaning has played a critical role in ensuring data quality for enterprise applications. Naturally, there has been extensive research in this area, and many data cleaning algorithms have been translated into tools to detect and to possibly repair certain classes of errors such as outliers, duplicates, missing values, and violations of integrity constraints. Since diﬀerent types of errors may coexist in the same data set, we often need to run more than one kind of tool. In this paper, we investigate two pragmatic questions: (1) are these tools robust enough to capture most errors in real-world data sets? and (2) what is the best strategy to holistically run multiple tools to optimize the detection eﬀort? To answer these two questions, we obtained multiple data cleaning tools that utilize a variety of error detection techniques. We also collected ﬁve real-world data sets, for which we could obtain both the raw data and the ground truth on existing errors. In this paper, we report our experimental ﬁndings on the errors detected by the tools we tested. First, we show that the coverage of each tool is well below 100\%. Second, we show that the order in which multiple tools are run makes a big diﬀerence. Hence, we propose a holistic multi-tool strategy that orders the invocations of the available tools to maximize their beneﬁt, while minimizing human eﬀort in verifying results. Third, since this holistic approach still does not lead to acceptable error coverage, we discuss two simple strategies that have the potential to improve the situation, namely domain speciﬁc tools and data enrichment. We close this paper by reasoning about the errors that are not detectable by any of the tools we tested.},
	pages = {993--1004},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Abedjan, Ziawasch and Chu, Xu and Deng, Dong and Fernandez, Raul Castro and Ilyas, Ihab F. and Ouzzani, Mourad and Papotti, Paolo and Stonebraker, Michael and Tang, Nan},
	urldate = {2024-02-27},
	date = {2016-08},
	langid = {english},
}

@inproceedings{jagadish_optimal_1998,
	location = {San Francisco, {CA}},
	title = {Optimal Histograms with Quality Guarantees},
	isbn = {978-1-55860-566-4},
	series = {{VLDB} '98},
	eventtitle = {{VLDB} '98},
	pages = {275--286},
	booktitle = {24rd International Conference on Very Large Data Bases},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Jagadish, H. V. and Koudas, Nick and Muthukrishnan, S. and Poosala, Viswanath and Sevcik, Kenneth C. and Suel, Torsten},
	urldate = {2024-02-26},
	date = {1998-08-24},
}

@article{lloyd_least_1982,
	title = {Least squares quantization in {PCM}},
	volume = {28},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/1056489},
	doi = {10.1109/TIT.1982.1056489},
	abstract = {It has long been realized that in pulse-code modulation ({PCM}), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\textasciicircum}bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
	pages = {129--137},
	number = {2},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Lloyd, Stuart P.},
	urldate = {2024-02-26},
	date = {1982-03},
	note = {Conference Name: {IEEE} Transactions on Information Theory},
}

@article{zhu_consistent_2023,
	title = {Consistent Range Approximation for Fair Predictive Modeling},
	volume = {16},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3611479.3611498},
	doi = {10.14778/3611479.3611498},
	abstract = {This paper proposes a novel framework for certifying the fairness of predictive models trained on biased data. It draws from query answering for incomplete and inconsistent databases to formulate the problem of consistent range approximation ({CRA}) of fairness queries for a predictive model on a target population. The framework employs background knowledge of the data collection process and biased data, working with or without limited statistics about the target population, to compute a range of answers for fairness queries. Using {CRA}, the framework builds predictive models that are certifiably fair on the target population, regardless of the availability of external data during training. The framework's efficacy is demonstrated through evaluations on real data, showing substantial improvement over existing state-of-the-art methods.},
	pages = {2925--2938},
	number = {11},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Zhu, Jiongli and Galhotra, Sainyam and Sabri, Nazanin and Salimi, Babak},
	urldate = {2024-02-13},
	date = {2023-07-01},
}

@article{park_keyword_2011,
	title = {Keyword Search in Relational Databases},
	volume = {26},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-010-0284-1},
	doi = {10.1007/s10115-010-0284-1},
	abstract = {This paper surveys research on enabling keyword search in relational databases. We present fundamental characteristics and discuss research dimensions, including data representation, ranking, efficient processing, query representation, and result presentation. Various approaches for developing the search system are described and compared within a common framework. We discuss the evolution of new research strategies to resolve the issues associated with probabilistic models, efficient top-k query processing, and schema analysis in relational databases.},
	pages = {175--193},
	number = {2},
	journaltitle = {Knowledge and Information Systems},
	shortjournal = {Knowl Inf Syst},
	author = {Park, Jaehui and Lee, Sang-goo},
	urldate = {2024-02-07},
	date = {2011-02-01},
	langid = {english},
	keywords = {Information retrieval, Integration, Keyword search, Ranking model, Relational database, Top-k query Processing},
}

@article{yu_keyword_2010,
	title = {Keyword Search in Relational Databases: A Survey},
	volume = {33},
	pages = {67--78},
	number = {1},
	journaltitle = {Bulletin of the {IEEE} Computer Society Technical Committee on Data Engineering},
	shortjournal = {{IEEE} Data Eng. Bull.},
	author = {Yu, Jeffrey Xu and Qin, Lu and Chang, Lijun},
	date = {2010},
}

@article{yang_keyword_2021,
	title = {Keyword Search on Large Graphs: A Survey},
	volume = {6},
	issn = {2364-1541},
	url = {https://doi.org/10.1007/s41019-021-00154-4},
	doi = {10.1007/s41019-021-00154-4},
	shorttitle = {Keyword Search on Large Graphs},
	abstract = {With the prevalence of Internet access and online services, various big graphs are generated in many real applications (e.g., online social networks and knowledge graphs). An important task on analyzing and mining these graphs is keyword search. Essentially, given a graph G and query Q associated with a set of keywords, the keyword search aims to find a substructure (e.g., rooted tree or subgraph) S in G such that nodes in S collectively cover part of or all keywords in Q, and in the meanwhile, S is optimal on some user specified semantics. Keyword search on graphs can be applied in many real-life applications, such as point-of-interests recommendation and web search facility. In spite of the great importance of graph keyword search, we, however, notice that the latest survey on this topic is far out of date. Consequently, there is prompt need to conduct a comprehensive survey in this research direction. Motivated by this, in this survey, we systematically review graph keyword search studies by classifying the existing works into different categories according to the specific problem definition. This survey aims to provide the researchers a comprehensive understanding of existing graph keyword search solutions.},
	pages = {142--162},
	number = {2},
	journaltitle = {Data Science and Engineering},
	shortjournal = {Data Sci. Eng.},
	author = {Yang, Jianye and Yao, Wu and Zhang, Wenjie},
	urldate = {2024-02-07},
	date = {2021-06-01},
	langid = {english},
	keywords = {Algorithm, Big graph, Index structure, Keyword search},
}

@article{necasky_modular_2022,
	title = {Modular framework for similarity-based dataset discovery using external knowledge},
	volume = {56},
	issn = {2514-9288},
	url = {https://doi.org/10.1108/DTA-09-2021-0261},
	doi = {10.1108/DTA-09-2021-0261},
	abstract = {Purpose Semantic retrieval and discovery of datasets published as open data remains a challenging task. The datasets inherently originate in the globally distributed web jungle, lacking the luxury of centralized database administration, database schemes, shared attributes, vocabulary, structure and semantics. The existing dataset catalogs provide basic search functionality relying on keyword search in brief, incomplete or misleading textual metadata attached to the datasets. The search results are thus often insufficient. However, there exist many ways of improving the dataset discovery by employing content-based retrieval, machine learning tools, third-party (external) knowledge bases, countless feature extraction methods and description models and so forth. Design/methodology/approach In this paper, the authors propose a modular framework for rapid experimentation with methods for similarity-based dataset discovery. The framework consists of an extensible catalog of components prepared to form custom pipelines for dataset representation and discovery. Findings The study proposes several proof-of-concept pipelines including experimental evaluation, which showcase the usage of the framework. Originality/value To the best of authors’ knowledge, there is no similar formal framework for experimentation with various similarity methods in the context of dataset discovery. The framework has the ambition to establish a platform for reproducible and comparable research in the area of dataset discovery. The prototype implementation of the framework is available on {GitHub}.},
	pages = {506--535},
	number = {4},
	journaltitle = {Data Technologies and Applications},
	author = {Nečaský, Martin and Škoda, Petr and Bernhauer, David and Klímek, Jakub and Skopal, Tomáš},
	urldate = {2023-05-05},
	date = {2022-01-01},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Dataset, Discovery, Framework, Knowledge graph, Search, Similarity},
}

@article{traub_agora_2021,
	title = {Agora: Bringing Together Datasets, Algorithms, Models and More in a Unified Ecosystem},
	volume = {49},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/3456859.3456861},
	doi = {10.1145/3456859.3456861},
	shorttitle = {Agora},
	abstract = {Data science and artiﬁcial intelligence are driven by a plethora of diverse data-related assets, including datasets, data streams, algorithms, processing software, compute resources, and domain knowledge. As providing all these assets requires a huge investment, data science and artiﬁcial intelligence technologies are currently dominated by a small number of providers who can afford these investments. This leads to lock-in effects and hinders features that require a ﬂexible exchange of assets among users. In this paper, we introduce Agora, our vision towards a uniﬁed ecosystem that brings together data, algorithms, models, and computational resources and provides them to a broad audience. Agora (i) treats assets as ﬁrst-class citizens and leverages a ﬁne-grained exchange of assets, (ii) allows for combining assets to novel applications, and (iii) ﬂexibly executes such applications on available resources. As a result, it enables easy creation and composition of data science pipelines as well as their scalable execution. In contrast to existing data management systems, Agora operates in a heavily decentralized and dynamic environment: Data, algorithms, and even compute resources are dynamically created, modiﬁed, and removed by different stakeholders. Agora presents novel research directions for the data management community as a whole: It requires to combine our traditional expertise in scalable data processing and management with infrastructure provisioning as well as economic and application aspects of data, algorithms, and infrastructure.},
	pages = {6--11},
	number = {4},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Traub, Jonas and Kaoudi, Zoi and Quiané-Ruiz, Jorge-Arnulfo and Markl, Volker},
	urldate = {2022-03-04},
	date = {2021-03-08},
	langid = {english},
}

@misc{mcinnes_umap_2020,
	title = {{UMAP}: Uniform Manifold Approximation and Projection for Dimension Reduction},
	url = {http://arxiv.org/abs/1802.03426},
	doi = {10.48550/arXiv.1802.03426},
	shorttitle = {{UMAP}},
	abstract = {{UMAP} (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. {UMAP} is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The {UMAP} algorithm is competitive with t-{SNE} for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, {UMAP} has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	number = {{arXiv}:1802.03426},
	publisher = {{arXiv}},
	author = {{McInnes}, Leland and Healy, John and Melville, James},
	urldate = {2024-02-13},
	date = {2020-09-17},
	eprinttype = {arxiv},
	eprint = {1802.03426 [cs, stat]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kennedy_revisiting_2022,
	title = {Revisiting Online Data Markets in 2022},
	volume = {51},
	abstract = {Well-functioning data markets match sellers with buyers to allocate data effectively. Although most of today’s data markets fall short of this ideal, there is a renewed interest in online data marketplaces that may fulﬁll the promise of data markets. In this paper, we survey participants of some of the most common data marketplaces to understand the platforms’ upsides and downsides. We ﬁnd that buyers and sellers spend the majority of their time and effort in price negotiations. Although the markets work as an effective storefront that lets buyers ﬁnd useful data fast, the high transaction costs required to negotiate price and circumvent the information asymmetry that exists between buyers and sellers indicates that today’s marketplaces are still far from offering an effective solution to data trading. We draw on the results of the interviews to present potential opportunities for improvement and future research.},
	pages = {30--37},
	number = {3},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Kennedy, Javen and Subramaniam, Pranav and Galhotra, Sainyam and Fernandez, Raul Castro},
	date = {2022},
	langid = {english},
}

@article{chen_query_2001,
	title = {Query Optimization in Compressed Database Systems},
	volume = {30},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/376284.375692},
	doi = {10.1145/376284.375692},
	abstract = {Over the last decades, improvements in {CPU} speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand {onlyIn} this paper, we present an effective approach for database compression based on lightweight, attribute-level compression techniques. We propose a {IIierarchical} Dictionary Encoding strategy that intelligently selects the most effective compression method for string-valued attributes. We show that eager and lazy decompression strategies produce sub-optimal plans for queries involving compressed string attributes. We then formalize the problem of compression-aware query optimization and propose one provably optimal and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using {TPC}-H data demonstrate the impact of our string compression methods and show the importance of compression-aware query optimization. Our approach results in up to an order speed up over existing approaches.},
	pages = {271--282},
	number = {2},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Chen, Zhiyuan and Gehrke, Johannes and Korn, Flip},
	date = {2001-05},
}

@article{le_survey_2016,
	title = {Survey on Keyword Search over {XML} Documents},
	volume = {45},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/3022860.3022863},
	doi = {10.1145/3022860.3022863},
	abstract = {Since {XML} has become a standard for information exchange over the Internet, more and more data are represented as {XML}. {XML} keyword search has been attracted a lot of interests because it provides a simple and user-friendly interface to query {XML} documents. This paper provides a survey on keyword search over {XML} document. We mainly focus on the topics of defining semantics for {XML} keyword search and the corresponding algorithms to find answers based on these semantics. We classify existing works for {XML} keyword search into three main types, which are tree-based approaches, graph-based approaches and semantics-based approaches. For each type of approaches, we further classify works into sub-classes and especially we summarize, make comparison and point out the relationships among sub-classes. In addition, for each type of approach, we point out the common problems they suffer},
	pages = {17--28},
	number = {3},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Le, Thuy Ngoc and Ling, Tok Wang},
	urldate = {2024-02-07},
	date = {2016-12-06},
}

@article{mansour_federated_2022,
	title = {Federated Data Science to Break Down Silos [Vision]},
	volume = {50},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/3516431.3516435},
	doi = {10.1145/3516431.3516435},
	abstract = {Similar to Open Data initiatives, data science as a community has launched initiatives for sharing not only data but entire pipelines, derivatives, artifacts, etc. (Open Data Science). However, the few efforts that exist focus on the technical part on how to facilitate sharing, conversion, etc. This vision paper goes a step further and proposes {KEK}, an open federated data science platform that does not only allow for sharing data science pipelines and their (meta)data but also provides methods for efficient search and, in the ideal case, even allows for combining and defining pipelines across platforms in a federated manner. In doing so, {KEK} addresses the so far neglected challenge of actually finding artifacts that are semantically related and that can be combined to achieve a certain goal.},
	pages = {16--22},
	number = {4},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Mansour, Essam and Srinivas, Kavitha and Hose, Katja},
	urldate = {2023-05-04},
	date = {2022-01-31},
}

@article{naumann_data_2014,
	title = {Data Profiling Revisited},
	volume = {42},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/2590989.2590995},
	doi = {10.1145/2590989.2590995},
	abstract = {Data profiling comprises a broad range of methods to efficiently analyze a given data set. In a typical scenario, which mirrors the capabilities of commercial data profiling tools, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, keys and foreign keys, and occasionally functional dependencies and association rules. Individual research projects have proposed several additional profiling tasks, such as the discovery of inclusion dependencies or conditional functional dependencies.Data profiling deserves a fresh look for two reasons: First, the area itself is neither established nor defined in any principled way, despite significant research activity on individual parts in the past. Second, more and more data beyond the traditional relational databases are being created and beg to be profiled. The article proposes new research directions and challenges, including interactive and incremental profiling and profiling heterogeneous and non-relational data.},
	pages = {40--49},
	number = {4},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Naumann, Felix},
	date = {2014-02},
}

@article{bharadwaj_discovering_2021,
	title = {Discovering Related Data at Scale},
	volume = {14},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3457390.3457403},
	doi = {10.14778/3457390.3457403},
	abstract = {Analysts frequently require data from multiple sources for their tasks, but finding these sources is challenging in exabyte-scale data lakes. In this paper, we address this problem for our enterprise's data lake by using machine-learning to identify related data sources. Leveraging queries made to the data lake over a month, we build a relevance model that determines whether two columns across two data streams are related or not. We then use the model to find relations at scale across tens of millions of column-pairs and thereafter construct a data relationship graph in a scalable fashion, processing a data lake that has 4.5 Petabytes of data in approximately 80 minutes. Using manually labeled datasets as ground-truth, we show that our techniques show improvements of at least 23\% when compared to state-of-the-art methods.},
	pages = {1392--1400},
	number = {8},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Bharadwaj, Sagar and Gupta, Praveen and Bhagwan, Ranjita and Guha, Saikat},
	date = {2021-04},
}

@article{bethlehem_selection_2010,
	title = {Selection Bias in Web Surveys},
	volume = {78},
	rights = {© 2010 The Author. Journal compilation © 2010 International Statistical Institute},
	issn = {1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00112.x},
	doi = {10.1111/j.1751-5823.2010.00112.x},
	abstract = {At first sight, web surveys seem to be an interesting and attractive means of data collection. They provide simple, cheap, and fast access to a large group of potential respondents. However, web surveys are not without methodological problems. Specific groups in the populations are under-represented because they have less access to Internet. Furthermore, recruitment of respondents is often based on self-selection. Both under-coverage and self-selection may lead to biased estimates. This paper describes these methodological problems. It also explores the effect of various correction techniques (adjustment weighting and use of reference surveys). This all leads to the question whether properly design web surveys can be used for data collection. The paper attempts to answer this question. It concludes that under-coverage problems may solve itself in the future, but that self-selection leads to unreliable survey outcomes.},
	pages = {161--188},
	number = {2},
	journaltitle = {International Statistical Review},
	author = {Bethlehem, Jelke},
	urldate = {2024-02-13},
	date = {2010},
	langid = {english},
	keywords = {Adjustment weighting, bias, online survey, reference survey, self-selection, under-coverage, web survey},
}

@article{hulsebos_gittables_2023,
	title = {{GitTables}: A Large-Scale Corpus of Relational Tables},
	volume = {1},
	url = {https://dl.acm.org/doi/10.1145/3588710},
	doi = {10.1145/3588710},
	shorttitle = {{GitTables}},
	abstract = {The success of deep learning has sparked interest in improving relational table tasks, like data preparation and search, with table representation models trained on large table corpora. Existing table corpora primarily contain tables extracted from {HTML} pages, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need resources with tables that resemble relational database tables. Here we introduce {GitTables}, a corpus of 1M relational tables extracted from {GitHub}. Our continuing curation aims at growing the corpus to at least 10M tables. Analyses of {GitTables} show that its structure, content, and topical coverage differ significantly from existing table corpora. We annotate table columns in {GitTables} with semantic types, hierarchical relations and descriptions from Schema.org and {DBpedia}. The evaluation of our annotation pipeline on the T2Dv2 benchmark illustrates that our approach provides results on par with human annotations. We present three applications of {GitTables}, demonstrating its value for learned semantic type detection models, schema completion methods, and benchmarks for table-to-{KG} matching, data search, and preparation. We make the corpus and code available at https://gittables.github.io.},
	pages = {30:1--30:17},
	number = {1},
	journaltitle = {Proceedings of the {ACM} on Management of Data},
	shortjournal = {Proc. {ACM} Manag. Data},
	author = {Hulsebos, Madelon and Demiralp, Çagatay and Groth, Paul},
	urldate = {2024-02-12},
	date = {2023-05-30},
	keywords = {data management, datasets, deep learning, relational tables},
}

@article{athanassoulis_data_2023,
	title = {Data Structures for Data-Intensive Applications: Tradeoffs and Design Guidelines},
	volume = {13},
	issn = {1931-7883, 1931-7891},
	url = {http://www.nowpublishers.com/article/Details/DBS-059},
	doi = {10.1561/1900000059},
	shorttitle = {Data Structures for Data-Intensive Applications},
	abstract = {Key-value data structures constitute the core of any datadriven system. They provide the means to store, search, and modify data residing at various levels of the storage and memory hierarchy, from durable storage (spinning disks, solid state disks, and other non-volatile memories) to random access memory, caches, and registers. Designing efficient data structures for given workloads has long been a focus of research and practice in both academia and industry.},
	pages = {1--168},
	number = {1},
	journaltitle = {Foundations and Trends® in Databases},
	shortjournal = {{FNT} in Databases},
	author = {Athanassoulis, Manos and Idreos, Stratos and Shasha, Dennis},
	urldate = {2024-02-08},
	date = {2023},
	langid = {english},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	eventtitle = {{NeurIPS} '20},
	pages = {9459--9474},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2024-02-07},
	date = {2020},
}

@inproceedings{izacard_leveraging_2021,
	location = {Online},
	title = {Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
	url = {https://aclanthology.org/2021.eacl-main.74},
	doi = {10.18653/v1/2021.eacl-main.74},
	abstract = {Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and {TriviaQA} open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.},
	eventtitle = {{EACL} '21},
	pages = {874--880},
	booktitle = {16th Conference of the European Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Izacard, Gautier and Grave, Edouard},
	urldate = {2024-02-07},
	date = {2021-04},
}

@inproceedings{liu_tapex_2022,
	title = {{TAPEX}: Table Pre-training via Learning a Neural {SQL} Executor},
	url = {https://openreview.net/forum?id=O50443AsCP},
	abstract = {Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose {TAPEX} to show that table pretraining can be achieved by learning a neural {SQL} executor over a synthetic corpus, which is obtained by automatically synthesizing executable {SQL} queries and their execution outputs. {TAPEX} addresses the data scarcity challenge via guiding the language model to mimic a {SQL} executor on the diverse, large-scale and highquality synthetic corpus. We evaluate {TAPEX} on four benchmark datasets. Experimental results demonstrate that {TAPEX} outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes improvements on the weakly-supervised {WikiSQL} denotation accuracy to 89.5\% (+2.3\%), the {WikiTableQuestions} denotation accuracy to 57.5\% (+4.8\%), the {SQA} denotation accuracy to 74.5\% (+3.5\%), and the {TabFact} accuracy to 84.2\% (+3.2\%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs and to achieve new state-of-the-art results on various downstream tasks. Our code can be found at https://github.com/microsoft/Table-Pretraining.},
	eventtitle = {{ICLR} '22},
	booktitle = {International Conference on Learning Representations},
	author = {Liu, Qian and Chen, Bei and Guo, Jiaqi and Ziyadi, Morteza and Lin, Zeqi and Chen, Weizhu and Lou, Jian-Guang},
	date = {2022},
}

@inproceedings{yin_tabert_2020,
	location = {Online},
	title = {{TaBERT}: Pretraining for Joint Understanding of Textual and Tabular Data},
	url = {https://aclanthology.org/2020.acl-main.745},
	doi = {10.18653/v1/2020.acl-main.745},
	shorttitle = {{TaBERT}},
	abstract = {Recent years have witnessed the burgeoning of pretrained language models ({LMs}) for text-based natural language ({NL}) understanding tasks. Such models are typically trained on free-form {NL} text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form {NL} questions and structured tabular data (e.g., database tables). In this paper we present {TaBERT}, a pretrained {LM} that jointly learns representations for {NL} sentences and (semi-)structured tables. {TaBERT} is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using {TaBERT} as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark {WikiTableQuestions}, while performing competitively on the text-to-{SQL} dataset Spider.},
	eventtitle = {{ACL} '20},
	pages = {8413--8426},
	booktitle = {58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
	urldate = {2024-02-07},
	date = {2020-07},
}

@inproceedings{herzig_tapas_2020,
	location = {Online},
	title = {{TaPas}: Weakly Supervised Table Parsing via Pre-training},
	url = {https://aclanthology.org/2020.acl-main.398},
	doi = {10.18653/v1/2020.acl-main.398},
	shorttitle = {{TaPas}},
	abstract = {Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present {TaPas}, an approach to question answering over tables without generating logical forms. {TaPas} trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. {TaPas} extends {BERT}'s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that {TaPas} outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on {SQA} from 55.1 to 67.2 and performing on par with the state-of-the-art on {WikiSQL} and {WikiTQ}, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from {WikiSQL} to {WikiTQ}, yields 48.7 accuracy, 4.2 points above the state-of-the-art.},
	eventtitle = {{ACL} '20},
	pages = {4320--4333},
	booktitle = {58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Herzig, Jonathan and Nowak, Pawel Krzysztof and Müller, Thomas and Piccinno, Francesco and Eisenschlos, Julian},
	urldate = {2024-02-07},
	date = {2020-07},
}

@article{wang_solo_2023,
	title = {Solo: Data Discovery Using Natural Language Questions Via A Self-Supervised Approach},
	volume = {1},
	url = {https://dl.acm.org/doi/10.1145/3626756},
	doi = {10.1145/3626756},
	shorttitle = {Solo},
	abstract = {Most deployed data discovery systems, such as Google Datasets, and open data portals only support keyword search. Keyword search is geared towards general audiences but limits the types of queries the systems can answer. We propose a new system that lets users write natural language questions directly. A major barrier to using this learned data discovery system is it needs expensive-to-collect training data, thus limiting its utility. In this paper, we introduce a self-supervised approach to assemble training datasets and train learned discovery systems without human intervention. It requires addressing several challenges, including the design of self-supervised strategies for data discovery, table representation strategies to feed to the models, and relevance models that work well with the synthetically generated questions. We combine all the above contributions into a system, Solo, that solves the problem end to end. The evaluation results demonstrate the new techniques outperform state-of-the-art approaches on well-known benchmarks. All in all, the technique is a stepping stone towards building learned discovery systems.},
	pages = {262:1--262:27},
	number = {4},
	journaltitle = {Proceedings of the {ACM} on Management of Data},
	shortjournal = {Proc. {ACM} Manag. Data},
	author = {Wang, Qiming and Castro Fernandez, Raul},
	urldate = {2024-02-07},
	date = {2023-12-12},
	keywords = {data discovery, natural language questions, self-supervised},
}

@online{noy_datasets_2023,
	title = {Datasets at your fingertips in Google Search},
	url = {https://blog.research.google/2023/02/datasets-at-your-fingertips-in-google.html},
	titleaddon = {Google Research},
	author = {Noy, Natasha and Benjelloun, Omar},
	urldate = {2024-02-07},
	date = {2023-02-28},
	langid = {english},
}

@article{scheuermann_multidimensional_1982,
	title = {Multidimensional B-trees for associative searching in database systems},
	volume = {7},
	issn = {0306-4379},
	url = {https://www.sciencedirect.com/science/article/pii/0306437982900242},
	doi = {10.1016/0306-4379(82)90024-2},
	abstract = {A new method for multiple attribute indexing, the Multidimensional B-Tree ({MBDT}), is developed. This method is well suited for dynamic databases, since it handles several types of associative queries efficiently and requires low-cost maintenance. Algorithms and search strategies for exact match, partial match, and range queries are presented and statistical procedures are given to estimate the average and worst case retrieval times. The applicability of our organization to practical databases is discussed and analytical tradeoffs with regard to index organizations based on k-d trees are established.},
	pages = {123--137},
	number = {2},
	journaltitle = {Information Systems},
	shortjournal = {Information Systems},
	author = {Scheuermann, Peter and Ouksel, Mohamed},
	urldate = {2024-02-07},
	date = {1982-01-01},
}

@article{fonseca_fsd50k_2021,
	title = {{FSD}50K: An Open Dataset of Human-Labeled Sound Events},
	volume = {30},
	issn = {2329-9290},
	url = {https://dl.acm.org/doi/10.1109/TASLP.2021.3133208},
	doi = {10.1109/TASLP.2021.3133208},
	shorttitle = {{FSD}50K},
	abstract = {Most existing datasets for sound event recognition ({SER}) are relatively small and/or domain-specific, with the exception of {AudioSet}, based on over 2 M tracks from {YouTube} videos and encompassing over 500 sound classes. However, {AudioSet} is not an open dataset as its official release consists of pre-computed audio features. Downloading the original audio tracks can be problematic due to {YouTube} videos gradually disappearing and usage rights issues. To provide an alternative benchmark dataset and thus foster {SER} research, we introduce {\textless}italic{\textgreater}{FSD}50K{\textless}/italic{\textgreater}, an open dataset containing over 51 k audio clips totalling over 100 h of audio manually labeled using 200 classes drawn from the {AudioSet} Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the {FSD}50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for {SER}. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for {SER} research.},
	pages = {829--852},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech and Language Processing},
	shortjournal = {{IEEE}/{ACM} Trans. Audio, Speech and Lang. Proc.},
	author = {Fonseca, Eduardo and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier},
	urldate = {2024-02-06},
	date = {2021-12-10},
}

@online{datarade_find_2024,
	title = {Find the right data, effortlessly},
	url = {https://datarade.ai/},
	author = {{Datarade}},
	urldate = {2024-02-06},
	date = {2024},
}

@online{advaneo_data_2024,
	title = {Data Marketplace: sharing platform for data analytics, data mining, {IoT}, {ML}},
	url = {https://www.advaneo-datamarketplace.de/en},
	shorttitle = {Data Marketplace},
	abstract = {Data marketplace ({IDSA} standard) for monetization/sharing of e.g. {IoT} data for data analytics, machine learning and data mining with full data sovereignty.},
	author = {{ADVANEO}},
	urldate = {2024-02-06},
	date = {2024},
	langid = {british},
}

@online{dawex_data_2024,
	title = {Data Marketplaces, Data Hubs \& Data Spaces},
	url = {https://www.dawex.com/en/},
	abstract = {Create data ecosystems with Dawex: discover  the Data Exchange technology awarded World Economic Forum Tech Pioneer 2020.},
	author = {{Dawex}},
	urldate = {2024-02-06},
	date = {2024},
	langid = {english},
}

@online{worldquant_data_2024,
	title = {Data Exchange},
	url = {https://www.worldquant.com/data-exchange/},
	abstract = {{WorldQuant} is a quantitative asset management firm, founded in 2007, with more than 850 employees across 13 countries.},
	author = {{WorldQuant}},
	urldate = {2024-02-06},
	date = {2024},
	langid = {american},
}

@online{xignite_market_2024,
	title = {Market Data Solutions},
	url = {https://www.xignite.com/},
	abstract = {Xignite is disrupting the market data industry with its {AWS} cloud-based financial data {APIs} and market data management solution.},
	author = {{Xignite}},
	urldate = {2024-02-06},
	date = {2024},
	langid = {english},
}

@article{dastin_amazon_2018,
	title = {Amazon scraps secret {AI} recruiting tool that showed bias against women},
	url = {https://www.reuters.com/article/idUSKCN1MK0AG/},
	abstract = {Amazon.com Inc's \&lt;{AMZN}.O\&gt; machine-learning specialists uncovered a big problem: their new recruiting engine did not like women.},
	journaltitle = {Reuters},
	author = {Dastin, Jeffrey},
	urldate = {2024-02-06},
	date = {2018-10-11},
	langid = {american},
}

@online{townsend_most_2017,
	title = {Most engineers are white — and so are the faces they use to train software},
	url = {https://www.vox.com/2017/1/18/14304964/data-facial-recognition-trouble-recognizing-black-white-faces-diversity},
	abstract = {A black researcher had to wear a white mask to test her own project.},
	titleaddon = {Vox},
	author = {Townsend, Tess},
	urldate = {2024-02-06},
	date = {2017-01-18},
	langid = {english},
}

@online{mulshine_major_2015,
	title = {A major flaw in Google's algorithm allegedly tagged two black people's faces with the word 'gorillas'},
	url = {https://www.businessinsider.com/google-tags-black-people-as-gorillas-2015-7},
	abstract = {Google has apologized and is "appalled" by the apparent glitch.},
	titleaddon = {Business Insider},
	author = {Mulshine, Molly},
	urldate = {2024-02-06},
	date = {2015-07-01},
	langid = {american},
}

@online{atlan_tackling_2023,
	title = {Tackling Data Catalog Challenges},
	url = {https://atlan.com/data-catalog-challenges/},
	shorttitle = {Tackling Data Catalog Challenges},
	abstract = {Tackling data catalog challenges: Learn how to mitigate them with a 10-step action plan. Discover common issues and how to choose the right solution.},
	author = {{Atlan}},
	urldate = {2024-01-31},
	date = {2023-04-28},
}

@online{kaggle_inc_kaggle_2024,
	title = {Kaggle Datasets},
	url = {https://www.kaggle.com/datasets},
	abstract = {Download Open Datasets on 1000s of Projects + Share Projects on One Platform. Explore Popular Topics Like Government, Sports, Medicine, Fintech, Food, More. Flexible Data Ingestion.},
	author = {{Kaggle Inc.}},
	urldate = {2024-01-29},
	date = {2024},
	langid = {english},
}

@software{behme_fainder_2024,
	title = {Fainder: A Fast and Accurate Index for Distribution-Aware Dataset Search},
	rights = {Apache-2.0},
	url = {https://github.com/lbhm/fainder},
	author = {Behme, Lennart and Galhotra, Sainyam and Beedkar, Kaustubh and Markl, Volker},
	urldate = {2024-01-23},
	date = {2024},
	keywords = {data-loading, deep-learning, lossy-compression},
}

@article{miller_making_2018,
	title = {Making Open Data Transparent: Data Discovery on Open Data},
	volume = {41},
	abstract = {Open Data plays a major role in open government initiatives. Governments around the world are adopting Open Data Principles promising to make their Open Data complete, primary, and timely. These properties make this data tremendously valuable. Open Data poses interesting new challenges for data integration research and we take a look at one of those challenges, data discovery. How can we find new data sets within this ever expanding sea of Open Data. How do we make this sea transparent?},
	pages = {59--70},
	number = {2},
	journaltitle = {Bulletin of the {IEEE} Computer Society Technical Committee on Data Engineering},
	author = {Miller, Renee J. and Nargesian, Fatemeh and Zhu, Erkang and Christodoulakis, Christina and Pu, Ken Q. and Andritsos, Periklis},
	date = {2018},
	langid = {english},
}

@article{nargesian_data_2023,
	title = {Data Lake Organization},
	volume = {35},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/abstract/document/9693372},
	doi = {10.1109/TKDE.2021.3091101},
	abstract = {We consider the problem of building an organizational directory of data lakes to support effective user navigation. The organization directory is defined as an acyclic graph that contains nodes representing sets of attributes and edges indicating subset relationships between nodes. A probabilistic model is constructed to model user navigational behaviour. The model also predicts the likelihood of users finding relevant tables in a data lake given an organization. We formulate the data lake organization problem as an optimization over the organizational structure in order to maximize the expected likelihood of discovering tables by navigating. An approximation algorithm is proposed with an analysis of its error bound. The effectiveness and efficiency of the algorithm are evaluated on both synthetic and real data lakes. Our experiments show that our algorithm constructs organizations that outperform many existing organizations including an existing hand-curated taxonomy, a linkage graph, and a common baseline organization. We have also conducted a formal user study which shows that navigation can help users discover relevant tables that are not easily accessible by keyword search queries. This suggests that keyword search and navigation using an organization are complementary modalities for data discovery in data lakes.},
	pages = {237--250},
	number = {1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{TKDE}},
	author = {Nargesian, Fatemeh and Pu, Ken and Ghadiri-Bashardoost, Bahar and Zhu, Erkang and Miller, Renée J.},
	urldate = {2024-01-18},
	date = {2023-01},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
}

@online{city_of_new_york_nyc_2024,
	title = {{NYC} Open Data},
	url = {https://opendata.cityofnewyork.us/},
	abstract = {{NYC} Open Data helps New Yorkers use and learn about City data},
	author = {{City of New York}},
	urldate = {2024-01-14},
	date = {2024},
	langid = {english},
}

@online{us_general_services_administration_datagov_2024,
	title = {data.gov},
	url = {https://data.gov/},
	author = {{U.S. General Services Administration}},
	urldate = {2024-01-13},
	date = {2024},
	langid = {english},
}

@online{amazon_web_services_aws_2024,
	title = {{AWS} Data Exchange},
	url = {https://aws.amazon.com/data-exchange/},
	abstract = {There is no other place where customers can find data files, data tables, and data {APIs} from a vast portfolio of third-party data sets. We continuously innovate to make the world's third-party data easy to find in one data catalog.},
	author = {{Amazon Web Services}},
	urldate = {2024-01-13},
	date = {2024},
	langid = {american},
}

@online{publications_office_of_the_european_union_dataeuropaeu_2024,
	title = {data.europa.eu},
	url = {https://data.europa.eu/en},
	author = {{Publications Office of the European Union}},
	urldate = {2024-01-13},
	date = {2024},
}

@article{rousseeuw_silhouettes_1987,
	title = {Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis},
	volume = {20},
	issn = {0377-0427},
	url = {https://www.sciencedirect.com/science/article/pii/0377042787901257},
	doi = {10.1016/0377-0427(87)90125-7},
	shorttitle = {Silhouettes},
	abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.},
	pages = {53--65},
	journaltitle = {Journal of Computational and Applied Mathematics},
	shortjournal = {Journal of Computational and Applied Mathematics},
	author = {Rousseeuw, Peter J.},
	urldate = {2023-12-21},
	date = {1987-11-01},
	keywords = {Graphical display, classification, cluster analysis, clustering validity},
}

@article{calinski_dendrite_1974,
	title = {A Dendrite Method for Cluster Analysis},
	volume = {3},
	issn = {0090-3272},
	url = {https://www.tandfonline.com/doi/abs/10.1080/03610927408827101},
	doi = {10.1080/03610927408827101},
	abstract = {A method for identifying clusters of points in a multidimensional Euclidean space is described and its application to taxonomy considered. It reconciles, in a sense, two different approaches to the investigation of the spatial relationships between the points, viz., the agglomerative and the divisive methods. A graph, the shortest dendrite of Florek etal. (1951a), is constructed on a nearest neighbour basis and then divided into clusters by applying the criterion of minimum within cluster sum of squares. This procedure ensures an effective reduction of the number of possible splits. The method may be applied to a dichotomous division, but is perfectly suitable also for a global division into any number of clusters. An informal indicator of the "best number" of clusters is suggested. It is a"variance ratio criterion" giving some insight into the structure of the points. The method is illustrated by three examples, one of which is original. The results obtained by the dendrite method are compared with those obtained by using the agglomerative method or Ward (1963) and the divisive method of Edwards and Cavalli-Sforza (1965).},
	pages = {1--27},
	number = {1},
	journaltitle = {Communications in Statistics},
	shortjournal = {Communications in Statistics},
	author = {Caliński, Tadeusz and Harabasz, Jerzy},
	date = {1974-01-01},
	note = {Publisher: Taylor \& Francis},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and {API} consistency. It has minimal dependencies and is distributed under the simpliﬁed {BSD} license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	pages = {2825--2830},
	number = {85},
	journaltitle = {Journal of Machine Learning Research},
	shortjournal = {{JMLR}},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	date = {2011},
}

@article{langenecker_sportstables_2023,
	title = {{SportsTables}: A New Corpus for Semantic Type Detection},
	volume = {23},
	issn = {1610-1995},
	url = {https://doi.org/10.1007/s13222-023-00457-y},
	doi = {10.1007/s13222-023-00457-y},
	shorttitle = {{SportsTables}},
	abstract = {Table corpora such as {VizNet} or {TURL} which contain annotated semantic types per column are important to build machine learning models for the task of automatic semantic type detection. However, there is a huge discrepancy between corpora and real-world data lakes since they contain a huge fraction of numerical data which are not present in existing corpora. Hence, in this paper, we introduce a new corpus that contains a much higher proportion of numerical columns than existing corpora. To reflect the distribution in real-world data lakes, our corpus {SportsTables} has on average approx. 86\% numerical columns, posing new challenges to existing semantic type detection models which have mainly targeted non-numerical columns so far. To demonstrate this effect, we show in this extended version paper of [18] the results of an extensive study using four different state-of-the-art approaches for semantic type detection on our new corpus. Overall, the results demonstrate significant performance differences in predicting semantic types for textual and numerical data.},
	pages = {189--197},
	number = {3},
	journaltitle = {Datenbank-Spektrum},
	shortjournal = {Datenbank Spektrum},
	author = {Langenecker, Sven and Sturm, Christoph and Schalles, Christian and Binnig, Carsten},
	urldate = {2023-11-28},
	date = {2023-11-01},
	langid = {english},
	keywords = {Column annotated corpora, Semantic type detection},
}

@inproceedings{fang_robust_2023,
	title = {Robust Heterogeneous Federated Learning under Data Corruption},
	abstract = {Model heterogeneous federated learning is a realistic and challenging problem. However, due to the limitations of data collection, storage, and transmission conditions, as well as the existence of free-rider participants, the clients may suffer from data corruption. This paper starts the first attempt to investigate the problem of data corruption in the model heterogeneous federated learning framework. We design a novel method named Augmented Heterogeneous Federated Learning ({AugHFL}), which consists of two stages: 1) In the local update stage, a corruption-robust data augmentation strategy is adopted to minimize the adverse effects of local corruption while enabling the models to learn rich local knowledge. 2) In the collaborative update stage, we design a robust re-weighted communication approach, which implements communication between heterogeneous models while mitigating corrupted knowledge transfer from others. Extensive experiments demonstrate the effectiveness of our method in coping with various corruption patterns in the model heterogeneous federated learning setting.},
	eventtitle = {{ICCV} '23},
	pages = {5020--5030},
	booktitle = {{IEEE}/{CVF} International Conference on Computer Vision},
	author = {Fang, Xiuwen and Ye, Mang and Yang, Xiyuan},
	date = {2023-10},
	langid = {english},
}

@article{zhang_refiner_2021,
	title = {Refiner: A Reliable Incentive-Driven Federated Learning System Powered by Blockchain},
	volume = {14},
	abstract = {Modern mobile applications often produce decentralized data, i.e., a huge amount of privacy-sensitive data distributed over a large number of mobile devices. Techniques for learning models from decentralized data must properly handle two natures of such data, namely privacy and massive engagement. Federated learning ({FL}) is a promising approach for such a learning task since the technique learns models from data without exposing privacy. However, traditional {FL} methods assume that the participating mobile devices are honest volunteers. This assumption makes traditional {FL} methods unsuitable for applications where two kinds of participants are engaged: 1) self-interested participants who, without economical stimulus, are reluctant to contribute their computing resources unconditionally, and 2) malicious participants who send corrupt updates to disrupt the learning process. This paper proposes Refiner, a reliable federated learning system for tackling the challenges introduced by massive engagements of self-interested and malicious participants. Refiner is built upon Ethereum, a public blockchain platform. To engage self-interested participants, we introduce an incentive mechanism which rewards each participant in terms of the amount of its training data and the performance of its local updates. To handle malicious participants, we propose an audit scheme which employs a committee of randomly chosen validators for punishing them with no reward and preclude corrupt updates from the global model. The proposed incentive and audit scheme is implemented with cryptocurrency and smart contract, two primitives offered by Ethereum. This paper demonstrates the main features of Refiner by training a digit classification model on the {MNIST} dataset.},
	pages = {2659--2662},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Zhang, Zhebin and Dong, Dajie and Ma, Yuhang and Ying, Yilong and Jiang, Dawei and Chen, Ke and Shou, Lidan and Chen, Gang},
	date = {2021},
	langid = {english},
}

@article{zhao_privacy-preserving_2021,
	title = {Privacy-Preserving Blockchain-Based Federated Learning for {IoT} Devices},
	volume = {8},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2020.3017377},
	abstract = {Home appliance manufacturers strive to obtain feedback from users to improve their products and services to build a smart home system. To help manufacturers develop a smart home system, we design a federated learning ({FL}) system leveraging a reputation mechanism to assist home appliance manufacturers to train a machine learning model based on customers’ data. Then, manufacturers can predict customers’ requirements and consumption behaviors in the future. The working flow of the system includes two stages: in the first stage, customers train the initial model provided by the manufacturer using both the mobile phone and the mobile-edge computing ({MEC}) server. Customers collect data from various home appliances using phones, and then they download and train the initial model with their local data. After deriving local models, customers sign on their models and send them to the blockchain. In case customers or manufacturers are malicious, we use the blockchain to replace the centralized aggregator in the traditional {FL} system. Since records on the blockchain are untampered, malicious customers or manufacturers’ activities are traceable. In the second stage, manufacturers select customers or organizations as miners for calculating the averaged model using received models from customers. By the end of the crowdsourcing task, one of the miners, who is selected as the temporary leader, uploads the model to the blockchain. To protect customers’ privacy and improve the test accuracy, we enforce differential privacy ({DP}) on the extracted features and propose a new normalization technique. We experimentally demonstrate that our normalization technique outperforms batch normalization when features are under {DP} protection. In addition, to attract more customers to participate in the crowdsourcing {FL} task, we design an incentive mechanism to award participants.},
	pages = {1817--1829},
	number = {3},
	journaltitle = {{IEEE} Internet of Things Journal},
	author = {Zhao, Yang and Zhao, Jun and Jiang, Linshan and Tan, Rui and Niyato, Dusit and Li, Zengxiang and Lyu, Lingjuan and Liu, Yingbo},
	date = {2021-02},
	keywords = {Blockchain, Computational modeling, Crowdsourcing, Differential privacy, Home appliances, Internet of Things ({IoT}), crowdsourcing, differential privacy ({DP}), federated learning ({FL}), mobile-edge computing ({MEC})},
}

@misc{qi_fedsampling_2023,
	title = {{FedSampling}: A Better Sampling Strategy for Federated Learning},
	url = {http://arxiv.org/abs/2306.14245},
	shorttitle = {{FedSampling}},
	abstract = {Federated learning ({FL}) is an important technique for learning models from decentralized data in a privacy-preserving way. Existing {FL} methods usually uniformly sample clients for local model learning in each round. However, different clients may have significantly different data sizes, and the clients with more data cannot have more opportunities to contribute to model training, which may lead to inferior performance. In this paper, instead of client uniform sampling, we propose a novel data uniform sampling strategy for federated learning ({FedSampling}), which can effectively improve the performance of federated learning especially when client data size distribution is highly imbalanced across clients. In each federated learning round, local data on each client is randomly sampled for local model learning according to a probability based on the server desired sample size and the total sample size on all available clients. Since the data size on each client is privacy-sensitive, we propose a privacy-preserving way to estimate the total sample size with a differential privacy guarantee. Experiments on four benchmark datasets show that {FedSampling} can effectively improve the performance of federated learning.},
	number = {{arXiv}:2306.14245},
	publisher = {{arXiv}},
	author = {Qi, Tao and Wu, Fangzhao and Lyu, Lingjuan and Huang, Yongfeng and Xie, Xing},
	urldate = {2023-08-17},
	date = {2023-06-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2306.14245 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{yang_introduction_2023,
	title = {An Introduction to Neural Data Compression},
	volume = {15},
	issn = {1572-2740},
	url = {https://doi.org/10.1561/0600000107},
	doi = {10.1561/0600000107},
	abstract = {Neural compression is the application of neural networks and other machine learning methods to data compression. Recent advances in statistical machine learning have opened up new possibilities for data compression, allowing compression algorithms to be learned end-to-end from data using powerful generative models such as normalizing flows, variational autoencoders, diffusion probabilistic models, and generative adversarial networks. This monograph aims to introduce this field of research to a broader machine learning audience by reviewing the necessary background in information theory (e.g., entropy coding, rate-distortion theory) and computer vision (e.g., image quality assessment, perceptual metrics), and providing a curated guide through the essential ideas and methods in the literature thus far.},
	pages = {113--200},
	number = {2},
	journaltitle = {Foundations and Trends® in Computer Graphics and Vision},
	shortjournal = {Found. Trends. Comput. Graph. Vis.},
	author = {Yang, Yibo and Mandt, Stephan and Theis, Lucas},
	date = {2023-04},
	note = {Place: Hanover, {MA}, {USA}
Publisher: Now Publishers Inc.},
}

@inproceedings{dwork_calibrating_2006,
	location = {Berlin, Heidelberg},
	title = {Calibrating Noise to Sensitivity in Private Data Analysis},
	isbn = {978-3-540-32732-5},
	abstract = {We continue a line of research initiated in [10,11] on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.},
	pages = {265--284},
	booktitle = {Theory of Cryptography},
	publisher = {Springer Berlin Heidelberg},
	author = {Dwork, Cynthia and {McSherry}, Frank and Nissim, Kobbi and Smith, Adam},
	date = {2006},
}

@article{fan_semantics-aware_2023,
	title = {Semantics-Aware Dataset Discovery from Data Lakes with Contextualized Column-Based Representation Learning},
	volume = {16},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3587136.3587146},
	doi = {10.14778/3587136.3587146},
	abstract = {Dataset discovery from data lakes is essential in many real application scenarios. In this paper, we propose Starmie, an end-to-end framework for dataset discovery from data lakes (with table union search as the main use case). Our proposed framework features a contrastive learning method to train column encoders from pre-trained language models in a fully unsupervised manner. The column encoder of Starmie captures the rich contextual semantic information within tables by leveraging a contrastive multi-column pre-training strategy. We utilize the cosine similarity between column embedding vectors as the column unionability score and propose a filter-and-verification framework that allows exploring a variety of design choices to compute the unionability score between two tables accordingly. Empirical results on real table benchmarks show that Starmie outperforms the best-known solutions in the effectiveness of table union search by 6.8 in {MAP} and recall. Moreover, Starmie is the first to employ the {HNSW} (Hierarchical Navigable Small World) index to accelerate query processing of table union search which provides a 3,000X performance gain over the linear scan baseline and a 400X performance gain over an {LSH} index (the state-of-the-art solution for data lake indexing).},
	pages = {1726--1739},
	number = {7},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Fan, Grace and Wang, Jin and Li, Yuliang and Zhang, Dan and Miller, Renée J.},
	urldate = {2023-07-06},
	date = {2023-05-08},
}

@article{esmailoghli_mate_2022,
	title = {{MATE}: Multi-Attribute Table Extraction},
	volume = {15},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3529337.3529353},
	doi = {10.14778/3529337.3529353},
	shorttitle = {{MATE}},
	abstract = {A core operation in data discovery is to find joinable tables for a given table. Real-world tables include both unary and n-ary join keys. However, existing table discovery systems are optimized for unary joins and are ineffective and slow in the existence of n-ary keys. In this paper, we introduce Mate, a table discovery system that leverages a novel hash-based index that enables n-ary join discovery through a space-efficient super key. We design a filtering layer that uses a novel hash, Xash. This hash function encodes the syntactic features of all column values and aggregates them into a super key, which allows the system to efficiently prune tables with non-joinable rows. Our join discovery system is able to prune up to 1000x more false positives and leads to over 60x faster table discovery in comparison to state-of-the-art.},
	pages = {1684--1696},
	number = {8},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Esmailoghli, Mahdi and Quiané-Ruiz, Jorge-Arnulfo and Abedjan, Ziawasch},
	urldate = {2023-07-06},
	date = {2022-04-01},
}

@inproceedings{briggs_federated_2020,
	title = {Federated Learning with Hierarchical Clustering of Local Updates to Improve Training on Non-{IID} Data},
	doi = {10.1109/IJCNN48605.2020.9207469},
	abstract = {Federated learning ({FL}) is a well established method for performing machine learning tasks over massively distributed data. However in settings where data is distributed in a non-iid (not independent and identically distributed) fashion - as is typical in real world situations - the joint model produced by {FL} suffers in terms of test set accuracy and/or communication costs compared to training on iid data. We show that learning a single joint model is often not optimal in the presence of certain types of non-iid data. In this work we present a modification to {FL} by introducing a hierarchical clustering step ({FL}+{HC}) to separate clusters of clients by the similarity of their local updates to the global joint model. Once separated, the clusters are trained independently and in parallel on specialised models. We present a robust empirical analysis of the hyperparameters for {FL}+{HC} for several iid and non-iid settings. We show how {FL}+{HC} allows model training to converge in fewer communication rounds (significantly so under some non-iid settings) compared to {FL} without clustering. Additionally, {FL}+{HC} allows for a greater percentage of clients to reach a target accuracy compared to standard {FL}. Finally we make suggestions for good default hyperparameters to promote superior performing specialised models without modifying the the underlying federated learning communication protocol.},
	eventtitle = {{IJCNN} '20},
	pages = {1--9},
	booktitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Briggs, Christopher and Fan, Zhong and Andras, Peter},
	date = {2020-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Cats, Clustering algorithms, Data models, Distributed databases, Merging, Task analysis, Training, clustering applications, distributed machine learning, federated learning},
}

@inproceedings{skopal_similarity_2021,
	location = {Dortmund, Germany},
	title = {Similarity vs. Relevance: From Simple Searches to Complex Discovery},
	isbn = {978-3-030-89657-7},
	abstract = {Similarity queries play the crucial role in content-based retrieval. The similarity function itself is regarded as the function of relevance between a query object and objects from database; the most similar objects are understood as the most relevant. However, such an automatic adoption of similarity as relevance leads to limited applicability of similarity search in domains like entity discovery, where relevant objects are not supposed to be similar in the traditional meaning. In this paper, we propose the meta-model of data-transitive similarity operating on top of a particular similarity model and a database. This meta-model enables to treat directly non-similar objects as similar if there exists a chain of objects having the neighboring members similar enough. Hence, this approach places the similarity in the role of relevance, where objects do not need to be directly similar but still remain relevant to each other (transitively similar). The data-transitive similarity concept allows to use standard similarity-search methods (queries, joins, rankings, analytics) in more complex tasks, like the entity discovery, where relevant results are often complementary or orthogonal to the query, rather than directly similar. Moreover, we show the data-transitive similarity is inherently self-explainable and non-metric. We discuss the approach in the domain of open dataset discovery.},
	eventtitle = {{SISAP} '21},
	pages = {104--117},
	booktitle = {International Conference on Similarity Search and Applications},
	publisher = {Springer International Publishing},
	author = {Skopal, Tomáš and Bernhauer, David and Škoda, Petr and Klímek, Jakub and Nečaský, Martin},
	editor = {Reyes, Nora and Connor, Richard and Kriege, Nils and Kazempour, Daniyal and Bartolini, Ilaria and Schubert, Erich and Chen, Jian-Jia},
	date = {2021},
}

@article{zhang_semantic_2021,
	title = {Semantic Table Retrieval Using Keyword and Table Queries},
	volume = {15},
	issn = {1559-1131},
	url = {https://dl.acm.org/doi/10.1145/3441690},
	doi = {10.1145/3441690},
	abstract = {Tables on the Web contain a vast amount of knowledge in a structured form. To tap into this valuable resource, we address the problem of table retrieval: answering an information need with a ranked list of tables. We investigate this problem in two different variants, based on how the information need is expressed: as a keyword query or as an existing table (“query-by-table”). The main novel contribution of this work is a semantic table retrieval framework for matching information needs (keyword or table queries) against tables. Specifically, we (i) represent queries and tables in multiple semantic spaces (both discrete sparse and continuous dense vector representations) and (ii) introduce various similarity measures for matching those semantic representations. We consider all possible combinations of semantic representations and similarity measures and use these as features in a supervised learning model. Using two purpose-built test collections based on Wikipedia tables, we demonstrate significant and substantial improvements over state-of-the-art baselines.},
	pages = {1--33},
	number = {3},
	journaltitle = {{ACM} Transactions on the Web},
	shortjournal = {{ACM} Trans. Web},
	author = {Zhang, Shuo and Balog, Krisztian},
	urldate = {2023-05-09},
	date = {2021},
	keywords = {Table search, table retrieval},
}

@inproceedings{zhou_fedfa_2023,
	title = {{FedFA}: Federated Feature Augmentation},
	url = {https://openreview.net/forum?id=U9yFP90jU0},
	abstract = {Federated learning is a distributed paradigm that allows multiple parties to collaboratively train deep models without exchanging the raw data. However, the data distribution among clients is naturally non-i.i.d., which leads to severe degradation of the learnt model. The primary goal of this paper is to develop a robust federated learning algorithm to address feature shift in clients’ samples, which can be caused by various factors, e.g., acquisition differences in medical imaging. To reach this goal, we propose {FEDFA} to tackle federated learning from a distinct perspective of federated feature augmentation. {FEDFA} is based on a major insight that each client’s data distribution can be characterized by statistics (i.e., mean and standard deviation) of latent features; and it is likely to manipulate these local statistics globally, i.e., based on information in the entire federation, to let clients have a better sense of the underlying distribution and therefore alleviate local data bias. Based on this insight, we propose to augment each local feature statistic probabilistically based on a normal distribution, whose mean is the original statistic and variance quantiﬁes the augmentation scope. Key to our approach is the determination of a meaningful Gaussian variance, which is accomplished by taking into account not only biased data of each individual client, but also underlying feature statistics characterized by all participating clients. We offer both theoretical and empirical justiﬁcations to verify the effectiveness of {FEDFA}. Our code is available at https://github.com/tfzhou/{FedFA}.},
	eventtitle = {{ICLR} '23},
	booktitle = {11th International Conference on Learning Representations},
	author = {Zhou, Tianfei and Konukoglu, Ender},
	date = {2023},
}

@article{chiarot_time_2023,
	title = {Time Series Compression Survey},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3560814},
	doi = {10.1145/3560814},
	abstract = {Smart objects are increasingly widespread and their ecosystem, also known as the Internet of Things ({IoT}), is relevant in many application scenarios. The huge amount of temporally annotated data produced by these smart devices demands efficient techniques for the transfer and storage of time series data. Compression techniques play an important role toward this goal and, even though standard compression methods could be used with some benefit, there exist several ones that specifically address the case of time series by exploiting their peculiarities to achieve more effective compression and more accurate decompression in the case of lossy compression techniques. This article provides a state-of-the-art survey of the principal time series compression techniques, proposing a taxonomy to classify them considering their overall approach and their characteristics. Furthermore, we analyze the performances of the selected algorithms by discussing and comparing the experimental results that were provided in the original articles. The goal of this article is to provide a comprehensive and homogeneous reconstruction of the state-of-the-art, which is currently fragmented across many articles that use different notations and where the proposed methods are not organized according to a classification.},
	pages = {1--32},
	number = {10},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Chiarot, Giacomo and Silvestri, Claudio},
	urldate = {2023-06-13},
	date = {2023-02-02},
	keywords = {Time series, compression, streams},
}

@misc{helali_linked_2023,
	title = {Linked Data Science Powered by Knowledge Graphs},
	url = {http://arxiv.org/abs/2303.02204},
	doi = {10.48550/arXiv.2303.02204},
	abstract = {In recent years, we have witnessed a growing interest in data science not only from academia but particularly from companies investing in data science platforms to analyze large amounts of data. In this process, a myriad of data science artifacts, such as datasets and pipeline scripts, are created. Yet, there has so far been no systematic attempt to holistically exploit the collected knowledge and experiences that are implicitly contained in the specification of these pipelines, e.g., compatible datasets, cleansing steps, {ML} algorithms, parameters, etc. Instead, data scientists still spend a considerable amount of their time trying to recover relevant information and experiences from colleagues, trial and error, lengthy exploration, etc. In this paper, we, therefore, propose a scalable system ({KGLiDS}) that employs machine learning to extract the semantics of data science pipelines and captures them in a knowledge graph, which can then be exploited to assist data scientists in various ways. This abstraction is the key to enabling Linked Data Science since it allows us to share the essence of pipelines between platforms, companies, and institutions without revealing critical internal information and instead focusing on the semantics of what is being processed and how. Our comprehensive evaluation uses thousands of datasets and more than thirteen thousand pipeline scripts extracted from data discovery benchmarks and the Kaggle portal and shows that {KGLiDS} significantly outperforms state-of-the-art systems on related tasks, such as dataset recommendation and pipeline classification.},
	number = {{arXiv}:2303.02204},
	publisher = {{arXiv}},
	author = {Helali, Mossad and Vashisth, Shubham and Carrier, Philippe and Hose, Katja and Mansour, Essam},
	urldate = {2023-05-09},
	date = {2023-03-09},
	eprinttype = {arxiv},
	eprint = {2303.02204 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{helal_demonstration_2021,
	title = {A Demonstration of {KGLac}: A Data Discovery and Enrichment Platform for Data Science},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3476311.3476317},
	doi = {10.14778/3476311.3476317},
	shorttitle = {A demonstration of {KGLac}},
	abstract = {Data science growing success relies on knowing where a relevant dataset exists, understanding its impact on a specific task, finding ways to enrich a dataset, and leveraging insights derived from it. With the growth of open data initiatives, data scientists need an extensible set of effective discovery operations to find relevant data from their enterprise datasets accessible via data discovery systems or open datasets accessible via data portals. Existing portals and systems suffer from limited discovery support and do not track the use of a dataset and insights derived from it. We will demonstrate {KGLac}, a system that captures metadata and semantics of datasets to construct a knowledge graph ({GLac}) interconnecting data items, e.g., tables and columns. {KGLac} supports various data discovery operations via {SPARQL} queries for table discovery, unionable and joinable tables, plus annotation with related derived insights. We harness a broad range of Machine Learning ({ML}) approaches with {GLac} to enable automatic graph learning for advanced and semantic data discovery. The demo will showcase how {KGLac} facilitates data discovery and enrichment while developing an {ML} pipeline to evaluate potential gender salary bias in {IT} jobs.},
	pages = {2675--2678},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Helal, Ahmed and Helali, Mossad and Ammar, Khaled and Mansour, Essam},
	urldate = {2023-05-08},
	date = {2021-07-01},
}

@article{neumaier_automated_2016,
	title = {Automated Quality Assessment of Metadata across Open Data Portals},
	volume = {8},
	issn = {1936-1955},
	url = {https://dl.acm.org/doi/10.1145/2964909},
	doi = {10.1145/2964909},
	abstract = {The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks ({CKAN}, Socrata, {OpenDataSoft}) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.},
	pages = {1--29},
	number = {1},
	journaltitle = {Journal of Data and Information Quality},
	shortjournal = {J. Data Inf. Qual.},
	author = {Neumaier, Sebastian and Umbrich, Jürgen and Polleres, Axel},
	urldate = {2023-05-05},
	date = {2016-10-25},
	keywords = {Open Data, data portal, data quality, quality assessment},
}

@inproceedings{mork_facilitating_2008,
	location = {Linz, Austria},
	title = {Facilitating Discovery on the Private Web Using Dataset Digests},
	isbn = {978-1-60558-349-5},
	url = {https://doi.org/10.1145/1497308.1497391},
	doi = {10.1145/1497308.1497391},
	abstract = {Whereas strategies for discovering content on the surface web are commonplace, similar strategies for the private web are nonexistent. In this paper we first establish a formal framework for advertising the existence of private web resources that subsumes many existing summarization strategies based on succinct statistical summaries (which we call digests). We then investigate the tradeoff between the data owners' desires to minimize disclosure and the searchers' desires to minimize query error, demonstrating that our techniques are superior to k-anonymity. Finally, we show that our techniques for summarization do, in fact, make it possible to discover private web data resources.},
	eventtitle = {{iiWAS} '08},
	pages = {451--455},
	booktitle = {10th International Conference on Information Integration and Web-Based Applications \& Services},
	author = {Mork, Peter and Smith, Ken and Blaustein, Barbara and Wolf, Chris and Sarver, Keri},
	date = {2008},
	keywords = {dataset discovery, disclosure, private web, search engine, structured data},
}

@article{koesten_everything_2020,
	title = {Everything you always wanted to know about a dataset: Studies in data summarisation},
	volume = {135},
	issn = {1071-5819},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071581918306153},
	doi = {10.1016/j.ijhcs.2019.10.004},
	shorttitle = {Everything you always wanted to know about a dataset},
	abstract = {Summarising data as text helps people make sense of it. It also improves data discovery, as search algorithms can match this text against keyword queries. In this paper, we explore the characteristics of text summaries of data in order to understand how meaningful summaries look like. We present two complementary studies: a data-search diary study with 69 students, which offers insight into the information needs of people searching for data; and a summarisation study, with a lab and a crowdsourcing component with overall 80 data-literate participants, who produced summaries for 25 datasets. In each study we carried out a qualitative analysis to identify key themes and commonly mentioned dataset attributes, which people consider when searching and making sense of data. The results helped us design a template to create more meaningful textual representations of data, alongside guidelines for improving data-search experience overall.},
	pages = {102367},
	journaltitle = {International Journal of Human-Computer Studies},
	shortjournal = {International Journal of Human-Computer Studies},
	author = {Koesten, Laura and Simperl, Elena and Blount, Tom and Kacprzak, Emilia and Tennison, Jeni},
	urldate = {2023-05-05},
	date = {2020-03},
	langid = {english},
}

@inproceedings{skoda_evaluation_2021,
	location = {New York, {NY}, {USA}},
	title = {Evaluation Framework for Search Methods Focused on Dataset Findability in Open Data Catalogs},
	isbn = {978-1-4503-8922-8},
	url = {https://dl.acm.org/doi/10.1145/3428757.3429973},
	doi = {10.1145/3428757.3429973},
	series = {{iiWAS} '20},
	abstract = {Many institutions publish datasets as Open Data in catalogs, however, their retrieval remains problematic issue due to the absence of dataset search benchmarking. We propose a framework for evaluating findability of datasets, regardless of retrieval models used. As task-agnostic labeling of datasets by ground truth turns out to be infeasible in the general domain of open data datasets, the proposed framework is based on evaluation of entire retrieval scenarios that mimic complex retrieval tasks. In addition to the framework we present a proof of concept specification and evaluation on several similarity-based retrieval models and several dataset discovery scenarios within a catalog, using our experimental evaluation tool. Instead of traditional matching of query with metadata of all the datasets, in similarity-based retrieval the query is formulated using a set of datasets (query by example) and the most similar datasets to the query set are retrieved from the catalog as a result.},
	eventtitle = {{iiWAS} '21},
	pages = {200--209},
	booktitle = {22nd International Conference on Information Integration and Web-based Applications \& Services},
	publisher = {Association for Computing Machinery},
	author = {Škoda, Petr and Bernhauer, David and Nečaský, Martin and Klímek, Jakub and Skopal, Tomáš},
	urldate = {2023-05-04},
	date = {2021-01-27},
	keywords = {catalogs, findability, open data, similarity},
}

@article{bernhauer_open_2022,
	title = {Open dataset discovery using context-enhanced similarity search},
	volume = {64},
	issn = {0219-1377, 0219-3116},
	url = {https://link.springer.com/10.1007/s10115-022-01751-z},
	doi = {10.1007/s10115-022-01751-z},
	abstract = {Today, open data catalogs enable users to search for datasets with full-text queries in metadata records combined with simple faceted ﬁltering. Using this combination, a user is able to discover a signiﬁcant number of the datasets relevant to a user’s search intent. However, there still remain relevant datasets that are hard to ﬁnd because of the enormous sparsity of their metadata (e.g., several keywords). As an alternative, in this paper, we propose an approach to dataset discovery based on similarity search over metadata descriptions enhanced by various semantic contexts. In general, the semantic contexts enrich the dataset metadata in a way that enables the identiﬁcation of additional relevant datasets to a query that could not be retrieved using just the keyword or full-text search. In experimental evaluation we show that context-enhanced similarity retrieval methods increase the ﬁndability of relevant datasets, improving thus the retrieval recall that is critical in dataset discovery scenarios. As a part of the evaluation, we created a catalog-like user interface for dataset discovery and recorded streams of user actions that served us to create the ground truth. For the sake of reproducibility, we published the entire evaluation testbed.},
	pages = {3265--3291},
	number = {12},
	journaltitle = {Knowledge and Information Systems},
	shortjournal = {Knowl Inf Syst},
	author = {Bernhauer, David and Nečaský, Martin and Škoda, Petr and Klímek, Jakub and Skopal, Tomáš},
	urldate = {2023-05-04},
	date = {2022-12},
	langid = {english},
}

@article{chepurko_arda_2020,
	title = {{ARDA}: Automatic Relational Data Augmentation for Machine Learning},
	volume = {13},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3397230.3397235},
	doi = {10.14778/3397230.3397235},
	shorttitle = {{ARDA}},
	abstract = {Automatic machine learning ({AML}) is a family of techniques to automate the process of training predictive models, aiming to both improve performance and make machine learning more accessible. While many recent works have focused on aspects of the machine learning pipeline like model selection, hyperparameter tuning, and feature selection, relatively few works have focused on automatic data augmentation. Automatic data augmentation involves ﬁnding new features relevant to the user’s predictive task with minimal “human-in-the-loop” involvement.},
	pages = {1373--1387},
	number = {9},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Chepurko, Nadiia and Marcus, Ryan and Zgraggen, Emanuel and Fernandez, Raul Castro and Kraska, Tim and Karger, David},
	urldate = {2023-04-20},
	date = {2020-05},
	langid = {english},
}

@report{shteyn_federated_2023,
	title = {Federated Learning: An Introduction},
	url = {https://www.theodi.org/article/federated-learning-an-introduction-report/},
	shorttitle = {Federated learning},
	abstract = {After decades of increased data collection, there is a greater demand for data privacy now more than ever. Privacy enhancing technologies ({PETs}) could facilitate greater sharing of sensiti...},
	pages = {32},
	institution = {Open Data Institute},
	author = {Shteyn, Anastasia and Inverarity, Calum and Kollnig, Konrad and Adigun, Yusuff},
	urldate = {2023-03-20},
	date = {2023-01-23},
	langid = {english},
}

@inproceedings{tang_incentive_2021,
	title = {An Incentive Mechanism for Cross-Silo Federated Learning: A Public Goods Perspective},
	doi = {10.1109/INFOCOM42981.2021.9488705},
	shorttitle = {An Incentive Mechanism for Cross-Silo Federated Learning},
	abstract = {In cross-silo federated learning ({FL}), organizations cooperatively train a global model with their local data. The organizations, however, may be heterogeneous in terms of their valuation on the precision of the trained global model and their training cost. Meanwhile, the computational and communication resources of the organizations are non-excludable public goods. That is, even if an organization does not perform any local training, other organizations cannot prevent that organization from using the outcome of their resources (i.e., the trained global model). To address the organization heterogeneity and the public goods feature, in this paper, we formulate a social welfare maximization problem and propose an incentive mechanism for cross-silo {FL}. With the proposed mechanism, organizations can achieve not only social welfare maximization but also individual rationality and budget balance. Moreover, we propose a distributed algorithm that enables organizations to maximize the social welfare without knowing the valuation and cost of each other. Our simulations with {MNIST} dataset show that the proposed algorithm converges faster than a benchmark method. Furthermore, when organizations have higher valuation on precision, the proposed mechanism and algorithm are more beneficial in the sense that the organizations can achieve higher social welfare through participating in cross-silo {FL}.},
	eventtitle = {{INFOCOM} '21},
	pages = {1--10},
	booktitle = {{IEEE} Conference on Computer Communications},
	author = {Tang, Ming and Wong, Vincent W.S.},
	date = {2021-05},
	note = {{ISSN}: 2641-9874},
	keywords = {Computational modeling, Data models, Distributed databases, Federated learning, Organizations, Resource management, Simulation, Training, game theory, incentive mechanism, public goods, resource allocation},
}

@misc{zeng_comprehensive_2021,
	title = {A Comprehensive Survey of Incentive Mechanism for Federated Learning},
	url = {http://arxiv.org/abs/2106.15406},
	abstract = {Federated learning utilizes various resources provided by participants to collaboratively train a global model, which potentially address the data privacy issue of machine learning. In such promising paradigm, the performance will be deteriorated without sufﬁcient training data and other resources in the learning process. Thus, it is quite crucial to inspire more participants to contribute their valuable resources with some payments for federated learning. In this paper, we present a comprehensive survey of incentive schemes for federate learning. Speciﬁcally, we identify the incentive problem in federated learning and then provide a taxonomy for various schemes. Subsequently, we summarize the existing incentive mechanisms in terms of the main techniques, such as Stackelberg game, auction, contract theory, Shapley value, reinforcement learning, blockchain. By reviewing and comparing some impressive results, we ﬁgure out three directions for the future study.},
	number = {{arXiv}:2106.15406},
	publisher = {{arXiv}},
	author = {Zeng, Rongfei and Zeng, Chao and Wang, Xingwei and Li, Bo and Chu, Xiaowen},
	urldate = {2023-03-16},
	date = {2021-06-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2106.15406 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@misc{fernandez_data-sharing_2023,
	title = {Data-Sharing Markets: Model, Protocol, and Algorithms to Incentivize the Formation of Data-Sharing Consortia},
	abstract = {Organizations that would mutually benefit from pooling their data are otherwise wary of sharing. This is because sharing data is costly—in time and effort—and, at the same time, the benefits of sharing are not clear. Without a clear cost-benefit analysis, participants default in not sharing. As a consequence, many opportunities to create valuable data-sharing consortia never materialize and the value of data remains locked.},
	author = {Fernandez, Raul Castro},
	date = {2023},
	langid = {english},
}

@article{arnaout_effective_2018,
	title = {Effective Searching of {RDF} Knowledge Graphs},
	volume = {48},
	issn = {1570-8268},
	url = {https://www.sciencedirect.com/science/article/pii/S1570826817300677},
	doi = {https://doi.org/10.1016/j.websem.2017.12.001},
	abstract = {{RDF} knowledge graphs are typically searched using triple-pattern queries. Often, triple-pattern queries will return too many or too few results, making it difficult for users to find relevant answers to their information needs. To remedy this, we propose a general framework for effective searching of {RDF} knowledge graphs. Our framework extends both the searched knowledge graph and triple-pattern queries with keywords to allow users to form a wider range of queries. In addition, it provides result ranking based on statistical machine translation, and performs automatic query relaxation to improve query recall. Finally, we also define a notion of result diversity in the setting of {RDF} data and provide mechanisms to diversify {RDF} search results using Maximal Marginal Relevance. We evaluate the effectiveness of our retrieval framework using various carefully-designed user studies on {DBpedia}, a large and real-world {RDF} knowledge graph.},
	pages = {66--84},
	journaltitle = {Journal of Web Semantics},
	author = {Arnaout, Hiba and Elbassuoni, Shady},
	date = {2018},
	keywords = {Diversity, {RDF}, Ranking, Relaxation},
}

@incollection{bhowmick_saqr_2014,
	location = {Cham},
	title = {{SAQR}: An Efficient Scheme for Similarity-Aware Query Refinement},
	volume = {8421},
	isbn = {978-3-319-05810-8},
	url = {http://link.springer.com/10.1007/978-3-319-05810-8_8},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{SAQR}},
	abstract = {Query reﬁnement techniques enable database systems to automatically adjust a submitted query so that its result satisﬁes some speciﬁed constraints. While current techniques are fairly successful in generating reﬁned queries based on cardinality constraints, they are rather oblivious to the (dis)similarity between the input query and its corresponding reﬁned version. Meanwhile, enforcing a similarity-aware query reﬁnement is a rather challenging task as it would require an exhaustive examination of the large space of possible query reﬁnements. To address this challenge, we propose a novel scheme for efﬁcient Similarity-aware Query Reﬁnement ({SAQR}). {SAQR} aims to balance the tradeoff between satisfying the cardinality and similarity constraints imposed on the reﬁned query so that to maximize its overall beneﬁt to the user. To achieve that goal, {SAQR} implements efﬁcient strategies to minimize the costs incurred in exploring the available search space. In particular, {SAQR} utilizes both similarity-based and cardinality-based pruning techniques to bound the search space and quickly ﬁnd a reﬁned query that meets the user expectations. Our experimental evaluation shows the scalability exhibited by {SAQR} under various workload settings, and the signiﬁcant beneﬁts it provides.},
	pages = {110--125},
	booktitle = {Database Systems for Advanced Applications},
	publisher = {Springer International Publishing},
	author = {Albarrak, Abdullah and Sharaf, Mohamed A. and Zhou, Xiaofang},
	editor = {Bhowmick, Sourav S. and Dyreson, Curtis E. and Jensen, Christian S. and Lee, Mong Li and Muliantara, Agus and Thalheim, Bernhard},
	urldate = {2022-12-21},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-05810-8_8},
}

@incollection{sack_rdf_2016,
	location = {Cham},
	title = {{RDF} Query Relaxation Strategies Based on Failure Causes},
	volume = {9678},
	isbn = {978-3-319-34129-3},
	url = {https://link.springer.com/10.1007/978-3-319-34129-3_27},
	series = {Lecture Notes in Computer Science},
	abstract = {Recent advances in Web-information extraction have led to the creation of several large Knowledge Bases ({KBs}). Querying these {KBs} often results in empty answers that do not serve the users’ needs. Relaxation of the failing queries is one of the cooperative techniques used to retrieve alternative results. Most of the previous work on {RDF} query relaxation compute a set of relaxed queries and execute them in a similarity-based ranking order. Thus, these approaches relax an {RDF} query without knowing its failure causes ({FCs}). In this paper, we study the idea of identifying these {FCs} to speed up the query relaxation process. We propose three relaxation strategies based on various information levels about the {FCs} of the user query and of its relaxed queries as well. A set of experiments conducted on the {LUBM} benchmark show the impact of our proposal in comparison with a state-of-the-art algorithm.},
	pages = {439--454},
	booktitle = {The Semantic Web. Latest Advances and New Domains},
	publisher = {Springer International Publishing},
	author = {Fokou, Géraud and Jean, Stéphane and Hadjali, Allel and Baron, Mickaël},
	editor = {Sack, Harald and Blomqvist, Eva and d'Aquin, Mathieu and Ghidini, Chiara and Ponzetto, Simone Paolo and Lange, Christoph},
	urldate = {2022-12-07},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-34129-3_27},
}

@misc{wei_vertical_2022,
	title = {Vertical Federated Learning: Challenges, Methodologies and Experiments},
	url = {http://arxiv.org/abs/2202.04309},
	shorttitle = {Vertical Federated Learning},
	abstract = {Recently, federated learning ({FL}) has emerged as a promising distributed machine learning ({ML}) technology, owing to the advancing computational and sensing capacities of enduser devices, however with the increasing concerns on users’ privacy. As a special architecture in {FL}, vertical {FL} ({VFL}) is capable of constructing a hyper {ML} model by embracing sub-models from different clients. These sub-models are trained locally by vertically partitioned data with distinct attributes. Therefore, the design of {VFL} is fundamentally different from that of conventional {FL}, raising new and unique research issues. In this paper, we aim to discuss key challenges in {VFL} with effective solutions, and conduct experiments on real-life datasets to shed light on these issues. Speciﬁcally, we ﬁrst propose a general framework on {VFL}, and highlight the key differences between {VFL} and conventional {FL}. Then, we discuss research challenges rooted in {VFL} systems under four aspects, i.e., security and privacy risks, expensive computation and communication costs, possible structural damage caused by model splitting, and system heterogeneity. Afterwards, we develop solutions to addressing the aforementioned challenges, and conduct extensive experiments to showcase the effectiveness of our proposed solutions.},
	number = {{arXiv}:2202.04309},
	publisher = {{arXiv}},
	author = {Wei, Kang and Li, Jun and Ma, Chuan and Ding, Ming and Wei, Sha and Wu, Fan and Chen, Guihai and Ranbaduge, Thilina},
	urldate = {2023-01-20},
	date = {2022-02-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2202.04309 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@inproceedings{gavriilidis_towards_2022,
	location = {Sydney, Australia},
	title = {Towards a Modular Data Management System Framework},
	abstract = {Today’s data management systems ({DMSes}) are implemented using monolithic architectures. This is explained by the fact that, historically, the {DMS} market was dominated by commercial solutions. However, with the advent of open source, lots of alternative {DMSes} and {DMS} components have emerged. Yet, they cannot be easily integrated because {DMSes} are still designed as monoliths. We propose {PolyDMS}, our vision towards composable {DMSes}. The core idea of {PolyDMS} is to split up and wrap individual components with standardized interfaces, such that a Component Orchestrator can flexibly construct {DMSes} out of them. Users can compose new {DMSes} by writing small programs in our System Definition Language. {PolyDMS} allows them to quickly instantiate new {DMSes} according to their needs while giving them the opportunity to reuse existing components. Our proof-of-concept implementation shows that composing multiple components into a new {DMS} can yield additional functionality and better performance.},
	eventtitle = {{CDMS} '22},
	pages = {6},
	booktitle = {Proceedings of the 1st International Workshop on Composable Data Management Systems},
	author = {Gavriilidis, Haralampos and Behme, Lennart and Papadopoulos, Sokratis and Bortoli, Stefano and Quiané-Ruiz, Jorge-Arnulfo and Markl, Volker},
	date = {2022-09-09},
	langid = {english},
}

@article{murray_tfdata_2021,
	title = {tf.data: A Machine Learning Data Processing Framework},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3476311.3476374},
	doi = {10.14778/3476311.3476374},
	shorttitle = {tf.data},
	abstract = {Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data {API} provides operators that can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions enable users to focus on the application logic of data processing, while tf.data’s runtime ensures that pipelines run efficiently.},
	pages = {2945--2958},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Murray, Derek G. and Šimša, Jiří and Klimovic, Ana and Indyk, Ihor},
	urldate = {2023-01-12},
	date = {2021-07},
	langid = {english},
}

@article{azcoitia_survey_2022,
	title = {A Survey of Data Marketplaces and Their Business Models},
	volume = {51},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/3572751.3572755},
	doi = {10.1145/3572751.3572755},
	abstract = {Data is becoming an indispensable production factor for the modern economy, matching or exceeding in importance traditional factors such as land, infrastructure, labor and capital. As part of this, a wide range of applications in different sectors require huge amounts of information to feed machine learning models and algorithms responsible for critical roles in production chains and business processes. A variety of data trading entities including, but not limited to data marketplaces, have thus appeared in order to satisfy and match the offer with the demand for data. In this paper, we present the results and conclusions from a comprehensive survey covering 190 commercial data trading entities, the types of data that their trade, as well as their business models and the technologies that they rely upon. We also point to promising open research questions in the areas of data marketplace federation, pricing, and data ownership protection that could benefit the growing ecosystem of data trading entities that we have surveyed.},
	pages = {18--29},
	number = {3},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Azcoitia, Santiago Andrés and Laoutaris, Nikolaos},
	date = {2022-11},
}

@software{the_tensorflow_federated_authors_tensorflow_2018,
	title = {{TensorFlow} Federated},
	url = {https://github.com/tensorflow/federated},
	version = {0.44.0},
	author = {{The TensorFlow Federated Authors}},
	date = {2018-12},
}

@article{stoddard_tanium_2021,
	title = {Tanium Reveal: A Federated Search Engine for Querying Unstructured File Data on Large Enterprise Networks},
	volume = {14},
	abstract = {Tanium Reveal is a federated search engine deployed on large-scale enterprise networks that is capable of executing data queries across billions of private data files within 60 seconds. Data resides at the edge of networks, potentially distributed on hundreds of thousands of endpoints. The anatomy of the search engine consists of local inverse indexes on each endpoint and a global communication platform called Tanium for issuing search queries to all endpoints. Reveal enables asynchronous parsing and indexing on endpoints without noticeable impact to the endpoints’ primary functionality. The engine harnesses the Tanium platform, which is based on a self-organizing, fault-tolerant, scalable, linear chain communication scheme. We demonstrate a multi-tier workflow for executing search queries across a network and for viewing matching snippets of text on any endpoint. We analyze metrics for federated indexing and searching in multiple environments including a production network with 1.05 billion searchable files distributed across 4236 endpoints. While primarily focusing on Boolean, phrase, and similarity query types, Reveal is compatible with further automation (e.g., semantic classification based on machine learning). Lastly, we discuss safeguards for sensitive information within Reveal including cryptographic hashing of private text and role-based access control ({RBAC}).},
	pages = {3096--3109},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Stoddard, Josh and Mustafa, Adam and Goela, Naveen},
	date = {2021},
	langid = {english},
}

@article{agrawal_rheem_2018,
	title = {{RHEEM}: Enabling Cross-Platform Data Processing},
	volume = {11},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3236187.3269458},
	doi = {10.14778/3236187.3236195},
	shorttitle = {{RHEEM}},
	abstract = {Solving business problems increasingly requires going beyond the limits of a single data processing platform (platform for short), such as Hadoop or a {DBMS}. As a result, organizations typically perform tedious and costly tasks to juggle their code and data across diﬀerent platforms. Addressing this pain and achieving automatic cross-platform data processing is quite challenging: ﬁnding the most eﬃcient platform for a given task requires quite good expertise for all the available platforms. We present Rheem, a general-purpose cross-platform data processing system that decouples applications from the underlying platforms. It not only determines the best platform to run an incoming task, but also splits the task into subtasks and assigns each subtask to a speciﬁc platform to minimize the overall cost (e.g., runtime or monetary cost). It features (i) an interface to easily compose data analytic tasks; (ii) a novel cost-based optimizer able to ﬁnd the most eﬃcient platform in almost all cases; and (iii) an executor to eﬃciently orchestrate tasks over diﬀerent platforms. As a result, it allows users to focus on the business logic of their applications rather than on the mechanics of how to compose and execute them. Using different real-world applications with Rheem, we demonstrate how cross-platform data processing can accelerate performance by more than one order of magnitude compared to single-platform data processing.},
	pages = {1414--1427},
	number = {11},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Agrawal, Divy and Ouzzani, Mourad and Papotti, Paolo and Quiané-Ruiz, Jorge-Arnulfo and Tang, Nan and Thirumuruganathan, Saravanan and Troudi, Anis and Chawla, Sanjay and Contreras-Rojas, Bertty and Elmagarmid, Ahmed and Idris, Yasser and Kaoudi, Zoi and Kruse, Sebastian and Lucas, Ji and Mansour, Essam},
	urldate = {2020-08-25},
	date = {2018-07-01},
	langid = {english},
}

@article{kuchnik_progressive_2021,
	title = {Progressive Compressed Records: Taking a Byte out of Deep Learning Data},
	volume = {14},
	doi = {10.14778/3476249.3476308},
	abstract = {Deep learning accelerators efficiently train over vast and growing amounts of data, placing a newfound burden on commodity networks and storage devices. A common approach to conserve bandwidth involves resizing or compressing data prior to training. We introduce Progressive Compressed Records ({PCRs}), a data format that uses compression to reduce the overhead of fetching and transporting data, effectively reducing the training time required to achieve a target accuracy. {PCRs} deviate from previous storage formats by combining progressive compression with an efficient storage layout to view a single dataset at multiple fidelities—all without adding to the total dataset size. We implement {PCRs} and evaluate them on a range of datasets, training tasks, and hardware architectures. Our work shows that: (i) the amount of compression a dataset can tolerate exceeds 50\% of the original encoding for many {DL} training tasks; (ii) it is possible to automatically and efficiently select appropriate compression levels for a given task; and (iii) {PCRs} enable tasks to readily access compressed data at runtime—utilizing as little as half the training bandwidth and thus potentially doubling training speed.},
	pages = {2627--2641},
	number = {11},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Kuchnik, Michael and Amvrosiadis, George and Smith, Virginia},
	date = {2021},
	langid = {english},
}

@article{wu_privacy_2020,
	title = {Privacy Preserving Vertical Federated Learning for Tree-based Models},
	volume = {13},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3407790.3407811},
	doi = {10.14778/3407790.3407811},
	abstract = {Federated learning ({FL}) is an emerging paradigm that enables multiple organizations to jointly train a model without revealing their private data to each other. This paper studies vertical federated learning, which tackles the scenarios where (i) collaborating organizations own data of the same set of users but with disjoint features, and (ii) only one organization holds the labels. We propose Pivot, a novel solution for privacy preserving vertical decision tree training and prediction, ensuring that no intermediate information is disclosed other than those the clients have agreed to release (i.e., the ﬁnal tree model and the prediction output). Pivot does not rely on any trusted third party and provides protection against a semi-honest adversary that may compromise m − 1 out of m clients. We further identify two privacy leakages when the trained decision tree model is released in plaintext and propose an enhanced protocol to mitigate them. The proposed solution can also be extended to tree ensemble models, e.g., random forest ({RF}) and gradient boosting decision tree ({GBDT}) by treating single decision trees as building blocks. Theoretical and experimental analysis suggest that Pivot is eﬃcient for the privacy achieved.},
	pages = {2090--2103},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Wu, Yuncheng and Cai, Shaofeng and Xiao, Xiaokui and Chen, Gang and Ooi, Beng Chin},
	urldate = {2021-09-21},
	date = {2020-08},
	langid = {english},
}

@article{marcus_neo_2019,
	title = {Neo: A Learned Query Optimizer},
	volume = {12},
	issn = {21508097},
	url = {http://arxiv.org/abs/1904.03711},
	doi = {10.14778/3342263.3342644},
	shorttitle = {Neo},
	abstract = {Query optimization is one of the most challenging problems in database systems. Despite the progress made over the past decades, query optimizers remain extremely complex components that require a great deal of hand-tuning for speciﬁc workloads and datasets. Motivated by this shortcoming and inspired by recent advances in applying machine learning to data management challenges, we introduce Neo (Neural Optimizer ), a novel learning-based query optimizer that relies on deep neural networks to generate query executions plans. Neo bootstraps its query optimization model from existing optimizers and continues to learn from incoming queries, building upon its successes and learning from its failures. Furthermore, Neo naturally adapts to underlying data patterns and is robust to estimation errors. Experimental results demonstrate that Neo, even when bootstrapped from a simple optimizer like {PostgreSQL}, can learn a model that oﬀers similar performance to state-of-the-art commercial optimizers, and in some cases even surpass them.},
	pages = {1705--1718},
	number = {11},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
	urldate = {2020-08-25},
	date = {2019-07-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.03711},
	keywords = {Computer Science - Databases},
}

@article{nakandala_cerebro_2020,
	title = {Cerebro: A Data System for Optimized Deep Learning Model Selection},
	volume = {13},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3407790.3407816},
	doi = {10.14778/3407790.3407816},
	shorttitle = {Cerebro},
	abstract = {Deep neural networks (deep nets) are revolutionizing many machine learning ({ML}) applications. But there is a major bottleneck to wider adoption: the pain and resource intensiveness of model selection. This empirical process involves exploring deep net architectures and hyper-parameters, often requiring hundreds of trials. Alas, most {ML} systems focus on training one model at a time, reducing throughput and raising overall resource costs; some also sacriﬁce reproducibility. We present Cerebro, a new data system to raise deep net model selection throughput at scale without raising resource costs and without sacriﬁcing reproducibility or accuracy. Cerebro uses a new parallel {SGD} execution strategy we call model hopper parallelism that hybridizes task- and data-parallelism to mitigate the cons of these prior paradigms and oﬀer the best of both worlds. Experiments on large {ML} benchmark datasets show that Cerebro oﬀers 3x to 10x runtime savings relative to data-parallel systems like Horovod and Parameter Server and up to 8x memory/storage savings or up to 100x network savings relative to task-parallel systems. Cerebro also supports heterogeneous resources and fault tolerance.},
	pages = {2159--2173},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Nakandala, Supun and Zhang, Yuhao and Kumar, Arun},
	urldate = {2021-04-26},
	date = {2020-08},
	langid = {english},
}

@article{kunft_intermediate_2019,
	title = {An Intermediate Representation for Optimizing Machine Learning Pipelines},
	volume = {12},
	issn = {21508097},
	url = {http://dl.acm.org/citation.cfm?doid=3342263.3360363},
	doi = {10.14778/3342263.3342633},
	abstract = {Machine learning ({ML}) pipelines for model training and validation typically include preprocessing, such as data cleaning and feature engineering, prior to training an {ML} model. Preprocessing combines relational algebra and user-deﬁned functions ({UDFs}), while model training uses iterations and linear algebra. Current systems are tailored to either of the two. As a consequence, preprocessing and {ML} steps are optimized in isolation. To enable holistic optimization of {ML} training pipelines, we present Lara, a declarative domainspeciﬁc language for collections and matrices. Lara’s intermediate representation ({IR}) reﬂects on the complete program, i.e., {UDFs}, control ﬂow, and both data types. Two views on the {IR} enable diverse optimizations. Monads enable operator pushdown and fusion across type and loop boundaries. Combinators provide the semantics of domainspeciﬁc operators and optimize data access and cross-validation of {ML} algorithms. Our experiments on preprocessing pipelines and selected {ML} algorithms show the eﬀects of our proposed optimizations on dense and sparse data, which achieve speedups of up to an order of magnitude.},
	pages = {1553--1567},
	number = {11},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Kunft, Andreas and Katsifodimos, Asterios and Schelter, Sebastian and Breß, Sebastian and Rabl, Tilmann and Markl, Volker},
	urldate = {2020-08-25},
	date = {2019-07-01},
	langid = {english},
}

@article{richter_seven-dimensional_2015,
	title = {A seven-dimensional analysis of hashing methods and its implications on query processing},
	volume = {9},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2850583.2850585},
	doi = {10.14778/2850583.2850585},
	abstract = {Hashing is a solved problem. It allows us to get constant time access for lookups. Hashing is also simple. It is safe to use an arbitrary method as a black box and expect good performance, and optimizations to hashing can only improve it by a negligible delta. Why are all of the previous statements plain wrong? That is what this paper is about. In this paper we thoroughly study hashing for integer keys and carefully analyze the most common hashing methods in a ﬁve-dimensional requirements space: () data-distribution, () load factor, () dataset size, () read/write-ratio, and () un/successfulratio. Each point in that design space may potentially suggest a different hashing scheme, and additionally also a different hash function. We show that a right or wrong decision in picking the right hashing scheme and hash function combination may lead to signiﬁcant difference in performance. To substantiate this claim, we carefully analyze two additional dimensions: () ﬁve representative hashing schemes (which includes an improved variant of Robin Hood hashing), () four important classes of hash functions widely used today. That is, we consider 20 different combinations in total. Finally, we also provide a glimpse about the effect of table memory layout and the use of {SIMD} instructions. Our study clearly indicates that picking the right combination may have considerable impact on insert and lookup performance, as well as memory footprint. A major conclusion of our work is that hashing should be considered a white box before blindly using it in applications, such as query processing. Finally, we also provide a strong guideline about when to use which hashing method.},
	pages = {96--107},
	number = {3},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Richter, Stefan and Alvarez, Victor and Dittrich, Jens},
	urldate = {2020-11-26},
	date = {2015-11},
	langid = {english},
}

@article{jindal_comparison_2013,
	title = {A comparison of knives for bread slicing},
	volume = {6},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2536336.2536338},
	doi = {10.14778/2536336.2536338},
	abstract = {Vertical partitioning is a crucial step in physical database design in row-oriented databases. A number of vertical partitioning algorithms have been proposed over the last three decades for a variety of niche scenarios. In principle, the underlying problem remains the same: decompose a table into one or more vertical partitions. However, it is not clear how good different vertical partitioning algorithms are in comparison to each other. In fact, it is not even clear how to experimentally compare different vertical partitioning algorithms. In this paper, we present an exhaustive experimental study of several vertical partitioning algorithms. We categorize vertical partitioning algorithms along three dimensions. We survey six vertical partitioning algorithms and discuss their pros and cons. We identify the major differences in the use-case settings for different algorithms and describe how to make an apples-to-apples comparison of different vertical partitioning algorithms under the same setting. We propose four metrics to compare vertical partitioning algorithms. We show experimental results from the {TPC}-H and {SSB} benchmark and present four key lessons learned: (1) we can do four orders of magnitude less computation and still ﬁnd the optimal layouts, (2) the beneﬁts of vertical partitioning depend strongly on the database buffer size, (3) {HillClimb} is the best vertical partitioning algorithm, and (4) vertical partitioning for {TPC}-H-like benchmarks can improve over column layout by only up to 5\%.},
	pages = {361--372},
	number = {6},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Jindal, Alekh and Palatinus, Endre and Pavlov, Vladimir and Dittrich, Jens},
	urldate = {2020-11-26},
	date = {2013-04},
	langid = {english},
}

@article{schad_runtime_2010,
	title = {Runtime measurements in the cloud: observing, analyzing, and reducing variance},
	volume = {3},
	abstract = {One of the main reasons why cloud computing has gained so much popularity is due to its ease of use and its ability to scale computing resources on demand. As a result, users can now rent computing nodes on large commercial clusters through several vendors, such as Amazon and rackspace. However, despite the attention paid by Cloud providers, performance unpredictability is a major issue in Cloud computing for (1) database researchers performing wall clock experiments, and (2) database applications providing servicelevel agreements. In this paper, we carry out a study of the performance variance of the most widely used Cloud infrastructure (Amazon {EC}2) from diﬀerent perspectives. We use established microbenchmarks to measure performance variance in {CPU}, I/O, and network. And, we use a multi-node {MapReduce} application to quantify the impact on real dataintensive applications. We collected data for an entire month and compare it with the results obtained on a local cluster. Our results show that {EC}2 performance varies a lot and often falls into two bands having a large performance gap in-between — which is somewhat surprising. We observe in our experiments that these two bands correspond to the different virtual system types provided by Amazon. Moreover, we analyze results considering diﬀerent availability zones, points in time, and locations. This analysis indicates that, among others, the choice of availability zone also inﬂuences the performance variability. A major conclusion of our work is that the variance on {EC}2 is currently so high that wall clock experiments may only be performed with considerable care. To this end, we provide some hints to users.},
	pages = {460--471},
	number = {1},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Schad, Jörg and Dittrich, Jens and Quiané-Ruiz, Jorge-Arnulfo},
	date = {2010-09},
	langid = {english},
}

@article{elgohary_compressed_2016,
	title = {Compressed Linear Algebra for Large-Scale Machine Learning},
	volume = {9},
	abstract = {Large-scale machine learning ({ML}) algorithms are often iterative, using repeated read-only data access and I/Obound matrix-vector multiplications to converge to an optimal model. It is crucial for performance to ﬁt the data into single-node or distributed main memory. General-purpose, heavy- and lightweight compression techniques struggle to achieve both good compression ratios and fast decompression speed to enable block-wise uncompressed operations. Hence, we initiate work on compressed linear algebra ({CLA}), in which lightweight database compression techniques are applied to matrices and then linear algebra operations such as matrix-vector multiplication are executed directly on the compressed representations. We contribute eﬀective column compression schemes, cache-conscious operations, and an efﬁcient sampling-based compression algorithm. Our experiments show that {CLA} achieves in-memory operations performance close to the uncompressed case and good compression ratios that allow us to ﬁt larger datasets into available memory. We thereby obtain signiﬁcant end-to-end performance improvements up to 26x or reduced memory requirements.},
	pages = {960--971},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Elgohary, Ahmed and Boehm, Matthias and Haas, Peter J and Reiss, Frederick R and Reinwald, Berthold},
	date = {2016},
	langid = {english},
}

@article{thusoo_hive_2009,
	title = {Hive: A Warehousing Solution over a Map-Reduce Framework},
	volume = {2},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/1687553.1687609},
	doi = {10.14778/1687553.1687609},
	shorttitle = {Hive},
	abstract = {The size of data sets being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive.
              Hadoop
              [3] is a popular open-source map-reduce implementation which is being used as an alternative to store and process extremely large data sets on commodity hardware. However, the map-reduce programming model is very low level and requires developers to write custom programs which are hard to maintain and reuse.},
	pages = {1626--1629},
	number = {2},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Thusoo, Ashish and Sarma, Joydeep Sen and Jain, Namit and Shao, Zheng and Chakka, Prasad and Anthony, Suresh and Liu, Hao and Wyckoff, Pete and Murthy, Raghotham},
	urldate = {2022-05-18},
	date = {2009-08},
	langid = {english},
}

@article{beyer_jaql_2011,
	title = {Jaql: A Scripting Language for Large Scale Semistructured Data Analysis},
	volume = {4},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3402755.3402761},
	doi = {10.14778/3402755.3402761},
	shorttitle = {Jaql},
	abstract = {This paper describes Jaql, a declarative scripting language for analyzing large semistructured datasets in parallel using Hadoop’s {MapReduce} framework. Jaql is currently used in {IBM}’s {InfoSphere} {BigInsights} [5] and Cognos Consumer Insight [9] products. Jaql’s design features are: (1) a ﬂexible data model, (2) reusability, (3) varying levels of abstraction, and (4) scalability. Jaql’s data model is inspired by {JSON} and can be used to represent datasets that vary from ﬂat, relational tables to collections of semistructured documents. A Jaql script can start without any schema and evolve over time from a partial to a rigid schema. Reusability is provided through the use of higher-order functions and by packaging related functions into modules. Most Jaql scripts work at a high level of abstraction for concise speciﬁcation of logical operations (e.g., join), but Jaql’s notion of physical transparency also provides a lower level of abstraction if necessary. This allows users to pin down the evaluation plan of a script for greater control or even add new operators. The Jaql compiler automatically rewrites Jaql scripts so they can run in parallel on Hadoop. In addition to describing Jaql’s design, we present the results of scale-up experiments on Hadoop running Jaql scripts for intranet data analysis and log processing.},
	pages = {1272--1283},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Beyer, Kevin S. and Ercegovac, Vuk and Gemulla, Rainer and Balmin, Andrey and Eltabakh, Mohamed and Kanne, Carl-Christian and Ozcan, Fatma and Shekita, Eugene J.},
	urldate = {2022-05-18},
	date = {2011-08},
	langid = {english},
}

@article{skiadopoulos_dbos_2021,
	title = {{DBOS}: a {DBMS}-oriented operating system},
	volume = {15},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3485450.3485454},
	doi = {10.14778/3485450.3485454},
	shorttitle = {{DBOS}},
	abstract = {This paper lays out the rationale for building a completely new operating system ({OS}) stack. Rather than build on a single node {OS} together with separate cluster schedulers, distributed filesystems, and network managers, we argue that a distributed transactional {DBMS} should be the basis for a scalable cluster {OS}. We show herein that such a database {OS} ({DBOS}) can do scheduling, file management, and inter-process communication with competitive performance to existing systems. In addition, significantly better analytics can be provided as well as a dramatic reduction in code complexity through implementing {OS} services as standard database queries, while implementing low-latency transactions and high availability only once.},
	pages = {21--30},
	number = {1},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Skiadopoulos, Athinagoras and Li, Qian and Kraft, Peter and Kaffes, Kostis and Hong, Daniel and Mathew, Shana and Bestor, David and Cafarella, Michael and Gadepally, Vijay and Graefe, Goetz and Kepner, Jeremy and Kozyrakis, Christos and Kraska, Tim and Stonebraker, Michael and Suresh, Lalith and Zaharia, Matei},
	urldate = {2022-01-31},
	date = {2021-09},
	langid = {english},
}

@article{renz-wieland_dynamic_2020,
	title = {Dynamic Parameter Allocation in Parameter Servers},
	volume = {13},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3407790.3407796},
	doi = {10.14778/3407790.3407796},
	abstract = {To keep up with increasing dataset sizes and model complexity, distributed training has become a necessity for large machine learning tasks. Parameter servers ease the implementation of distributed parameter management—a key concern in distributed training—, but can induce severe communication overhead. To reduce communication overhead, distributed machine learning algorithms use techniques to increase parameter access locality ({PAL}), achieving up to linear speed-ups. We found that existing parameter servers provide only limited support for {PAL} techniques, however, and therefore prevent eﬃcient training. In this paper, we explore whether and to what extent {PAL} techniques can be supported, and whether such support is beneﬁcial. We propose to integrate dynamic parameter allocation into parameter servers, describe an eﬃcient implementation of such a parameter server called Lapse, and experimentally compare its performance to existing parameter servers across a number of machine learning tasks. We found that Lapse provides near-linear scaling and can be orders of magnitude faster than existing parameter servers.},
	pages = {1877--1890},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Renz-Wieland, Alexander and Gemulla, Rainer and Zeuch, Steffen and Markl, Volker},
	urldate = {2022-02-09},
	date = {2020-08},
	langid = {english},
}

@article{renz-wieland_just_2021,
	title = {Just Move It! Dynamic Parameter Allocation in Action},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3476311.3476325},
	doi = {10.14778/3476311.3476325},
	shorttitle = {Just move it!},
	abstract = {Parameter servers ({PSs}) ease the implementation of distributed machine learning systems, but their performance can fall behind that of single machine baselines due to communication overhead. We demonstrate Lapse, an open source {PS} with dynamic parameter allocation. Previous work has shown that dynamic parameter allocation can improve {PS} performance by up to two orders of magnitude and lead to near-linear speed-ups over single machine baselines. This demonstration illustrates how Lapse is used and why it can provide order-of-magnitude speed-ups over other {PSs}. To do so, this demonstration interactively analyzes and visualizes how dynamic parameter allocation looks like in action.},
	pages = {2707--2710},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Renz-Wieland, Alexander and Drobisch, Tobias and Kaoudi, Zoi and Gemulla, Rainer and Markl, Volker},
	urldate = {2022-02-09},
	date = {2021-07},
	langid = {english},
}

@article{cafarella_ten_2018,
	title = {Ten Years of {WebTables}},
	volume = {11},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3229863.3240492},
	doi = {10.14778/3229863.3240492},
	abstract = {In 2008, we wrote about {WebTables}, an eﬀort to exploit the large and diverse set of structured databases casually published online in the form of {HTML} tables. The past decade has seen a ﬂurry of research and commercial activities around the {WebTables} project itself, as well as the broad topic of informal online structured data. In this paper, we1 will review the {WebTables} project, and try to place it in the broader context of the decade of work that followed. We will also show how the progress over the past ten years sets up an exciting agenda for the future, and will draw upon many corners of the data management community.},
	pages = {2140--2149},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Cafarella, Michael J. and Halevy, Alon and Lee, Hongrae and Madhavan, Jayant and Yu, Cong and Wang, Daisy Zhe and Wu, Eugene},
	urldate = {2021-12-09},
	date = {2018-08},
	langid = {english},
}

@article{xie_query_2018,
	title = {Query Log Compression for Workload Analytics},
	volume = {12},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3291264.3291265},
	doi = {10.14778/3291264.3291265},
	abstract = {Analyzing database access logs is a key part of performance tuning, intrusion detection, benchmark development, and many other database administration tasks. Unfortunately, it is common for production databases to deal with millions or more queries each day, so these logs must be summarized before they can be used. Designing an appropriate summary encoding requires trading oﬀ between conciseness and information content. For example: simple workload sampling may miss rare, but high impact queries. In this paper, we present {LogR}, a lossy log compression scheme suitable for use in many automated log analytics tools, as well as for human inspection. We formalize and analyze the space/ﬁdelity trade-oﬀ in the context of a broader family of “pattern” and “pattern mixture” log encodings to which {LogR} belongs. We show through a series of experiments that {LogR} compressed encodings can be created eﬃciently, come with provable information-theoretic bounds on their accuracy, and outperform state-of-art log summarization strategies.},
	pages = {183--196},
	number = {3},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Xie, Ting and Chandola, Varun and Kennedy, Oliver},
	urldate = {2021-11-22},
	date = {2018-11},
	langid = {english},
}

@article{cafarella_webtables_2008,
	title = {{WebTables}: Exploring the Power of Tables on the Web},
	volume = {1},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/1453856.1453916},
	doi = {10.14778/1453856.1453916},
	shorttitle = {{WebTables}},
	abstract = {The World-Wide Web consists of a huge number of unstructured documents, but it also contains structured data in the form of {HTML} tables. We extracted 14.1 billion {HTML} tables from Google’s general-purpose web crawl, and used statistical classiﬁcation techniques to ﬁnd the estimated 154M that contain high-quality relational data. Because each relational table has its own “schema” of labeled and typed columns, each such table can be considered a small structured database. The resulting corpus of databases is larger than any other corpus we are aware of, by at least ﬁve orders of magnitude.},
	pages = {538--549},
	number = {1},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Cafarella, Michael J. and Halevy, Alon and Wang, Daisy Zhe and Wu, Eugene and Zhang, Yang},
	urldate = {2021-11-12},
	date = {2008-08},
	langid = {english},
}

@article{zhang_distributed_2021,
	title = {Distributed Deep Learning on Data Systems: A Comparative Analysis of Approaches},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3467861.3467867},
	doi = {10.14778/3467861.3467867},
	shorttitle = {Distributed deep learning on data systems},
	abstract = {Deep learning ({DL}) is growing in popularity for many data analytics applications, including among enterprises. Large business-critical datasets in such settings typically reside in {RDBMSs} or other data systems. The {DB} community has long aimed to bring machine learning ({ML}) to {DBMS}-resident data. Given past lessons from in-{DBMS} {ML} and recent advances in scalable {DL} systems, {DBMS} and cloud vendors are increasingly interested in adding more {DL} support for {DB}-resident data. Recently, a new parallel {DL} model selection execution approach called Model Hopper Parallelism ({MOP}) was proposed. In this paper, we characterize the particular suitability of {MOP} for {DL} on data systems, but to bring {MOP}-based {DL} to {DBresident} data, we show that there is no single “best” approach, and an interesting tradeoff space of approaches exists. We explain four canonical approaches and build prototypes upon Greenplum Database, compare them analytically on multiple criteria (e.g., runtime efficiency and ease of governance) and compare them empirically with large-scale {DL} workloads. Our experiments and analyses show that it is non-trivial to meet all practical desiderata well and there is a Pareto frontier; for instance, some approaches are 3x-6x faster but fare worse on governance and portability. Our results and insights can help {DBMS} and cloud vendors design better {DL} support for {DB} users. All of our source code, data, and other artifacts are available at https://github.com/makemebitter/cerebro-ds.},
	pages = {1769--1782},
	number = {10},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Zhang, Yuhao and {McQuillan}, Frank and Jayaram, Nandish and Kak, Nikhil and Khanna, Ekta and Kislal, Orhan and Valdano, Domino and Kumar, Arun},
	urldate = {2021-11-11},
	date = {2021-06},
	langid = {english},
}

@article{kang_jointly_2020,
	title = {Jointly Optimizing Preprocessing and Inference for {DNN}-based Visual Analytics},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3425879.3425881},
	doi = {10.14778/3425879.3425881},
	abstract = {While deep neural networks ({DNNs}) are an increasingly popular way to query large corpora of data, their significant runtime remains an active area of research. As a result, researchers have proposed systems and optimizations to reduce these costs by allowing users to trade off accuracy and speed. In this work, we examine end-to-end {DNN} execution in visual analytics systems on modern accelerators. Through a novel measurement study, we show that the preprocessing of data (e.g., decoding, resizing) can be the bottleneck in many visual analytics systems on modern hardware.},
	pages = {87--100},
	number = {2},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Kang, Daniel and Mathur, Ankit and Veeramacheneni, Teja and Bailis, Peter and Zaharia, Matei},
	urldate = {2021-08-23},
	date = {2020-10},
	langid = {english},
}

@article{mohan_analyzing_2021,
	title = {Analyzing and Mitigating Data Stalls in {DNN} Training},
	volume = {14},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3446095.3446100},
	doi = {10.14778/3446095.3446100},
	abstract = {Training Deep Neural Networks ({DNNs}) is resource-intensive and time-consuming. While prior research has explored many different ways of reducing {DNN} training time, the impact of input data pipeline, i.e., fetching raw data items from storage and performing data pre-processing in memory, has been relatively unexplored. This paper makes the following contributions: (1) We present the first comprehensive analysis of how the input data pipeline affects the training time of widely-used computer vision and audio Deep Neural Networks ({DNNs}), that typically involve complex data pre-processing. We analyze nine different models across three tasks and four datasets while varying factors such as the amount of memory, number of {CPU} threads, storage device, {GPU} generation etc on servers that are a part of a large production cluster at Microsoft. We find that in many cases, {DNN} training time is dominated by data stall time: time spent waiting for data to be fetched and pre-processed. (2) We build a tool, {DS}-Analyzer to precisely measure data stalls using a differential technique, and perform predictive what-if analysis on data stalls. (3) Finally, based on the insights from our analysis, we design and implement three simple but effective techniques in a data-loading library, {CoorDL}, to mitigate data stalls. Our experiments on a range of {DNN} tasks, models, datasets, and hardware configs show that when {PyTorch} uses {CoorDL} instead of the state-of-the-art {DALI} data loading library, {DNN} training time is reduced significantly (by as much as 5X on a single server).},
	pages = {771--784},
	number = {5},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Mohan, Jayashree and Phanishayee, Amar and Raniwala, Ashish and Chidambaram, Vijay},
	date = {2021-01},
}

@report{ocean_protocol_foundation_ocean_2019,
	title = {Ocean Protocol: A Decentralized Substrate for {AI} Data Services},
	url = {https://oceanprotocol.com/tech-whitepaper.pdf},
	author = {{Ocean Protocol Foundation} and {BigchainDB GmbH} and {Newton Circus (DEX Pte. Ltd.)}},
	urldate = {2020-07-09},
	date = {2019},
}

@inproceedings{kadlag_supporting_2004,
	location = {Berlin, Heidelberg},
	title = {Supporting Exploratory Queries in Databases},
	isbn = {978-3-540-24571-1},
	abstract = {Users of database applications, especially in the e-commerce domain, often resort to exploratory “trial-and-error” queries since the underlying data space is huge and unfamiliar, and there are several alternatives for search attributes in this space. For example, scouting for cheap airfares typically involves posing multiple queries, varying flight times, dates, and airport locations. Exploratory queries are problematic from the perspective of both the user and the server. For the database server, it results in a drastic reduction in effective throughput since much of the processing is duplicated in each successive query. For the client, it results in a marked increase in response times, especially when accessing the service through wireless channels.},
	pages = {594--605},
	booktitle = {Database Systems for Advanced Applications},
	publisher = {Springer Berlin Heidelberg},
	author = {Kadlag, Abhijit and Wanjari, Amol V. and Freire, Juliana and Haritsa, Jayant R.},
	editor = {Lee, Yoon Joon and Li, Jianzhong and Whang, Kyu-Young and Lee, Doheon},
	date = {2004},
}

@inproceedings{islam_user_2012,
	location = {Berlin, Heidelberg},
	title = {User Feedback Based Query Refinement by Exploiting Skyline Operator},
	isbn = {978-3-642-34002-4},
	url = {http://link.springer.com/10.1007/978-3-642-34002-4_33},
	doi = {10.1007/978-3-642-34002-4_33},
	abstract = {This paper presents {FlexIQ}, a framework for feedback based query refinement. In {FlexIQ}, feedback is used to discover the query intent of the user and skyline operator is used to confine the search space of the proposed query refinement algorithms. The feedback consists of both unexpected information currently present in the query output and expected information that is missing from the query output. Once the feedback is given by the user, our framework refines the initial query by exploiting skyline operator to minimize the unexpected information as well as maximize the expected information in the refined query output. We validate our framework both theoretically and experimentally. In particular, we demonstrate the effectiveness of our framework by comparing its performance with decision tree based query refinement.},
	eventtitle = {{ER} '12},
	pages = {423--438},
	booktitle = {Conceptual Modeling},
	publisher = {Springer Berlin Heidelberg},
	author = {Islam, Md. Saiful and Liu, Chengfei and Zhou, Rui},
	editor = {Atzeni, Paolo and Cheung, David and Ram, Sudha},
	date = {2012},
}

@article{fontoura_relaxation_2008,
	title = {Relaxation in Text Search Using Taxonomies},
	volume = {1},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/1453856.1453930},
	doi = {10.14778/1453856.1453930},
	abstract = {In this paper we propose a novel document retrieval model in which text queries are augmented with multi-dimensional taxonomy restrictions. These restrictions may be relaxed at a cost to result quality. This new model may be applicable in many arenas, including multifaceted, product, and local search, where documents are augmented with hierarchical metadata such as topic or location. We present eﬃcient algorithms for indexing and query processing in this new retrieval model. We decompose query processing into two sub-problems: ﬁrst, an online search problem to determine the correct overall level of relaxation cost that must be incurred to generate the top k results; and second, a budgeted relaxation search problem in which all results at a particular relaxation cost must be produced at minimal cost. We show the latter problem is solvable exactly in two hierarchical dimensions, is {NP}-hard in three or more dimensions, but admits eﬃcient approximation algorithms with provable guarantees. We present experimental results evaluating our algorithms on both synthetic and real data, showing order of magnitude improvements over the baseline algorithm.},
	pages = {672--683},
	number = {1},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Fontoura, Marcus and Josifovski, Vanja and Kumar, Ravi and Olston, Christopher and Tomkins, Andrew and Vassilvitskii, Sergei},
	urldate = {2022-12-22},
	date = {2008-08},
	langid = {english},
}

@inproceedings{koudas_relaxing_2006,
	title = {Relaxing Join and Selection Queries},
	series = {{VLDB} '06},
	abstract = {Database users can be frustrated by having an empty answer to a query. In this paper, we propose a framework to systematically relax queries involving joins and selections. When considering relaxing a query condition, intuitively one seeks the 'minimal' amount of relaxation that yields an answer. We first characterize the types of answers that we return to relaxed queries. We then propose a lattice based framework in order to aid query relaxation. Nodes in the lattice correspond to different ways to relax queries. We characterize the properties of relaxation at each node and present algorithms to compute the corresponding answer. We then discuss how to traverse this lattice in a way that a non-empty query answer is obtained with the minimum amount of query condition relaxation. We implemented this framework and we present our results of a thorough performance evaluation using real and synthetic data. Our results indicate the practical utility of our framework.},
	eventtitle = {{VLDB} '06},
	pages = {199--210},
	booktitle = {32nd International Conference on Very Large Data Bases},
	publisher = {{VLDB} Endowment},
	author = {Koudas, Nick and Li, Chen and Tung, Anthony K. H. and Vernica, Rares},
	date = {2006},
	note = {event-place: Seoul, Korea},
}

@inproceedings{jannach_techniques_2007,
	location = {Berlin, Heidelberg},
	title = {Techniques for Fast Query Relaxation in Content-Based Recommender Systems},
	isbn = {978-3-540-69912-5},
	abstract = {`Query relaxation' is one of the basic approaches to deal with unfulfillable or conflicting customer requirements in content-based recommender systems: When no product in the catalog exactly matches the customer requirements, the idea is to retrieve those products that fulfill as many of the requirements as possible by removing (relaxing) parts of the original query to the catalog. In general, searching for such an `maximum succeeding subquery' is a non-trivial task because a) the theoretical search space exponentially grows with the number of the subqueries and b) the allowed response times are strictly limited in interactive recommender applications.},
	pages = {49--63},
	booktitle = {{KI} 2006: Advances in Artificial Intelligence},
	publisher = {Springer Berlin Heidelberg},
	author = {Jannach, Dietmar},
	editor = {Freksa, Christian and Kohlhase, Michael and Schill, Kerstin},
	date = {2007},
}

@article{gauch_search_1991,
	title = {Search Improvement via Automatic Query Reformulation},
	volume = {9},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/125187.125193},
	doi = {10.1145/125187.125193},
	pages = {249--280},
	number = {3},
	journaltitle = {{ACM} Transactions on Information Systems},
	shortjournal = {{ACM} Trans. Inf. Syst.},
	author = {Gauch, Susan and Smith, John B.},
	date = {1991-07},
	keywords = {Expert Systems, full-text information retrieval, online search assistance, query reformulation, textbases},
}

@article{motro_vague_1988,
	title = {{VAGUE}: A User Interface to Relational Databases That Permits Vague Queries},
	volume = {6},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/45945.48027},
	doi = {10.1145/45945.48027},
	abstract = {A specific query establishes a rigid qualification and is concerned only with data that match it precisely. A vague query establishes a target qualification and is concerned also with data that are close to this target. Most conventional database systems cannot handle vague queries directly, forcing their users to retry specific queries repeatedly with minor modifications until they match data that are satisfactory. This article describes a system called {VAGUE} that can handle vague queries directly. The principal concept behind {VAGUE} is its extension to the relational data model with data metrics, which are definitions of distances between values of the same domain. A problem with implementing data distances is that different users may have different interpretations for the notion of distance. {VAGUE} incorporates several features that enable it to adapt itself to the individual views and priorities of its users.},
	pages = {187--214},
	number = {3},
	journaltitle = {{ACM} Transactions on Information Systems},
	shortjournal = {{ACM} Trans. Inf. Syst.},
	author = {Motro, Amihai},
	date = {1988-07},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
}

@inproceedings{junker_quickxplain_2004,
	title = {{QUICKXPLAIN}: Preferred Explanations and Relaxations for over-Constrained Problems},
	isbn = {0-262-51183-5},
	abstract = {Over-constrained problems can have an exponential number of conflicts, which explain the failure, and an exponential number of relaxations, which restore the consistency. A user of an interactive application, however, desires explanations and relaxations containing the most important constraints. To address this need, we define preferred explanations and relaxations based on user preferences between constraints and we compute them by a generic method which works for arbitrary {CP}, {SAT}, or {DL} solvers. We significantly accelerate the basic method by a divide-and-conquer strategy and thus provide the technological basis for the explanation facility of a principal industrial constraint programming tool, which is, for example, used in numerous configuration applications.},
	eventtitle = {{AAAI} '04},
	pages = {167--172},
	booktitle = {19th National Conference on Artifical Intelligence},
	publisher = {{AAAI} Press},
	author = {Junker, Ulrich},
	date = {2004},
	note = {event-place: San Jose, California},
}

@article{mottin_holistic_2016,
	title = {A holistic and principled approach for the empty-answer problem},
	volume = {25},
	issn = {1066-8888, 0949-877X},
	url = {http://link.springer.com/10.1007/s00778-016-0431-8},
	doi = {10.1007/s00778-016-0431-8},
	abstract = {We propose a principled optimization-based interactive query relaxation framework for queries that return no answers. Given an initial query that returns an empty-answer set, our framework dynamically computes and suggests alternative queries with fewer conditions than those the user has initially requested, in order to help the user arrive at a query with a non-empty-answer, or at a query for which no matter how many additional conditions are ignored, the answer will still be empty. Our proposed approach for suggesting query relaxations is driven by a novel probabilistic framework based on optimizing a wide variety of application-dependent objective functions. We describe optimal and approximate solutions of different optimization problems using the framework. Moreover, we discuss two important extensions to the base framework: the specification of a minimum size on the number of results returned by a relaxed query and the possibility of proposing multiple conditions at the same time. We analyze the proposed solutions, experimentally verify their efficiency and effectiveness, and illustrate their advantages over the existing approaches.},
	pages = {597--622},
	number = {4},
	journaltitle = {The {VLDB} Journal},
	shortjournal = {The {VLDB} Journal},
	author = {Mottin, Davide and Marascu, Alice and Roy, Senjuti Basu and Das, Gautam and Palpanas, Themis and Velegrakis, Yannis},
	urldate = {2022-12-19},
	date = {2016-08-01},
	langid = {english},
}

@article{martinenghi_taxonomy-based_2014,
	title = {Taxonomy-based relaxation of query answering in relational databases},
	volume = {23},
	issn = {1066-8888, 0949-877X},
	url = {http://link.springer.com/10.1007/s00778-013-0350-x},
	doi = {10.1007/s00778-013-0350-x},
	abstract = {Traditional information search in which queries are posed against a known and rigid schema over a structured database is shifting toward a Web scenario in which exposed schemas are vague or absent and data come from heterogeneous sources. In this framework, query answering cannot be precise and needs to be relaxed, with the goal of matching user requests with accessible data. In this paper, we propose a logical model and a class of abstract query languages as a foundation for querying relational data sets with vague schemas. Our approach relies on the availability of taxonomies, that is, simple classiﬁcations of terms arranged in a hierarchical structure. The model is a natural extension of the relational model in which data domains are organized in hierarchies, according to different levels of generalization between terms. We ﬁrst propose a conservative extension of the relational algebra for this model in which special operators allow the speciﬁcation of relaxed queries over vaguely structured information. We also study equivalence and rewriting properties of the algebra that can be used for query optimization. We then illustrate a logicbased query language that can provide a basis for expressing relaxed queries in a declarative way. We ﬁnally investigate the expressive power of the proposed query languages and the independence of the taxonomy in this context.},
	pages = {747--769},
	number = {5},
	journaltitle = {The {VLDB} Journal},
	shortjournal = {The {VLDB} Journal},
	author = {Martinenghi, Davide and Torlone, Riccardo},
	urldate = {2022-12-16},
	date = {2014-10},
	langid = {english},
}

@misc{subramaniam_comprehensive_2021,
	title = {Comprehensive and Comprehensible Data Catalogs: The What, Who, Where, When, Why, and How of Metadata Management},
	url = {http://arxiv.org/abs/2103.07532},
	shorttitle = {Comprehensive and Comprehensible Data Catalogs},
	abstract = {Scalable data science requires access to metadata, which is increasingly managed by databases called data catalogs. With today’s data catalogs, users choose between designs that make it easy to store or retrieve metadata, but not both. We ﬁnd this problem arises because catalogs lack an easy to understand mental model.},
	number = {{arXiv}:2103.07532},
	publisher = {{arXiv}},
	author = {Subramaniam, Pranav and Ma, Yintong and Li, Chi and Mohanty, Ipsita and Fernandez, Raul Castro},
	urldate = {2022-08-10},
	date = {2021-08-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2103.07532 [cs]},
	note = {Number: {arXiv}:2103.07532},
	keywords = {Computer Science - Databases},
}

@misc{wang_field_2021,
	title = {A Field Guide to Federated Optimization},
	url = {http://arxiv.org/abs/2107.06917},
	abstract = {Federated learning and analytics are a distributed approach for collaboratively learning models (or statistics) from decentralized data, motivated by and designed for privacy protection. The distributed learning process can be formulated as solving federated optimization problems, which emphasize communication eﬃciency, data heterogeneity, compatibility with privacy and system requirements, and other constraints that are not primary considerations in other problem settings. This paper provides recommendations and guidelines on formulating, designing, evaluating and analyzing federated optimization algorithms through concrete examples and practical implementation, with a focus on conducting eﬀective simulations to infer real-world performance. The goal of this work is not to survey the current literature, but to inspire researchers and practitioners to design federated learning algorithms that can be used in various practical applications.},
	number = {{arXiv}:2107.06917},
	publisher = {{arXiv}},
	author = {Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and {McMahan}, H. Brendan and Arcas, Blaise Aguera y and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and Diggavi, Suhas and Eichner, Hubert and Gadhikar, Advait and Garrett, Zachary and Girgis, Antonious M. and Hanzely, Filip and Hard, Andrew and He, Chaoyang and Horvath, Samuel and Huo, Zhouyuan and Ingerman, Alex and Jaggi, Martin and Javidi, Tara and Kairouz, Peter and Kale, Satyen and Karimireddy, Sai Praneeth and Konecny, Jakub and Koyejo, Sanmi and Li, Tian and Liu, Luyang and Mohri, Mehryar and Qi, Hang and Reddi, Sashank J. and Richtarik, Peter and Singhal, Karan and Smith, Virginia and Soltanolkotabi, Mahdi and Song, Weikang and Suresh, Ananda Theertha and Stich, Sebastian U. and Talwalkar, Ameet and Wang, Hongyi and Woodworth, Blake and Wu, Shanshan and Yu, Felix X. and Yuan, Honglin and Zaheer, Manzil and Zhang, Mi and Zhang, Tong and Zheng, Chunxiang and Zhu, Chen and Zhu, Wennan},
	urldate = {2022-12-09},
	date = {2021-07-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2107.06917 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{mottin_probabilistic_2013,
	title = {A Probabilistic Optimization Framework for the Empty-Answer Problem},
	volume = {6},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2556549.2556560},
	doi = {10.14778/2556549.2556560},
	abstract = {We propose a principled optimization-based interactive query relaxation framework for queries that return no answers. Given an initial query that returns an empty answer set, our framework dynamically computes and suggests alternative queries with less conditions than those the user has initially requested, in order to help the user arrive at a query with a non-empty answer, or at a query for which no matter how many additional conditions are ignored, the answer will still be empty. Our proposed approach for suggesting query relaxations is driven by a novel probabilistic framework based on optimizing a wide variety of application-dependent objective functions. We describe optimal and approximate solutions of different optimization problems using the framework. We analyze these solutions, experimentally verify their efﬁciency and effectiveness, and illustrate their advantage over the existing approaches.},
	pages = {1762--1773},
	number = {14},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Mottin, Davide and Marascu, Alice and Roy, Senjuti Basu and Das, Gautam and Palpanas, Themis and Velegrakis, Yannis},
	urldate = {2022-12-07},
	date = {2013-09},
	langid = {english},
}

@incollection{elbassuoni_query_2011,
	location = {Berlin, Heidelberg},
	title = {Query Relaxation for Entity-Relationship Search},
	volume = {6644},
	isbn = {978-3-642-21063-1 978-3-642-21064-8},
	url = {http://link.springer.com/10.1007/978-3-642-21064-8_5},
	series = {Lecture Notes in Computer Science},
	abstract = {Entity-relationship-structured data is becoming more important on the Web. For example, large knowledge bases have been automatically constructed by information extraction from Wikipedia and other Web sources. Entities and relationships can be represented by subject-property-object triples in the {RDF} model, and can then be precisely searched by structured query languages like {SPARQL}. Because of their Boolean-match semantics, such queries often return too few or even no results. To improve recall, it is thus desirable to support users by automatically relaxing or reformulating queries in such a way that the intention of the original user query is preserved while returning a sufﬁcient number of ranked results.},
	pages = {62--76},
	booktitle = {The Semanic Web: Research and Applications},
	publisher = {Springer},
	author = {Elbassuoni, Shady and Ramanath, Maya and Weikum, Gerhard},
	urldate = {2022-11-07},
	date = {2011},
	langid = {english},
}

@article{huang_approximating_2012,
	title = {Approximating query answering on {RDF} databases},
	volume = {15},
	issn = {1386-145X, 1573-1413},
	url = {http://link.springer.com/10.1007/s11280-011-0131-7},
	doi = {10.1007/s11280-011-0131-7},
	abstract = {Database users may be frustrated by no answers returned when they pose a query on the database. In this paper, we study the problem of relaxing queries on {RDF} databases in order to acquire approximate answers. We address two problems in efficient query relaxation. First, to ensure the quality of answers, we compute the similarities between relaxed queries with regard to the user query and use them to score the potential relevant answers. Second, for obtaining top-k answers, we develop two algorithms. One is based on the best-first strategy and relaxed queries are executed in the ranking order. The batch based algorithm executes the relaxed queries as a batch and avoids unnecessary execution cost. At last, we implement and experimentally evaluate our approaches.},
	pages = {89--114},
	number = {1},
	journaltitle = {World Wide Web},
	shortjournal = {World Wide Web},
	author = {Huang, Hai and Liu, Chengfei and Zhou, Xiaofang},
	urldate = {2022-11-07},
	date = {2012-01},
	langid = {english},
}

@article{bursztyn_teaching_2016,
	title = {Teaching an {RDBMS} about Ontological Constraints},
	volume = {9},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/2994509.2994532},
	doi = {10.14778/2994509.2994532},
	abstract = {In the presence of an ontology, query answers must reﬂect not only data explicitly present in the database, but also implicit data, which holds due to the ontology, even though it is not present in the database. A large and useful set of ontology languages enjoys {FOL} reducibility of query answering: answering a query can be reduced to evaluating a certain ﬁrst-order logic ({FOL}) formula (obtained from the query and ontology) against only the explicit facts.},
	pages = {1161--1172},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Bursztyn, Damian and Goasdoué, François and Manolescu, Ioana},
	urldate = {2022-11-07},
	date = {2016-08},
	langid = {english},
}

@incollection{feurer_auto-sklearn_2019,
	location = {Cham},
	title = {Auto-sklearn: Efficient and Robust Automated Machine Learning},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_6},
	abstract = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning ({AutoML}) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new {AutoML} system based on the Python machine learning package scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub Auto-sklearn, improves on existing {AutoML} methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won six out of ten phases of the first {ChaLearn} {AutoML} challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in {AutoML}. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of Auto-sklearn.},
	pages = {113--134},
	booktitle = {Automated Machine Learning: Methods, Systems, Challenges},
	publisher = {Springer International Publishing},
	author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost Tobias and Blum, Manuel and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	date = {2019},
	doi = {10.1007/978-3-030-05318-5_6},
}

@article{costan_intel_2016,
	title = {Intel {SGX} Explained},
	volume = {2016/086},
	url = {https://eprint.iacr.org/2016/086},
	journaltitle = {Cryptology {ePrint} Archive},
	author = {Costan, Victor and Devadas, Srinivas},
	date = {2016},
}

@article{xia_data_2022,
	title = {Data Station: Delegated, Trustworthy, and Auditable Computation to Enable Data-Sharing Consortia with a Data Escrow},
	volume = {15},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3551793.3551861},
	doi = {10.14778/3551793.3551861},
	shorttitle = {Data station},
	abstract = {Pooling and sharing data increases and distributes its value. But since data cannot be revoked once shared, scenarios that require controlled release of data for regulatory, privacy, and legal reasons default to not sharing. Because selectively controlling what data to release is difficult, the few data-sharing consortia that exist are often built around data-sharing agreements resulting from long and tedious one-off negotiations.},
	pages = {3172--3185},
	number = {11},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Xia, Siyuan and Zhu, Zhiru and Zhu, Chris and Zhao, Jinjin and Chard, Kyle and Elmore, Aaron J. and Foster, Ian and Franklin, Michael and Krishnan, Sanjay and Fernandez, Raul Castro},
	urldate = {2022-10-10},
	date = {2022-07},
	langid = {english},
}

@inproceedings{bonawitz_practical_2017,
	location = {Dallas, {TX}},
	title = {Practical Secure Aggregation for Privacy-Preserving Machine Learning},
	isbn = {978-1-4503-4946-8},
	url = {https://dl.acm.org/doi/10.1145/3133956.3133982},
	doi = {10.1145/3133956.3133982},
	eventtitle = {{CCS} '17},
	pages = {1175--1191},
	booktitle = {{ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {{ACM}},
	author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and {McMahan}, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
	urldate = {2021-09-23},
	date = {2017-10-30},
	langid = {english},
}

@inproceedings{mcmahan_communication-efficient_2017,
	location = {Fort Lauderdale, {FL}},
	title = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
	abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering ﬁve different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-{IID} data distributions that are a deﬁning characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10–100× as compared to synchronized stochastic gradient descent.},
	eventtitle = {{AISTATS} '17},
	pages = {1273--1282},
	booktitle = {20th International Conference on Artificial Intelligence and Statistics},
	author = {{McMahan}, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Agüera y Arcas, Blaise},
	date = {2017},
	langid = {english},
}

@inproceedings{mohassel_secureml_2017,
	location = {Son Jose, {CA}},
	title = {{SecureML}: A System for Scalable Privacy-Preserving Machine Learning},
	doi = {10.1109/SP.2017.12},
	shorttitle = {{SecureML}},
	abstract = {Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose {MPC}-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.},
	eventtitle = {{SP} '17},
	pages = {19--38},
	booktitle = {{IEEE} Symposium on Security and Privacy},
	author = {Mohassel, Payman and Zhang, Yupeng},
	date = {2017-05},
	keywords = {Data models, Linear regression, Logistics, Neural networks, Privacy, Privacy-preserving machine learning, Protocols, Training, secure computation},
}

@inproceedings{chen_when_2018,
	title = {When Machine Learning Meets Blockchain: A Decentralized, Privacy-preserving and Secure Design},
	doi = {10.1109/BigData.2018.8622598},
	shorttitle = {When Machine Learning Meets Blockchain},
	abstract = {With the onset of the big data era, designing efficient and effective machine learning algorithms to analyze large-scale data is in dire need. In practice, data is typically generated by multiple parties and stored in a geographically distributed manner, which spurs the study of distributed machine learning. Traditional master-worker type of distributed machine learning algorithms assumes a trusted central server and focuses on the privacy issue in linear learning models, while privacy in nonlinear learning models and security issues are not well studied. To address these issues, in this paper, we explore the blockchain technique to propose a decentralized privacy-preserving and secure machine learning system, called {LearningChain}, by considering a general (linear or nonlinear) learning model and without a trusted central server. Specifically, we design a decentralized Stochastic Gradient Descent ({SGD}) algorithm to learn a general predictive model over the blockchain. In decentralized {SGD}, we develop differential privacy based schemes to protect each party's data privacy, and propose an l-nearest aggregation algorithm to protect the system from potential Byzantine attacks. We also conduct theoretical analysis on the privacy and security of the proposed {LearningChain}. Finally, we implement {LearningChain} on Etheurum and demonstrate its efficiency and effectiveness through extensive experiments.},
	eventtitle = {Big Data '18},
	pages = {1178--1187},
	booktitle = {2018 {IEEE} International Conference on Big Data},
	author = {Chen, Xuhui and Ji, Jinlong and Luo, Changqing and Liao, Weixian and Li, Pan},
	date = {2018-12},
	keywords = {Blockchain, Byzantine Attack, Decentralized Machine Learning, Differential Privacy, Differential privacy, Machine learning, Machine learning algorithms, Privacy, Security, Servers},
}

@inproceedings{hu_fdml_2019,
	location = {Anchorage, {AK}},
	title = {{FDML}: A Collaborative Machine Learning Framework for Distributed Features},
	isbn = {978-1-4503-6201-6},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330765},
	doi = {10.1145/3292500.3330765},
	shorttitle = {{FDML}},
	abstract = {Most current distributed machine learning systems try to scale up model training by using a data-parallel architecture that divides the computation for different samples among workers. We study distributed machine learning from a different motivation, where the information about the same samples, e.g., users and objects, are owned by several parities that wish to collaborate but do not want to share raw data with each other. We propose an asynchronous stochastic gradient descent ({SGD}) algorithm for such a feature distributed machine learning ({FDML}) problem, to jointly learn from distributed features, with theoretical convergence guarantees under bounded asynchrony. Our algorithm does not require sharing the original features or even local model parameters between parties, thus preserving the data locality. The system can also easily incorporate differential privacy mechanisms to preserve a higher level of privacy. We implement the {FDML} system in a parameter server architecture and compare our system with fully centralized learning (which violates data locality) and learning based on only local features, through extensive experiments performed on both a public data set a9a, and a large dataset of 5, 000, 000 records and 8700 decentralized features from three collaborating apps at Tencent including T encent {MyApp}, T ecent {QQ} Browser and T encent Mobile Safeguard. Experimental results have demonstrated that the proposed {FDML} system can be used to significantly enhance app recommendation in Tencent {MyApp} by leveraging user and item features from other apps, while preserving the locality and privacy of features in each individual app to a high degree.},
	eventtitle = {{KDD} '19},
	pages = {2232--2240},
	booktitle = {25th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Hu, Yaochen and Niu, Di and Yang, Jianming and Zhou, Shengping},
	urldate = {2022-01-26},
	date = {2019-07-25},
	langid = {english},
}

@inproceedings{truex_hybrid_2019,
	location = {London, United Kingdom},
	title = {A Hybrid Approach to Privacy-Preserving Federated Learning},
	isbn = {978-1-4503-6833-9},
	url = {http://dl.acm.org/citation.cfm?doid=3338501.3357370},
	doi = {10.1145/3338501.3357370},
	abstract = {Federated learning facilitates the collaborative training of models without the sharing of raw data. However, recent attacks demonstrate that simply maintaining data locality during training processes does not provide sufficient privacy guarantees. Rather, we need a federated learning system capable of preventing inference over both the messages exchanged during training and the final trained model while ensuring the resulting model also has acceptable predictive accuracy. Existing federated learning approaches either use secure multiparty computation ({SMC}) which is vulnerable to inference or differential privacy which can lead to low accuracy given a large number of parties with relatively small amounts of data each. In this paper, we present an alternative approach that utilizes both differential privacy and {SMC} to balance these trade-offs. Combining differential privacy with secure multiparty computation enables us to reduce the growth of noise injection as the number of parties increases without sacrificing privacy while maintaining a pre-defined rate of trust. Our system is therefore a scalable approach that protects against inference threats and produces models with high accuracy. Additionally, our system can be used to train a variety of machine learning models, which we validate with experimental results on 3 different machine learning algorithms. Our experiments demonstrate that our approach out-performs state of the art solutions.},
	eventtitle = {{AISec} '19},
	pages = {1--11},
	booktitle = {12th {ACM} Workshop on Artificial Intelligence and Security},
	publisher = {{ACM} Press},
	author = {Truex, Stacey and Baracaldo, Nathalie and Anwar, Ali and Steinke, Thomas and Ludwig, Heiko and Zhang, Rui and Zhou, Yi},
	urldate = {2022-02-21},
	date = {2019},
	langid = {english},
}

@article{groger_there_2021,
	title = {There is no {AI} without data},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3448247},
	doi = {10.1145/3448247},
	abstract = {Industry experiences on the data challenges of {AI} and the call for a data ecosystem for industrial enterprises.},
	pages = {98--108},
	number = {11},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Gröger, Christoph},
	urldate = {2021-11-15},
	date = {2021-11},
	langid = {english},
}

@online{bundesministerium_fur_bildung_und_forschung_ikt_2021,
	title = {{IKT} 2020 - Forschung für Innovation},
	url = {https://www.bmbf.de/bmbf/de/forschung/digitale-wirtschaft-und-gesellschaft/informationsgesellschaft/ikt-2020/ikt-2020-forschung-fuer-innovation.html},
	abstract = {Informations- und Kommunikationstechnologien ({IKT}) sind der Innovationsmotor Nr. 1. In der Hightech-Strategie der Bundesregierung gehören {IKT} deshalb zu den bedeutendsten Innovationsfeldern.},
	author = {{Bundesministerium für Bildung und Forschung}},
	urldate = {2022-10-07},
	date = {2021},
	langid = {german},
}

@online{bundesministerium_fur_bildung_und_forschung_zukunftsprojekt_2016,
	title = {Zukunftsprojekt Industrie 4.0},
	url = {https://www.bmbf.de/bmbf/de/forschung/digitale-wirtschaft-und-gesellschaft/industrie-4-0/industrie-4-0.html},
	abstract = {Die Wirtschaft steht an der Schwelle zur vierten industriellen Revolution. Durch das Internet getrieben, wachsen reale und virtuelle Welt zu einem Internet der Dinge zusammen. Mit dem Projekt Industrie 4.0 wollen wir diesen Prozess unterstützen.},
	author = {{Bundesministerium für Bildung und Forschung}},
	urldate = {2022-10-07},
	date = {2016},
	langid = {german},
}

@online{bundesministerium_fur_bildung_und_forschung_zukunftsstrategie_2022,
	title = {Zukunftsstrategie Forschung und Innovation},
	url = {https://www.bmbf.de/bmbf/de/forschung/zukunftsstrategie/zukunftsstrategie_node.html},
	author = {{Bundesministerium für Bildung und Forschung}},
	urldate = {2022-10-07},
	date = {2022},
	langid = {german},
}

@online{bundesministerium_fur_umwelt_naturschutz_nukleare_sicherheit_und_verbraucherschutz_green-it-initiative_2022,
	title = {Green-{IT}-Initiative des Bundes},
	url = {https://www.bmuv.de/themen/nachhaltigkeit-digitalisierung/digitalisierung/green-it-initiative},
	author = {{Bundesministerium für Umwelt, Naturschutz, nukleare Sicherheit und Verbraucherschutz}},
	urldate = {2022-10-07},
	date = {2022},
	langid = {german},
}

@report{bundesministerium_fur_bildung_und_forschung_forschung_2018,
	title = {Forschung und Innovation für die Menschen},
	url = {https://www.bmbf.de/SharedDocs/Publikationen/de/bmbf/1/31431_Forschung_und_Innovation_fuer_die_Menschen.pdf?__blob=publicationFile&v=6},
	pages = {66},
	author = {{Bundesministerium für Bildung und Forschung}},
	urldate = {2022-10-07},
	date = {2018},
	langid = {german},
}

@report{bundesministerium_fur_wirtschaft_und_klimaschutz_nationale_2019,
	title = {Nationale Industriestrategie 2030},
	url = {https://www.bmwk.de/Redaktion/DE/Downloads/M-O/nationale-industriestrategie.pdf?__blob=publicationFile&v=29},
	pages = {20},
	author = {{Bundesministerium für Wirtschaft und Klimaschutz}},
	urldate = {2022-10-07},
	date = {2019},
	langid = {german},
}

@report{die_bundesregierung_strategie_2018,
	title = {Strategie Künstliche Intelligenz der Bundesregierung},
	url = {https://www.bmwk.de/Redaktion/DE/Publikationen/Technologie/strategie-kuenstliche-intelligenz-der-bundesregierung.pdf?__blob=publicationFile&v=10},
	author = {{Die Bundesregierung}},
	urldate = {2022-10-07},
	date = {2018-11},
}

@software{leclerc_ffcv_2022,
	title = {{FFCV}},
	url = {https://github.com/libffcv/ffcv/},
	version = {Commit e97289f},
	author = {Leclerc, Guillaume and Ilyas, Andrew and Engstrom, Logan and Park, Sung Min and Salman, Hadi and Madry, Aleksander},
	date = {2022},
}

@article{bradski_opencv_2000,
	title = {The {OpenCV} Library},
	journaltitle = {Dr. Dobb's Journal of Software Tools},
	author = {Bradski, G.},
	date = {2000},
	keywords = {bibtex-import},
}

@inproceedings{tan_efficientnet_2019,
	location = {Long Beach, {CA}},
	title = {{EfficientNet}: Rethinking Model Scaling for Convolutional Neural Networks},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/tan19a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {Convolutional Neural Networks ({ConvNets}) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on {MobileNets} and {ResNet}. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called {EfficientNets}, which achieve much better accuracy and efficiency than previous {ConvNets}. In particular, our {EfficientNet}-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on {ImageNet}, while being 8.4x smaller and 6.1x faster on inference than the best existing {ConvNet} (Huang et al., 2018). Our {EfficientNets} also transfer well and achieve state-of-the-art accuracy on {CIFAR}-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
	eventtitle = {{ICML} '19},
	pages = {6105--6114},
	booktitle = {36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Tan, Mingxing and Le, Quoc V.},
	date = {2019-06-09},
}

@article{asudeh_towards_2022,
	title = {Towards Distribution-aware Query Answering in Data Markets},
	volume = {15},
	issn = {150-8097},
	doi = {10.14778/3551793.3551858},
	abstract = {Addressing the increasing demand for data exchange has led to the development of data markets that facilitate transactional interactions between data buyers and data sellers. Still, cost-effective and distribution-aware query answering is a substantial challenge in these environments. In this paper, while differentiating different types of data markets, we take the initial steps towards addressing this challenge. In particular, we envision a unified query answering framework and discuss its functionalities. Our framework enables integrating data from different sources in a data market into a dataset that meets user-provided schema and distribution requirements cost-effectively. In order to facilitate consumers’ query answering, our system discovers data views in the form of join-paths on relevant data sources, defines a get-next operation to query views, and estimates the cost of get-next on each view. The query answering engine then selects the next views to sample sequentially to collect the output data. Depending on the knowledge of the system from the underlying data sources, the view selection problem can be modeled as an instance of a multi-arm bandit or coupon collector’s problem.},
	pages = {3137--3144},
	number = {11},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Asudeh, Abolfazl and Nargesian, Fatemeh},
	date = {2022},
	langid = {english},
}

@software{webdataset_contributors_webdataset_2022,
	title = {Webdataset: High Performance I/O for Large Scale Deep Learning},
	url = {https://github.com/webdataset/webdataset},
	abstract = {A high-performance Python-based I/O system for large (and small) deep learning problems, with strong support for {PyTorch}.},
	author = {{Webdataset contributors}},
	urldate = {2022-09-22},
	date = {2022},
}

@misc{huang_cross-silo_2022,
	title = {Cross-Silo Federated Learning: Challenges and Opportunities},
	url = {http://arxiv.org/abs/2206.12949},
	shorttitle = {Cross-Silo Federated Learning},
	abstract = {Federated learning ({FL}) is an emerging technology that enables the training of machine learning models from multiple clients while keeping the data distributed and private. Based on the participating clients and the model training scale, federated learning can be classiﬁed into two types: cross-device {FL} where clients are typically mobile devices and the client number can reach up to a scale of millions; cross-silo {FL} where clients are organizations or companies and the client number is usually small (e.g., within a hundred). While existing studies mainly focus on cross-device {FL}, this paper aims to provide an overview of the cross-silo {FL}. More speciﬁcally, we ﬁrst discuss applications of cross-silo {FL} and outline its major challenges. We then provide a systematic overview of the existing approaches to the challenges in cross-silo {FL} by focusing on their connections and differences to cross-device {FL}. Finally, we discuss future directions and open issues that merit research efforts from the community.},
	number = {{arXiv}:2206.12949},
	publisher = {{arXiv}},
	author = {Huang, Chao and Huang, Jianwei and Liu, Xin},
	urldate = {2022-09-20},
	date = {2022-06-26},
	langid = {english},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning},
}

@book{cramer_secure_2015,
	location = {{USA}},
	edition = {1st},
	title = {Secure Multiparty Computation and Secret Sharing},
	isbn = {1-107-04305-0},
	abstract = {In a data-driven society, individuals and companies encounter numerous situations where private information is an important resource. How can parties handle confidential data if they do not trust everyone involved? This text is the first to present a comprehensive treatment of unconditionally secure techniques for multiparty computation ({MPC}) and secret sharing. In a secure {MPC}, each party possesses some private data, while secret sharing provides a way for one party to spread information on a secret such that all parties together hold full information, yet no single party has all the information. The authors present basic feasibility results from the last 30 years, generalizations to arbitrary access structures using linear secret sharing, some recent techniques for efficiency improvements, and a general treatment of the theory of secret sharing, focusing on asymptotic results with interesting applications related to {MPC}.},
	publisher = {Cambridge University Press},
	author = {Cramer, Ronald and Damgård, Ivan Bjerre and Nielsen, Jesper Buus},
	date = {2015},
}

@misc{zhu_relationship_2020,
	title = {On the relationship between (secure) multi-party computation and (secure) federated learning},
	url = {http://arxiv.org/abs/2008.02609},
	abstract = {The contribution of this short note, contains the following two parts: – in the ﬁrst part, we are able to show that the federate learning ({FL}) procedure presented by Kairouz et al. [9], is a random processing. Namely, an m-ary functionality for the {FL} procedure can be deﬁned in the context of multi-party computation ({MPC}); Furthermore, an instance of {FL} protocol along Kairouz et al.’s deﬁnition can be viewed as an implementation of the deﬁned m-ary functionality. As such, an instance of {FL} procedure is also an instance of {MPC} protocol. In short, {FL} is a subset of {MPC}.},
	number = {2008.02609},
	publisher = {{arXiv}},
	author = {Zhu, Huafei},
	urldate = {2022-01-12},
	date = {2020-08-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.02609},
	keywords = {Computer Science - Cryptography and Security},
}

@article{wei_survey_2013,
	title = {A Survey of Faceted Search},
	volume = {12},
	rights = {Copyright (c) 2013 European Journal of Web Engineering},
	issn = {1544-5976},
	url = {https://journals.riverpublishers.com/index.php/JWE/},
	abstract = {Faceted Search is an exploratory search mechanism, which provides an iterative way to refine search results by a faceted taxonomy. With the benefit of search results diversification, no need for a priori knowledge, and never leading to zero result, it can significantly reduce information overload. Faceted Search has witnessed a booming interest in the last ten years. In this paper, we first analyze the representative facet search models. Next, we present a general faceted search framework, and survey the related methods and techniques, including facet term extraction, hierarchy construction, compound term generation and facet ranking. Then we discuss the metrics for faceted search evaluation, and also highlight the main characteristics of a number of existing faceted search systems. Some directions for future research are finally presented.
\&nbsp;},
	pages = {41--64},
	number = {1},
	journaltitle = {Journal of Web Engineering},
	author = {Wei, Bifan and Liu, Jun and Zheng, Qinghua and Zhang, Wei and Fu, Xiaoyu and Feng, Boqin},
	urldate = {2022-08-18},
	date = {2013-11-20},
	langid = {english},
	keywords = {Metrics},
}

@article{ouellette_ronin_2021,
	title = {{RONIN}: Data Lake Exploration},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3476311.3476364},
	doi = {10.14778/3476311.3476364},
	shorttitle = {{RONIN}},
	abstract = {Dataset discovery can be performed using search (with a query or keywords) to find relevant data. However, the result of this discovery can be overwhelming to explore. Existing navigation techniques mostly focus on linkage graphs that enable navigation from one data set to another based on similarity or joinability of attributes. However, users often do not know which data set to start the navigation from. {RONIN} proposes an alternative way to navigate by building a hierarchical structure on a collection of data sets: the user navigates between groups of data sets in a hierarchical manner to narrow down to the data of interest. We demonstrate {RONIN}, a tool that enables user exploration of a data lake by seamlessly integrating the two common modalities of discovery: data set search and navigation of a hierarchical structure. In {RONIN}, a user can perform a keyword search or joinability search over a data lake, then, navigate the result using a hierarchical structure, called an organization, that is created on the fly. While navigating an organization, the user may switch to the search mode, and back to navigation on an organization that is updated based on search. This integration of search and navigation provides great power in allowing users to find and explore interesting data in a data lake.},
	pages = {2863--2866},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Ouellette, Paul and Sciortino, Aidan and Nargesian, Fatemeh and Bashardoost, Bahar Ghadiri and Zhu, Erkang and Pu, Ken Q. and Miller, Renée J.},
	urldate = {2022-08-17},
	date = {2021-07},
	langid = {english},
}

@article{rezig_dice_2021,
	title = {{DICE}: Data Discovery by Example},
	volume = {14},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3476311.3476353},
	doi = {10.14778/3476311.3476353},
	abstract = {In order to conduct analytical tasks, data scientists often need to find relevant data from an avalanche of sources (e.g., data lakes, large organizational databases). This effort is typically made in an ad hoc, non-systematic manner, which makes it a daunting endeavour. Current data discovery systems typically require the users to find relevant tables manually, usually by issuing multiple queries (e.g., using {SQL}). However, expressing such queries is nontrivial, as it requires knowledge of the underlying structure (schema) of the data organization in advance. This issue is further exacerbated when data resides in data lakes, where there is no predefined schema that data must conform to. On the other hand, data scientists can often come up with a few example records of interest quickly. Motivated by this observation, we developed {DICE}—a human-in-the-loop system for {\textless}u{\textgreater}D{\textless}/u{\textgreater}ata d{\textless}u{\textgreater}I{\textless}/u{\textgreater}s{\textless}u{\textgreater}C{\textless}/u{\textgreater}overy by {\textless}u{\textgreater}E{\textless}/u{\textgreater}xample—that takes user-provided example records as input and returns more records that satisfy the user intent. {DICE}'s key idea is to synthesize a {SQL} query that captures the user intent, specified via examples. To this end, {DICE} follows a three-step process: (1) {DICE} first discovers a few candidate queries by finding join paths across tables within the data lake. (2) Then {DICE} consults with the user for validation by presenting a few records to them, and, thus, eliminating spurious queries. (3) Based on the user feedback, {DICE} refines the search and repeats the process until the user is satisfied with the results. We will demonstrate how {DICE} can help in data discovery through an interactive, example-based interaction.},
	pages = {2819--2822},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Rezig, El Kindi and Bhandari, Anshul and Fariha, Anna and Price, Benjamin and Vanterpool, Allan and Gadepally, Vijay and Stonebraker, Michael},
	date = {2021-07},
	note = {Publisher: {VLDB} Endowment},
}

@article{megler_are_2015,
	title = {Are Data Sets Like Documents?: Evaluating Similarity-Based Ranked Search over Scientific Data},
	volume = {27},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/6807734/},
	doi = {10.1109/TKDE.2014.2320737},
	shorttitle = {Are Data Sets Like Documents?},
	abstract = {The past decade has seen a dramatic increase in the amount of data captured and made available to scientists for research. This increase amplifies the difficulty scientists face in finding the data most relevant to their information needs. In prior work, we hypothesized that Information Retrieval-style ranked search can be applied to datasets to help a scientist discover the most relevant data amongst the thousands of datasets in many formats, much like text-based ranked search helps users make sense of the vast number of Internet documents. To test this hypothesis, we explored the use of ranked search for scientific data using an existing multi-terabyte observational archive as our test-bed. In this paper, we investigate whether the concept of varying relevance, and therefore ranked search, applies to numeric data – that is, are data sets are enough like documents for Information Retrieval techniques and evaluation measures to apply? We present a user study that demonstrates that dataset similarity resonates with users as a basis for relevance and, therefore, for ranked search. We evaluate a prototype implementation of ranked search over datasets with a second user study and demonstrate that ranked search improves a scientist’s ability to find needed data.},
	pages = {32--45},
	number = {1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Megler, V. M. and Maier, David},
	urldate = {2022-08-11},
	date = {2015-01-01},
	langid = {english},
}

@inproceedings{fernandez_towards_2016,
	location = {San Francisco, {CA}},
	title = {Towards Large-Scale Data Discovery: Position Paper},
	isbn = {978-1-4503-4312-1},
	url = {https://doi.org/10.1145/2948674.2948675},
	doi = {10.1145/2948674.2948675},
	series = {{ExploreDB} '16},
	abstract = {With thousands of data sources spread across multiple databases and data lakes, modern organizations face a data discovery challenge. Analysts spend more time finding relevant data to answer the questions at hand than analyzing it.In this paper we introduce a data discovery system that facilitates locating relevant data among thousands of data sources. We represent data sources succinctly through signatures, and then create search paths that permit quick execution of a set of data discovery primitives used for finding relevant data. We have built a prototype that is being used to solve data discovery challenges of two big organizations.},
	eventtitle = {{ExploreDB} '16},
	pages = {3--5},
	booktitle = {3rd International Workshop on Exploratory Search in Databases and the Web},
	publisher = {Association for Computing Machinery},
	author = {Fernandez, Raul Castro and Abedjan, Ziawasch and Madden, Samuel and Stonebraker, Michael},
	date = {2016},
	note = {event-place: San Francisco, California},
}

@article{roh_survey_2021,
	title = {A Survey on Data Collection for Machine Learning: A Big Data - {AI} Integration Perspective},
	volume = {33},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2019.2946162},
	shorttitle = {A Survey on Data Collection for Machine Learning},
	abstract = {Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence ({AI}) integration and opens many opportunities for new research.},
	pages = {1328--1347},
	number = {4},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
	date = {2021-04},
	keywords = {Data acquisition, Data collection, Data models, Labeling, Machine learning, Smart manufacturing, Training data, data acquisition, data labeling, machine learning},
}

@inproceedings{bhardwaj_datahub_2015,
	location = {Asilomar, {CA}},
	title = {{DataHub}: Collaborative Data Science \& Dataset Version Management at Scale},
	abstract = {Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, {DATAHUB}, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.},
	eventtitle = {{CIDR} '15},
	pages = {7},
	booktitle = {7th Biennial Conference on Innovative Data Systems Research},
	author = {Bhardwaj, Anant and Bhattacherjee, Souvik and Chavan, Amit and Deshpande, Amol and Elmore, Aaron J. and Madden, Samuel and Parameswaran, Aditya},
	date = {2015-01-07},
	langid = {english},
}

@inproceedings{parekh_ontology-based_2004,
	title = {Ontology-based Semantic Metadata for Geoscience Data},
	abstract = {In Geoscience domain, large amounts of data are accessible, however they vary in formats and are stored at various organizations leading to problems of data discovery, data interoperability and usability. In this paper, we propose a new semantic metadata paradigm based on ontologies and the use of Semantic Web languages. Our suggested data model ontology is used to guide the generation of metadata for individual datasets. This data model ontology defines elements to incorporate information about data identification, spatial extent, temporal extent, data presentation form, data content and data distribution regarding the dataset. Combining domain specific ontologies with this data model ontology offers a new approach to the generation of semantic metadata for datasets. The system allows the data provider to select concepts from domain ontologies that best describe the content within the dataset. This selection along with the links to domain ontologies is stored within the metadata file, thereby generating semantic metadata for the dataset. This metadata is capable of facilitating the end users of data with content based discovery of datasets irrespective of their locations and formats.},
	pages = {485--490},
	booktitle = {Proceedings of the International Conference on Information and Knowledge Engineering},
	author = {Parekh, Viral and Gwo, Jin-Ping Jack and Finin, Tim},
	date = {2004},
}

@article{kacprzak_characterising_2019,
	title = {Characterising Dataset Search—An Analysis of Search Logs and Data Requests},
	volume = {55},
	issn = {1570-8268},
	url = {https://www.sciencedirect.com/science/article/pii/S1570826818300556},
	doi = {https://doi.org/10.1016/j.websem.2018.11.003},
	abstract = {Large amounts of data are becoming increasingly available online. In order to benefit from it we need tools to retrieve the most relevant datasets that match ones data needs. Several vocabularies have been developed to describe datasets in order to increase their discoverability, but for data publishers is costly to cumbersome to annotate them using all, leading to the question of what properties are more important. In this work we contribute with a systematic study of the patterns and specific attributes that data consumers use to search for data and how it compares with general web search. We performed a query log analysis based on logs from four national open data portals and conducted a qualitative analysis of user data requests for requests issued to one of them. Search queries issued on data portals differ from those issued to web search engines in their length, topic, and structure. Based on our findings we hypothesise that portals search functionalities are currently used in an exploratory manner, rather than to retrieve a specific resource. In our study of data requests we found that geospatial and temporal attributes, as well as information on the required granularity of the data are the most common features. The findings of both analyses suggest that these features are of higher importance in dataset retrieval in contrast to general web search, suggesting that efforts of dataset publishers should focus on generating dataset descriptions including them.},
	pages = {37--55},
	journaltitle = {Journal of Web Semantics},
	shortjournal = {J. Web Semant.},
	author = {Kacprzak, Emilia and Koesten, Laura and Ibáñez, Luis-Daniel and Blount, Tom and Tennison, Jeni and Simperl, Elena},
	date = {2019},
	keywords = {Dataset search, Search logs, Vertical search},
}

@inproceedings{koesten_trials_2017,
	location = {Denver Colorado {USA}},
	title = {The Trials and Tribulations of Working with Structured Data - A Study on Information Seeking Behaviour},
	isbn = {978-1-4503-4655-9},
	url = {https://dl.acm.org/doi/10.1145/3025453.3025838},
	doi = {10.1145/3025453.3025838},
	shorttitle = {The Trials and Tribulations of Working with Structured Data},
	abstract = {Structured data such as databases, spreadsheets and web tables is becoming critical in every domain and professional role. Yet we still do not know much about how people interact with it. Our research focuses on the information seeking behaviour of people looking for new sources of structured data online, including the task context in which the data will be used, data search, and the identiﬁcation of relevant datasets from a set of possible candidates. We present a mixed-methods study covering in-depth interviews with 20 participants with various professional backgrounds, supported by the analysis of search logs of a large data portal. Based on this study, we propose a framework for human structured-data interaction and discuss challenges people encounter when trying to ﬁnd and assess data that helps their daily work. We provide design recommendations for data publishers and developers of online data platforms such as data catalogs and marketplaces. These recommendations highlight important questions for {HCI} research to improve how people engage and make use of this incredibly useful online resource.},
	eventtitle = {{CHI} '17},
	pages = {1277--1289},
	booktitle = {2017 Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Koesten, Laura M. and Kacprzak, Emilia and Tennison, Jenifer F. A. and Simperl, Elena},
	urldate = {2022-08-01},
	date = {2017-05-02},
	langid = {english},
}

@inproceedings{reddi_adaptive_2021,
	title = {Adaptive Federated Optimization},
	url = {http://arxiv.org/abs/2003.00295},
	abstract = {Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging ({FEDAVG}) are often difﬁcult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including {ADAGRAD}, {ADAM}, and {YOGI}, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efﬁciency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can signiﬁcantly improve the performance of federated learning.},
	eventtitle = {{ICLR} '21},
	booktitle = {International Conference on Learning Representations},
	publisher = {{arXiv}},
	author = {Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Konečný, Jakub and Kumar, Sanjiv and {McMahan}, H. Brendan},
	urldate = {2022-07-29},
	date = {2021-09-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.00295 [cs, math, stat]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{halevy_managing_2016,
	title = {Managing Google’s data lake: an overview of the {GOODS} system},
	abstract = {For most large enterprises today, data constitutes their core asset, along with code and infrastructure. For most enterprises, the amount of data that they produce internally has exploded in recent years. At the same time, in many cases, engineers and data scientists do not use centralized data-management systems and end up creating what became known as a data lake—a collection of datasets that often are not well organized or not organized at all and where one needs to “ﬁsh” for useful datasets. In this paper, we describe our experience building and deploying {GOODS}, a system to manage Google’s internal data lake. {GOODS} crawls Google’s infrastructure and builds a catalog of discovered datasets, including structured ﬁles, databases, spreadsheets, and even services that provide access to the data. {GOODS} extracts metadata about datasets in a post-hoc way: engineers continue to generate and organize datasets in the same way that they have before, and {GOODS} provides value without disrupting teams’ practices. The technical challenges that we had to address resulted both from the scale and heterogeneity of Google’s data lake and from our decision to extract metadata in a post-hoc manner. We believe that many of the lessons that we learned are applicable to building large-scale enterprise-level data-management systems in general.},
	pages = {10},
	journaltitle = {Bulletin of the {IEEE} Computer Society Technical Committee on Data Engineering},
	author = {Halevy, Alon and Korn, Flip and Noy, Natalya F and Olston, Christopher and Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong},
	date = {2016},
	langid = {english},
}

@article{sansone_dats_2017,
	title = {{DATS}, the data tag suite to enable discoverability of datasets},
	volume = {4},
	issn = {2052-4463},
	url = {https://doi.org/10.1038/sdata.2017.59},
	doi = {10.1038/sdata.2017.59},
	abstract = {Today's science increasingly requires effective ways to find and access existing datasets that are distributed across a range of repositories. For researchers in the life sciences, discoverability of datasets may soon become as essential as identifying the latest publications via {PubMed}. Through an international collaborative effort funded by the National Institutes of Health ({NIH})'s Big Data to Knowledge ({BD}2K) initiative, we have designed and implemented the {DAta} Tag Suite ({DATS}) model to support the {DataMed} data discovery index. {DataMed}'s goal is to be for data what {PubMed} has been for the scientific literature. Akin to the Journal Article Tag Suite ({JATS}) used in {PubMed}, the {DATS} model enables submission of metadata on datasets to {DataMed}. {DATS} has a core set of elements, which are generic and applicable to any type of dataset, and an extended set that can accommodate more specialized data types. {DATS} is a platform-independent model also available as an annotated serialization in schema.org, which in turn is widely used by major search engines like Google, Microsoft, Yahoo and Yandex.},
	pages = {8},
	number = {170059},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Sansone, Susanna-Assunta and Gonzalez-Beltran, Alejandra and Rocca-Serra, Philippe and Alter, George and Grethe, Jeffrey S. and Xu, Hua and Fore, Ian M. and Lyle, Jared and Gururaj, Anupama E. and Chen, Xiaoling and Kim, Hyeon-eui and Zong, Nansu and Li, Yueling and Liu, Ruiling and Ozyurt, I. Burak and Ohno-Machado, Lucila},
	urldate = {2022-07-22},
	date = {2017-06-06},
}

@article{mork_facilitating_2010,
	title = {Facilitating Discovery on the Private Web Using Dataset Digests},
	volume = {5},
	issn = {1744-2621, 1744-263X},
	url = {http://www.inderscience.com/link.php?id=34042},
	doi = {10.1504/IJMSO.2010.034042},
	abstract = {Whereas strategies for discovering content on the surface web are commonplace, similar strategies for the private web are non-existent. In this paper, we first establish a general framework for advertising the existence of private web resources that subsumes many existing summarisation strategies, and is based on succinct statistical summaries (which we call digests). We then investigate the trade-off between the data owners’ desires to minimise disclosure of sensitive information and the searchers’ desires to minimise query error, demonstrating that our techniques are superior to using k-anonymity for that purpose. Finally, we show that our techniques for summarisation do, in fact, make it possible to discover private web data resources.},
	pages = {170--183},
	number = {3},
	journaltitle = {International Journal of Metadata, Semantics and Ontologies},
	shortjournal = {{IJMSO}},
	author = {Mork, Peter and Smith, Ken and Blaustein, Barbara and Wolf, Christopher and Samuel, Ken and Sarver, Keri and Vayndiner, Irina},
	urldate = {2022-07-22},
	date = {2010},
	langid = {english},
}

@inproceedings{kunze_dataset_2013,
	title = {Dataset Retrieval},
	doi = {10.1109/ICSC.2013.12},
	abstract = {Recently, a large number of dataset repositories, catalogs and portals are emerging in the science and government realms. Once a large number of datasets are published on such data portals, the question arises how to retrieve datasets satisfying an information need. In this paper, we present an approach for retrieving datasets according to user queries. We define dataset retrieval as a specialization of information retrieval. Instead of retrieving documents that are relevant to a certain information need, dataset retrieval describes the process of returning relevant {RDF} datasets. As with information retrieval, the term relevance cannot be clearly defined when using traditional methods like stemming. The inherent usage of {RDF} in these datasets enables a better way of retrieving relevant ones. We therefore propose an additional retrieval mechanism, which is inspired by facet search: dataset filtering. When querying, the entire set of available datasets is processed by a set of semantic filters each of which can unambiguously decide whether or not a given dataset is relevant to the query. The resulting set is then given back to the requester. We implemented and evaluated our approach in {CKAN}, which fuels publicdata.eu and is the most popular data portal worldwide.},
	eventtitle = {{ICSC} '13},
	pages = {1--8},
	booktitle = {7th International Conference on Semantic Computing},
	author = {Kunze, Sven R. and Auer, Sören},
	date = {2013-09},
	keywords = {Catalogs, Companies, Portals, {RDF}, Resource description framework, Semantics, {VoID}, Vocabulary, dataset filtering, dataset repository, dataset retrieval, information retrieval, similarity relation, vocabulary},
}

@article{guha_schemaorg_2016,
	title = {Schema.org: Evolution of Structured Data on the Web},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2844544},
	doi = {10.1145/2844544},
	shorttitle = {Schema.org},
	abstract = {Big data makes common schemas even more necessary.},
	pages = {44--51},
	number = {2},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Guha, R. V. and Brickley, Dan and Macbeth, Steve},
	urldate = {2022-07-22},
	date = {2016-01-25},
	langid = {english},
}

@incollection{sack_dataset_2016,
	location = {Cham},
	title = {Dataset Recommendation for Data Linking: An Intensional Approach},
	volume = {9678},
	isbn = {978-3-319-34128-6 978-3-319-34129-3},
	url = {https://doi.org/10.1007/978-3-319-34129-3_3},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Dataset Recommendation for Data Linking},
	abstract = {With the growing quantity and diversity of publicly available web datasets, most notably Linked Open Data, recommending datasets, which meet speciﬁc criteria, has become an increasingly important, yet challenging problem. This task is of particular interest when addressing issues such as entity retrieval, semantic search and data linking. Here, we focus on that last issue. We introduce a dataset recommendation approach to identify linking candidates based on the presence of schema overlap between datasets. While an understanding of the nature of the content of speciﬁc datasets is a crucial prerequisite, we adopt the notion of dataset proﬁles, where a dataset is characterized through a set of schema concept labels that best describe it and can be potentially enriched by retrieving their textual descriptions. We identify schema overlap by the help of a semantico-frequential concept similarity measure and a ranking criterium based on the tf*idf cosine similarity. The experiments, conducted over all available linked datasets on the Linked Open Data cloud, show that our method achieves an average precision of up to 53 \% for a recall of 100 \%. As an additional contribution, our method returns the mappings between the schema concepts across datasets – a particularly useful input for the data linking step.},
	pages = {36--51},
	booktitle = {The Semantic Web. Latest Advances and New Domains},
	publisher = {Springer International Publishing},
	author = {Ben Ellefi, Mohamed and Bellahsene, Zohra and Dietze, Stefan and Todorov, Konstantin},
	editor = {Sack, Harald and Blomqvist, Eva and d'Aquin, Mathieu and Ghidini, Chiara and Ponzetto, Simone Paolo and Lange, Christoph},
	urldate = {2022-07-22},
	date = {2016},
	langid = {english},
}

@article{gebru_datasheets_2021,
	title = {Datasheets for Datasets},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3458723},
	doi = {10.1145/3458723},
	abstract = {Documentation to facilitate communication between dataset creators and consumers.},
	pages = {86--92},
	number = {12},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daumé {III}, Hal and Crawford, Kate},
	urldate = {2022-07-22},
	date = {2021-12},
	langid = {english},
}

@article{chapman_dataset_2020,
	title = {Dataset Search: A Survey},
	volume = {29},
	issn = {1066-8888},
	url = {https://doi.org/10.1007/s00778-019-00564-x},
	doi = {10.1007/s00778-019-00564-x},
	abstract = {Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta-released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems and discuss what makes dataset search a field in its own right, with unique challenges and open questions. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to tackle these questions as well as immediate next steps that will take the field forward.},
	pages = {251--272},
	number = {1},
	journaltitle = {The {VLDB} Journal},
	author = {Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ibáñez, Luis-Daniel and Kacprzak, Emilia and Groth, Paul},
	date = {2020-01},
	keywords = {Dataset, Dataset retrieval, Dataset search, Information search and retrieval},
}

@article{gregory_searching_2019,
	title = {Searching Data: A Review of Observational Data Retrieval Practices in Selected Disciplines},
	volume = {70},
	issn = {2330-1635, 2330-1643},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/asi.24165},
	doi = {10.1002/asi.24165},
	shorttitle = {Searching Data},
	abstract = {A cross-disciplinary examination of the user behaviors involved in seeking and evaluating data is surprisingly absent from the research data discussion. This review explores the data retrieval literature to identify commonalities in how users search for and evaluate observational research data in selected disciplines. Two analytical frameworks, rooted in information retrieval and science and technology studies, are used to identify key similarities in practices as a first step toward developing a model describing data retrieval.},
	pages = {419--432},
	number = {5},
	journaltitle = {Journal of the Association for Information Science and Technology},
	shortjournal = {{JASIST}},
	author = {Gregory, Kathleen and Groth, Paul and Cousijn, Helena and Scharnhorst, Andrea and Wyatt, Sally},
	urldate = {2022-07-20},
	date = {2019-05},
	langid = {english},
}

@inproceedings{benjelloun_google_2020,
	location = {Athens, Greece},
	title = {Google Dataset Search by the Numbers},
	isbn = {978-3-030-62466-8},
	doi = {10.1007/978-3-030-62466-8_41},
	abstract = {Scientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadata—expressed using schema.org and similar vocabularies—from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.},
	eventtitle = {{ISWC} '20},
	pages = {667--682},
	booktitle = {19th International Semantic Web Conference},
	publisher = {Springer International Publishing},
	author = {Benjelloun, Omar and Chen, Shiyu and Noy, Natasha},
	date = {2020},
}

@article{abedjan_profiling_2015,
	title = {Profiling Relational Data: A Survey},
	volume = {24},
	issn = {0949-877X},
	url = {https://doi.org/10.1007/s00778-015-0389-y},
	doi = {10.1007/s00778-015-0389-y},
	abstract = {Profiling data to determine metadata about a given dataset is an important and frequent activity of any {IT} professional and researcher and is necessary for various use-cases. It encompasses a vast array of methods to examine datasets and produce metadata. Among the simpler results are statistics, such as the number of null values and distinct values in a column, its data type, or the most frequent patterns of its data values. Metadata that are more difficult to compute involve multiple columns, namely correlations, unique column combinations, functional dependencies, and inclusion dependencies. Further techniques detect conditional properties of the dataset at hand. This survey provides a classification of data profiling tasks and comprehensively reviews the state of the art for each class. In addition, we review data profiling tools and systems from research and industry. We conclude with an outlook on the future of data profiling beyond traditional profiling tasks and beyond relational databases.},
	pages = {557--581},
	number = {4},
	journaltitle = {The {VLDB} Journal},
	author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
	date = {2015-08-01},
}

@article{castelo_auctus_2021,
	title = {Auctus: A Dataset Search Engine for Data Discovery and Augmentation},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3476311.3476346},
	doi = {10.14778/3476311.3476346},
	shorttitle = {Auctus},
	abstract = {The large volumes of structured data currently available, from Web tables to open-data portals and enterprise data, open up new opportunities for progress in answering many important scientific, societal, and business questions. However, finding relevant data is difficult. While search engines have addressed this problem for Web documents, there are many new challenges involved in supporting the discovery of structured data. We demonstrate how the Auctus dataset search engine addresses some of these challenges. We describe the system architecture and how users can explore datasets through a rich set of queries. We also present case studies which show how Auctus supports data augmentation to improve machine learning models as well as to enrich analytics.},
	pages = {2791--2794},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Castelo, Sonia and Rampin, Rémi and Santos, Aécio and Bessa, Aline and Chirigati, Fernando and Freire, Juliana},
	urldate = {2022-07-19},
	date = {2021-07},
	langid = {english},
}

@collection{ludwig_federated_2022,
	location = {Cham},
	title = {Federated Learning: A Comprehensive Overview of Methods and Applications},
	isbn = {978-3-030-96895-3 978-3-030-96896-0},
	url = {https://link.springer.com/10.1007/978-3-030-96896-0},
	shorttitle = {Federated Learning},
	publisher = {Springer International Publishing},
	editor = {Ludwig, Heiko and Baracaldo, Nathalie},
	urldate = {2022-07-13},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-030-96896-0},
}

@inproceedings{zheng_cerebro_2021,
	title = {Cerebro: A Platform for Multi-Party Cryptographic Collaborative Learning},
	isbn = {978-1-939133-24-3},
	url = {https://www.usenix.org/conference/usenixsecurity21/presentation/zheng},
	abstract = {Many organizations need large amounts of high quality data for their applications, and one way to acquire such data is to combine datasets from multiple parties. Since these organizations often own sensitive data that cannot be shared in the clear with others due to policy regulation and business competition, there is increased interest in utilizing secure multi-party computation ({MPC}). {MPC} allows multiple parties to jointly compute a function without revealing their inputs to each other. We present Cerebro, an end-to-end collaborative learning platform that enables parties to compute learning tasks without sharing plaintext data. By taking an end-to-end approach to the system design, Cerebro allows multiple parties with complex economic relationships to safely collaborate on machine learning computation through the use of release policies and auditing, while also enabling users to achieve good performance without manually navigating the complex performance tradeoffs between {MPC} protocols.},
	eventtitle = {{USENIX} Security '21},
	pages = {2723--2740},
	booktitle = {30th {USENIX} Security Symposium},
	publisher = {{USENIX} Association},
	author = {Zheng, Wenting and Deng, Ryan and Chen, Weikeng and Popa, Raluca Ada and Panda, Aurojit and Stoica, Ion},
	date = {2021-08},
}

@misc{wu_serving_2022,
	title = {Serving and Optimizing Machine Learning Workflows on Heterogeneous Infrastructures},
	url = {http://arxiv.org/abs/2205.04713},
	abstract = {With the advent of ubiquitous deployment of smart devices and the Internet of Things, data sources for machine learning inference have increasingly moved to the edge of the network. Existing machine learning inference platforms typically assume a homogeneous infrastructure and do not take into account the more complex and tiered computing infrastructure that includes edge devices, local hubs, edge datacenters, and cloud datacenters. On the other hand, recent machine learning efforts have provided viable solutions for model compression, pruning and quantization for heterogeneous environments; for a machine learning model, now we may easily find or even generate a series of models with different tradeoffs between accuracy and efficiency.},
	number = {{arXiv}:2205.04713},
	publisher = {{arXiv}},
	author = {Wu, Yongji and Lentz, Matthew and Zhuo, Danyang and Lu, Yao},
	urldate = {2022-06-02},
	date = {2022-05-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2205.04713 [cs]},
	note = {Number: {arXiv}:2205.04713},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@misc{zheng_fl-market_2022,
	title = {{FL}-Market: Trading Private Models in Federated Learning},
	url = {http://arxiv.org/abs/2106.04384},
	shorttitle = {{FL}-Market},
	abstract = {The difficulty in acquiring a sufficient amount of training data is a major bottleneck for machine learning ({ML}) based data analytics. Recently, commoditizing {ML} models has been proposed as an economical and moderate solution to {ML}-oriented data acquisition. However, existing model marketplaces assume that the broker can access data owners’ private training data, which may not be realistic in practice. In this paper, to promote trustworthy data acquisition for {ML} tasks, we propose {FL}-Market, a locally private model marketplace that protects privacy not only against model buyers but also against the untrusted broker. {FL}-Market decouples {ML} from the need to centrally gather training data on the broker’s side using federated learning, an emerging privacy-preserving {ML} paradigm in which data owners collaboratively train an {ML} model by uploading local gradients (to be aggregated into a global gradient for model updating). Then, {FL}-Market enables data owners to locally perturb their gradients by local differential privacy and thus further prevents privacy risks. To drive {FL}-Market, we propose a deep learning-empowered auction mechanism for intelligently deciding the local gradients’ perturbation levels and an optimal aggregation mechanism for aggregating the perturbed gradients. Our auction and aggregation mechanisms can jointly maximize the global gradient’s accuracy, which optimizes model buyers’ utility. Our experiments verify the effectiveness of the proposed mechanisms.},
	number = {{arXiv}:2106.04384},
	publisher = {{arXiv}},
	author = {Zheng, Shuyuan and Cao, Yang and Yoshikawa, Masatoshi and Li, Huizhong and Yan, Qiang},
	urldate = {2022-07-05},
	date = {2022-07-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2106.04384 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Machine Learning},
}

@article{reina_openfl_2021,
	title = {{OpenFL}: An open-source framework for Federated Learning},
	url = {http://arxiv.org/abs/2105.06413},
	shorttitle = {{OpenFL}},
	abstract = {Federated learning ({FL}) is a computational paradigm that enables organizations to collaborate on machine learning ({ML}) projects without sharing sensitive data, such as, patient records, ﬁnancial data, or classiﬁed secrets. Open Federated Learning ({OpenFL})5 is an open-source framework for training {ML} algorithms using the data-private collaborative learning paradigm of {FL}. {OpenFL} works with training pipelines built with both {TensorFlow} and {PyTorch}, and can be easily extended to other {ML} and deep learning frameworks. Here, we summarize the motivation and development characteristics of {OpenFL}, with the intention of facilitating its application to existing {ML} model training in a production environment. Finally, we describe the ﬁrst use of the {OpenFL} framework to train consensus {ML} models in a consortium of international healthcare organizations, as well as how it facilitates the ﬁrst computational competition on {FL}.},
	journaltitle = {{arXiv}:2105.06413},
	author = {Reina, G. Anthony and Gruzdev, Alexey and Foley, Patrick and Perepelkina, Olga and Sharma, Mansi and Davidyuk, Igor and Trushkin, Ilya and Radionov, Maksim and Mokrov, Aleksandr and Agapov, Dmitry and Martin, Jason and Edwards, Brandon and Sheller, Micah J. and Pati, Sarthak and Moorthy, Prakash Narayana and Wang, Shih-han and Shah, Prashant and Bakas, Spyridon},
	urldate = {2022-06-01},
	date = {2021-05-13},
	langid = {english},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{paulik_federated_2021,
	title = {Federated Evaluation and Tuning for On-Device Personalization: System Design \& Applications},
	url = {http://arxiv.org/abs/2102.08503},
	shorttitle = {Federated Evaluation and Tuning for On-Device Personalization},
	abstract = {We describe the design of our federated task processing system. Originally, the system was created to support two speciﬁc federated tasks: evaluation and tuning of on-device {ML} systems, primarily for the purpose of personalizing these systems. In recent years, support for an additional federated task has been added: federated learning ({FL}) of deep neural networks. To our knowledge, only one other system has been described in literature that supports {FL} at scale. We include comparisons to that system to help discuss design decisions and attached trade-offs. Finally, we describe two speciﬁc large scale personalization use cases in detail to showcase the applicability of federated tuning to on-device personalization and to highlight application speciﬁc solutions.},
	journaltitle = {{arXiv}:2102.08503},
	author = {Paulik, Matthias and Seigel, Matt and Mason, Henry and Telaar, Dominic and Kluivers, Joris and van Dalen, Rogier and Lau, Chi Wai and Carlson, Luke and Granqvist, Filip and Vandevelde, Chris and Agarwal, Sudeep and Freudiger, Julien and Byde, Andrew and Bhowmick, Abhishek and Kapoor, Gaurav and Beaumont, Si and Cahill, Áine and Hughes, Dominic and Javidbakht, Omid and Dong, Fei and Rishi, Rehan and Hung, Stanley},
	urldate = {2022-06-09},
	date = {2021-02-16},
	langid = {english},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{zhu_entropy-aware_2018,
	location = {Milwaukee, {WI}},
	title = {Entropy-Aware I/O Pipelining for Large-Scale Deep Learning on {HPC} Systems},
	doi = {10.1109/MASCOTS.2018.00023},
	abstract = {Deep neural networks have recently gained tremendous interest due to their capabilities in a wide variety of application areas such as computer vision and speech recognition. Thus it is important to exploit the unprecedented power of leadership High-Performance Computing ({HPC}) systems for greater potential of deep learning. While much attention has been paid to leverage the latest processors and accelerators, I/O support also needs to keep up with the growth of computing power for deep neural networks. In this research, we introduce an entropy-aware I/O framework called {DeepIO} for large-scale deep learning on {HPC} systems. Its overarching goal is to coordinate the use of memory, communication, and I/O resources for efficient training of datasets. {DeepIO} features an I/O pipeline that utilizes several novel optimizations: {RDMA} (Remote Direct Memory Access)-assisted in-situ shuffling, input pipelining, and entropy-aware opportunistic ordering. In addition, we design a portable storage interface to support efficient I/O on any underlying storage system. We have implemented {DeepIO} as a prototype for the popular {TensorFlow} framework and evaluated it on a variety of different storage systems. Our evaluation shows that {DeepIO} delivers significantly better performance than existing memory-based storage systems.},
	eventtitle = {{MASCOTS} '18},
	pages = {145--156},
	booktitle = {26th {IEEE} International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems},
	author = {Zhu, Yue and Chowdhury, Fahim and Fu, Huansong and Moody, Adam and Mohror, Kathryn and Sato, Kento and Yu, Weikuan},
	date = {2018-09},
	keywords = {Bandwidth, Deep Neural Network, {HPC}, I/O, Neural networks, Pipeline processing, Prediction algorithms, Tensile stress, {TensorFlow}, Training},
}

@inproceedings{dubois_lossy_2021,
	title = {Lossy Compression for Lossless Prediction},
	url = {https://openreview.net/forum?id=GfCs5NhoR8Q},
	abstract = {Most data is automatically collected and only ever “seen” by algorithms. Yet, data compressors preserve perceptual ﬁdelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than 1000× on {ImageNet}) compared to {JPEG} on 8 datasets, without decreasing downstream classiﬁcation performance.},
	eventtitle = {Neural Compression Workshop at {ICLR} '21},
	author = {Dubois, Yann and Bloem-Reddy, Benjamin and Ullrich, Karen and Maddison, Chris J.},
	date = {2021},
}

@article{li_data_2021,
	title = {Data Acquisition for Improving Machine Learning Models},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3467861.3467872},
	doi = {10.14778/3467861.3467872},
	abstract = {The vast advances in Machine Learning ({ML}) over the last ten years have been powered by the availability of suitably prepared data for training purposes. The future of {ML}-enabled enterprise hinges on data. As such, there is already a vibrant market offering data annotation services to tailor sophisticated {ML} models.},
	pages = {1832--1844},
	number = {10},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Li, Yifan and Yu, Xiaohui and Koudas, Nick},
	urldate = {2022-06-16},
	date = {2021-06},
	langid = {english},
}

@inproceedings{howard_searching_2019,
	location = {Seoul, Korea},
	title = {Searching for {MobileNetV}3},
	url = {https://ieeexplore.ieee.org/document/9008835/},
	doi = {10.1109/ICCV.2019.00140},
	abstract = {We present the next generation of {MobileNets} based on a combination of complementary search techniques as well as a novel architecture design. {MobileNetV}3 is tuned to mobile phone {CPUs} through a combination of hardwareaware network architecture search ({NAS}) complemented by the {NetAdapt} algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new {MobileNet} models for release: {MobileNetV}3-Large and {MobileNetV}3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efﬁcient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling ({LR}-{ASPP}). We achieve new state of the art results for mobile classiﬁcation, detection and segmentation. {MobileNetV}3-Large is 3.2\% more accurate on {ImageNet} classiﬁcation while reducing latency by 20\% compared to {MobileNetV}2. {MobileNetV}3-Small is 6.6\% more accurate compared to a {MobileNetV}2 model with comparable latency. {MobileNetV}3-Large detection is over 25\% faster at roughly the same accuracy as {MobileNetV}2 on {COCO} detection. {MobileNetV}3-Large {LRASPP} is 34\% faster than {MobileNetV}2 R-{ASPP} at similar accuracy for Cityscapes segmentation.},
	eventtitle = {{ICCV} '19},
	pages = {1314--1324},
	booktitle = {{IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	date = {2019-10},
}

@article{shamir_how_1979,
	title = {How to Share a Secret},
	volume = {22},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/359168.359176},
	doi = {10.1145/359168.359176},
	abstract = {In this paper we show how to divide data D into n pieces in such a way that D is easily reconstructable from any k pieces, but even complete knowledge of k - 1 pieces reveals absolutely no information about D. This technique enables the construction of robust key management schemes for cryptographic systems that can function securely and reliably even when misfortunes destroy half the pieces and security breaches expose all but one of the remaining pieces.},
	pages = {612--613},
	number = {11},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Shamir, Adi},
	date = {1979-11},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
	keywords = {cryptography, interpolation, key management},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, {AlphaFold}, in the challenging 14th Critical Assessment of protein Structure Prediction ({CASP}14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of {AlphaFold} is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	pages = {583--589},
	number = {7873},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	date = {2021-08-01},
}

@article{fernandez_data_2020,
	title = {Data Market Platforms: Trading Data Assets to Solve Data Problems},
	volume = {13},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3407790.3407800},
	doi = {10.14778/3407790.3407800},
	abstract = {Data only generates value for a few organizations with expertise and resources to make data shareable, discoverable, and easy to integrate. Sharing data that is easy to discover and integrate is hard because data owners lack information (who needs what data) and they do not have incentives to prepare the data in a way that is easy to consume by others.In this paper, we propose data market platforms to address the lack of information and incentives and tackle the problems of data sharing, discovery, and integration. In a data market platform, data owners want to share data because they will be rewarded if they do so. Consumers are encouraged to share their data needs because the market will solve the discovery and integration problem for them in exchange for some form of currency.We consider internal markets that operate within organizations to bring down data silos, as well as external markets that operate across organizations to increase the value of data for everybody. We outline a research agenda that revolves around two problems. The problem of market design, or how to design rules that lead to desired outcomes, and the systems problem, how to implement the market and enforce the rules. Treating data as a first-class asset is sorely needed to extend the value of data to more organizations, and we propose data market platforms as one mechanism to achieve this goal.},
	pages = {1933--1947},
	number = {12},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Fernandez, Raul Castro and Subramaniam, Pranav and Franklin, Michael J.},
	date = {2020-07},
	note = {Publisher: {VLDB} Endowment},
}

@online{idc_total_2021,
	title = {Total Data Volume Worldwide 2010-2025},
	url = {https://www.statista.com/statistics/871513/worldwide-data-created/},
	abstract = {The total amount of data created, captured, copied, and consumed globally is forecast to increase rapidly, reaching 64.2 zettabytes in 2020.},
	titleaddon = {Statista},
	author = {{IDC} and {Statista}},
	urldate = {2022-06-11},
	date = {2021},
	langid = {english},
}

@misc{keskar_large-batch_2017,
	title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
	url = {http://arxiv.org/abs/1609.04836},
	shorttitle = {On Large-Batch Training for Deep Learning},
	abstract = {The stochastic gradient descent ({SGD}) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say 32–512 data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions—and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to ﬂat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
	number = {{arXiv}:1609.04836},
	publisher = {{arXiv}},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	urldate = {2022-06-10},
	date = {2017-02-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1609.04836 [cs, math]},
	note = {Number: {arXiv}:1609.04836},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@inproceedings{charles_large-cohort_2021,
	title = {On Large-Cohort Training for Federated Learning},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/file/ab9ebd57177b5106ad7879f0896685d4-Paper.pdf},
	abstract = {Federated learning methods typically learn a model by iteratively sampling updates from a population of clients. In this work, we explore how the number of clients sampled at each round (the cohort size) impacts the quality of the learned model and the training dynamics of federated learning algorithms. Our work poses three fundamental questions. First, what challenges arise when trying to scale federated learning to larger cohorts? Second, what parallels exist between cohort sizes in federated learning and batch sizes in centralized learning? Last, how can we design federated learning methods that eﬀectively utilize larger cohort sizes? We give partial answers to these questions based on extensive empirical evaluation. Our work highlights a number of challenges stemming from the use of larger cohorts. While some of these (such as generalization issues and diminishing returns) are analogs of large-batch training challenges, others (including training failures and fairness concerns) are unique to federated learning.},
	eventtitle = {{NeurIPS} '21},
	pages = {20461--20475},
	booktitle = {35th Conference on Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Charles, Zachary and Garrett, Zachary and Huo, Zhouyuan and Shmulyian, Sergei and Smith, Virginia},
	date = {2021},
}

@inproceedings{nguyen_federated_2022,
	location = {Valencia, Spain},
	title = {Federated Learning with Buffered Asynchronous Aggregation},
	volume = {151},
	url = {https://proceedings.mlr.press/v151/nguyen22b.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {Scalability and privacy are two critical concerns for cross-device federated learning ({FL}) systems. In this work, we identify that synchronous {FL} – cannot scale efficiently beyond a few hundred clients training in parallel. It leads to diminishing returns in model performance and training speed, analogous to large-batch training. On the other hand, asynchronous aggregation of client updates in {FL} (i.e., asynchronous {FL}) alleviates the scalability issue. However, aggregating individual client updates is incompatible with Secure Aggregation, which could result in an undesirable level of privacy for the system. To address these concerns, we propose a novel buffered asynchronous aggregation method, {FedBuff}, that is agnostic to the choice of optimizer, and combines the best properties of synchronous and asynchronous {FL}. We empirically demonstrate that {FedBuff} is \$3.3{\textbackslash}times\$ more efficient than synchronous {FL} and up to \$2.5{\textbackslash}times\$ more efficient than asynchronous {FL}, while being compatible with privacy-preserving technologies such as Secure Aggregation and differential privacy. We provide theoretical convergence guarantees in a smooth non-convex setting. Finally, we show that under differentially private training, {FedBuff} can outperform {FedAvgM} at low privacy settings and achieve the same utility for higher privacy settings.},
	eventtitle = {{AISTATS} '22},
	pages = {3581--3607},
	booktitle = {25th International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Nguyen, John and Malik, Kshitiz and Zhan, Hongyuan and Yousefpour, Ashkan and Rabbat, Mike and Malek, Mani and Huba, Dzmitry},
	date = {2022-03-28},
}

@inproceedings{huba_papaya_2022,
	location = {Santa Clara, {CA}},
	title = {{PAPAYA}: Practical, Private, and Scalable Federated Learning},
	volume = {4},
	url = {https://proceedings.mlsys.org/paper/2022/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf},
	eventtitle = {{MLSys} '22},
	pages = {814--832},
	booktitle = {5th {MLSys} Conference},
	author = {Huba, Dzmitry and Nguyen, John and Malik, Kshitiz and Zhu, Ruiyu and Rabbat, Mike and Yousefpour, Ashkan and Wu, Carole-Jean and Zhan, Hongyuan and Ustinov, Pavel and Srinivas, Harish and Wang, Kaikai and Shoumikhin, Anthony and Min, Jesik and Malek, Mani},
	date = {2022},
}

@report{ocean_protocol_foundation_ocean_2022,
	title = {Ocean Protocol: Tools for the Web3 Data Economy},
	url = {https://oceanprotocol.com/tech-whitepaper.pdf},
	abstract = {Ocean Protocol is an on-ramp for data services into crypto ecosystems, usingdata {NFTs} and datatokens. Data {NFTs} are like master tapes, and datatokensare like {CDs}: base {IP} and licenses respectively. A data {NFT} is a non-fungible {ERC}721 token representing copyright of a data service; a datatoken is a fungible {ERC}20 token to access the service. Ocean smart contracts and libraries make it easy to publish data services (deploy and mint data {NFTs} \& datatokens) and consume data services (spend datatokens). Ocean contracts are deployed to Ethereum mainnet and other {EVM} networks. Ethereum composability enables crypto wallets as data wallets, crypto exchanges as data marketplaces, {DAOs} as data coops, datatoken-backed stable assets, and more. Ocean Market is an open-source community marketplace for data. It supports automatic determination of price using an “automated market maker” ({AMM}). Each datatoken has its own {AMM} pool. Anyone can add liquidity, akastake (equivalent in {AMMs}). This is curation, as stake is a proxy to dataset quality. Staking is one-sided to eliminate rug pulls. Publishing amounts to an “initial data offering” ({IDO}). We envision thousands of data marketplaces, where Ocean Market is just one. In addition to Ocean Market being open-source (and therefore forkable), Ocean includes tools to help developers build their own marketplaces and other apps. Ocean’s “Compute-to-Data” feature gives compute access onprivately held data, which never leaves the data owner’s premises. Ocean-based marketplaces enable monetization of private data while preserving privacy. These tools are part of a system designed for long-term growth of a permissionless Web3 Data Economy. The Ocean Data Farming program incentivizes a supply of data. The community-driven {OceanDAO} funds software development, outreach, and more. Token dynamics are designed such that {OCEAN} health rises with usage volume.},
	type = {Technical Whitepaper},
	author = {{Ocean Protocol Foundation} and {BigchainDB GmbH}},
	urldate = {2022-06-02},
	date = {2022-05-05},
}

@inproceedings{kang_neurosurgeon_2017,
	location = {Xi'an, China},
	title = {Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge},
	isbn = {978-1-4503-4465-4},
	url = {https://dl.acm.org/doi/10.1145/3037697.3037698},
	doi = {10.1145/3037697.3037698},
	shorttitle = {Neurosurgeon},
	abstract = {The computation for today’s intelligent personal assistants such as Apple Siri, Google Now, and Microsoft Cortana, is performed in the cloud. This cloud-only approach requires signiﬁcant amounts of data to be sent to the cloud over the wireless network and puts signiﬁcant computational pressure on the datacenter. However, as the computational resources in mobile devices become more powerful and energy efﬁcient, questions arise as to whether this cloud-only processing is desirable moving forward, and what are the implications of pushing some or all of this compute to the mobile devices on the edge.},
	eventtitle = {{ASPLOS} '17},
	pages = {615--629},
	booktitle = {22nd International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {{ACM}},
	author = {Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
	urldate = {2022-06-01},
	date = {2017-04-04},
	langid = {english},
}

@inproceedings{palkar_weld_2017,
	location = {Chaminade, {CA}},
	title = {Weld: A Common Runtime for High Performance Data Analytics},
	abstract = {Modern analytics applications combine multiple functions from different libraries and frameworks to build increasingly complex workﬂows. Even though each function may achieve high performance in isolation, the performance of the combined workﬂow is often an order of magnitude below hardware limits due to extensive data movement across the functions. To address this problem, we propose Weld, a runtime for data-intensive applications that optimizes across disjoint libraries and functions. Weld uses a common intermediate representation to capture the structure of diverse dataparallel workloads, including {SQL}, machine learning and graph analytics. It then performs key data movement optimizations and generates efﬁcient parallel code for the whole workﬂow. Weld can be integrated incrementally into existing frameworks like {TensorFlow}, Apache Spark, {NumPy} and Pandas without changing their user-facing {APIs}. We show that Weld can speed up these frameworks, as well as applications that combine them, by up to 30×.},
	eventtitle = {{CIDR} '17},
	pages = {8},
	booktitle = {8th Biennial Conference on Innovative Data Systems Research},
	author = {Palkar, Shoumik and Thomas, James J and Shanbhag, Anil and Narayanan, Deepak and Pirk, Holger and Schwarzkopf, Malte and Amarasinghe, Saman and Zaharia, Matei},
	date = {2017},
	langid = {english},
}

@online{snowflake_snowflake_2022,
	title = {Snowflake Data Cloud},
	url = {https://www.snowflake.com/},
	abstract = {Join the Data Cloud to unify multiple data workloads, easily discover \& securely share live governed data, democratize data analytics \& more.},
	author = {{Snowflake}},
	urldate = {2022-05-18},
	date = {2022},
	langid = {american},
}

@article{dean_mapreduce_2008,
	title = {{MapReduce}: Simplified Data Processing on Large Clusters},
	volume = {51},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1327452.1327492},
	doi = {10.1145/1327452.1327492},
	shorttitle = {{MapReduce}},
	abstract = {{MapReduce} is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks.},
	pages = {107--113},
	number = {1},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Dean, Jeffrey and Ghemawat, Sanjay},
	urldate = {2022-05-18},
	date = {2008-01},
	langid = {english},
}

@online{apache_software_foundation_apache_2011,
	title = {Apache Atlas – Data Governance and Metadata Framework for Hadoop},
	url = {https://atlas.apache.org},
	author = {{Apache Software Foundation}},
	urldate = {2022-05-18},
	date = {2011},
}

@online{majumdar_metacat_2018,
	title = {Metacat: Making Big Data Discoverable and Meaningful at Netflix},
	url = {https://netflixtechblog.com/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520},
	shorttitle = {Metacat},
	abstract = {Most large companies have numerous data sources with different data formats and large data volumes. These data stores are accessed and analyzed by many people throughout the enterprise. At Netflix, our data warehouse consists of a large number of data sets stored in Amazon S3 (via Hive), Druid, Elasticsearch, Redshift, Snowflake and {MySql}. Our platform supports Spark, Presto, Pig, and Hive for consuming, processing and producing data sets. Given the diverse set of data sources, and to make sure our data platform can interoperate across these data sets as one “single” data warehouse, we built Metacat. In this blog, we will discuss our motivations in building Metacat, a metadata service to make data easy to discover, process and manage.},
	titleaddon = {Netflix Technology Blog},
	author = {Majumdar, Ajoy and Li, Zhen},
	urldate = {2022-05-18},
	date = {2018-06-14},
	langid = {english},
}

@incollection{haines_workflow_2022,
	location = {Berkeley, {CA}},
	title = {Workflow Orchestration with Apache Airflow},
	isbn = {978-1-4842-7452-1},
	url = {https://doi.org/10.1007/978-1-4842-7452-1_8},
	abstract = {Generally speaking, there are two kinds of problems you'll find yourself running into more often than not as a data engineer. The first stems from broken promises, aka bad upstream data sources, and the more general realm of the unknown unknowns with respect to data movement through your data pipelines. The second problem you'll find yourself up against is time. This is not the part in the book where I start to talk to you about life, death, and decision making, but rather time as a boundary or a threshold. Time exists between the physical runtime of jobs, as well as a very real line in the sand when it relates to data service level agreements ({SLAs}). These data contracts revolve around expectations in terms of the data format (aka schemas) as well as the agreed upon time when data should be expected to become available. Another way in which time gets the best of us is at the intersection of both of these common problems, e.g., upstream problems married happily with stale data, or missed {SLAs}.},
	pages = {255--295},
	booktitle = {Modern Data Engineering with Apache Spark: A Hands-On Guide for Building Mission-Critical Streaming Applications},
	publisher = {Apress},
	author = {Haines, Scott},
	date = {2022},
	doi = {10.1007/978-1-4842-7452-1_8},
}

@article{duggan_bigdawg_2015,
	title = {The {BigDAWG} Polystore System},
	volume = {44},
	issn = {0163-5808},
	url = {https://doi.org/10.1145/2814710.2814713},
	doi = {10.1145/2814710.2814713},
	abstract = {This paper presents a new view of federated databases to address the growing need for managing information that spans multiple data models. This trend is fueled by the proliferation of storage engines and query languages based on the observation that 'no one size fits all'. To address this shift, we propose a polystore architecture; it is designed to unify querying over multiple data models. We consider the challenges and opportunities associated with polystores. Open questions in this space revolve around query optimization and the assignment of objects to storage engines. We introduce our approach to these topics and discuss our prototype in the context of the Intel Science and Technology Center for Big Data},
	pages = {11--16},
	number = {2},
	journaltitle = {{SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Duggan, Jennie and Elmore, Aaron J. and Stonebraker, Michael and Balazinska, Magda and Howe, Bill and Kepner, Jeremy and Madden, Sam and Maier, David and Mattson, Tim and Zdonik, Stan},
	date = {2015-08},
}

@article{melnik_dremel_2010,
	title = {Dremel: Interactive Analysis of Web-Scale Datasets},
	volume = {3},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/1920841.1920886},
	doi = {10.14778/1920841.1920886},
	abstract = {Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of {CPUs} and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements {MapReduce}-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.},
	pages = {330--339},
	number = {1},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Melnik, Sergey and Gubarev, Andrey and Long, Jing Jing and Romer, Geoffrey and Shivakumar, Shiva and Tolton, Matt and Vassilakis, Theo},
	date = {2010-09},
	note = {Publisher: {VLDB} Endowment},
}

@online{lan_datahub_2019,
	title = {{DataHub}: A Generalized Metadata Search \& Discovery Tool},
	url = {https://engineering.linkedin.com/blog/2019/data-hub},
	shorttitle = {{DataHub}},
	abstract = {As the operator of the world’s largest professional network and the Economic Graph, {LinkedIn}’s Data team is constantly working on scaling its infrastructure to meet the demands of our ever-growing big data ecosystem. As the data grows in volume and richness, it becomes increasingly challenging for data scientists and engineers to discover the data assets available, understand their provenances, and take appropriate actions based on the insights. To help us continue scaling productivity and innovation in data alongside this growth, we created a generalized metadata search and discovery tool, {DataHub}.},
	titleaddon = {{LinkedIn} Engineering},
	author = {Lan, Mars and Adebajo, Seyi and Das, Shirshanka},
	urldate = {2022-05-18},
	date = {2019-08-14},
	langid = {english},
}

@inproceedings{carey_towards_1995,
	title = {Towards Heterogeneous Multimedia Information Systems: The Garlic Approach},
	doi = {10.1109/RIDE.1995.378736},
	shorttitle = {Towards heterogeneous multimedia information systems},
	abstract = {Provides an overview of the Garlic project, a new project at the {IBM} Almaden Research Center. The goal of this project is to develop a system and associated tools for the management of large quantities of heterogeneous multimedia information. Garlic permits traditional and multimedia data to he stored in a variety of existing data repositories, including databases, files, text managers, image managers, video servers, and so on; the data is seen through a unified schema expressed in an object-oriented data model and can be queried and manipulated using an object-oriented dialect of {SQL}, perhaps through an advanced query/browser tool that we are also developing. The Garlic architecture is designed to be extensible to new kinds of data repositories, and access efficiency is addressed via a "middleware" query processor that uses database query optimization techniques to exploit the native associative search capabilities of the underlying data repositories.{\textless}{\textgreater}},
	eventtitle = {{RIDE}-{DOM} '95},
	pages = {124--131},
	booktitle = {5th International Workshop on Research Issues in Data Engineering-Distributed Object Management},
	author = {Carey, Michael J. and Haas, Laura M. and Schwarz, Peter M. and Arya, M. and Cody, W.F. and Fagin, Ronald and Flickner, Myron D. and Luniewski, A.W. and Niblack, W. and Petkovic, Dragutin and Thomas, J. and Williams, J.H. and Wimmers, E.L.},
	date = {1995-03},
	keywords = {Data models, File servers, Image databases, Information systems, Management information systems, Multimedia databases, Multimedia systems, Object oriented databases, Project management, Query processing},
}

@inproceedings{gog_musketeer_2015,
	location = {Bordeaux, France},
	title = {Musketeer: All for One, One for All in Data Processing Systems},
	isbn = {978-1-4503-3238-5},
	url = {https://dl.acm.org/doi/10.1145/2741948.2741968},
	doi = {10.1145/2741948.2741968},
	shorttitle = {Musketeer},
	abstract = {Many systems for the parallel processing of big data are available today. Yet, few users can tell by intuition which system, or combination of systems, is “best” for a given workﬂow. Porting workﬂows between systems is tedious. Hence, users become “locked in”, despite faster or more efﬁcient systems being available. This is a direct consequence of the tight coupling between user-facing front-ends that express workﬂows (e.g., Hive, {SparkSQL}, Lindi, {GraphLINQ}) and the back-end execution engines that run them (e.g., {MapReduce}, Spark, {PowerGraph}, Naiad).},
	eventtitle = {{EuroSys} '15},
	pages = {1--16},
	booktitle = {10th European Conference on Computer Systems},
	publisher = {{ACM}},
	author = {Gog, Ionel and Schwarzkopf, Malte and Crooks, Natacha and Grosvenor, Matthew P. and Clement, Allen and Hand, Steven},
	urldate = {2022-05-18},
	date = {2015-04-17},
	langid = {english},
}

@article{hellerstein_architecture_2007,
	title = {Architecture of a Database System},
	volume = {1},
	issn = {1931-7883, 1931-7891},
	url = {http://www.nowpublishers.com/article/Details/DBS-002},
	doi = {10.1561/1900000002},
	abstract = {Database Management Systems ({DBMSs}) are a ubiquitous and critical component of modern computing, and the result of decades of research and development in both academia and industry. Historically, {DBMSs} were among the earliest multi-user server systems to be developed, and thus pioneered many systems design techniques for scalability and reliability now in use in many other contexts. While many of the algorithms and abstractions used by a {DBMS} are textbook material, there has been relatively sparse coverage in the literature of the systems design issues that make a {DBMS} work. This paper presents an architectural discussion of {DBMS} design principles, including process models, parallel architecture, storage system design, transaction system implementation, query processor and optimizer architectures, and typical shared components and utilities. Successful commercial and open-source systems are used as points of reference, particularly when multiple alternative designs have been adopted by diﬀerent groups.},
	pages = {141--259},
	number = {2},
	journaltitle = {Foundations and Trends® in Databases},
	shortjournal = {{FNT} in Databases},
	author = {Hellerstein, Joseph M. and Stonebraker, Michael and Hamilton, James},
	urldate = {2020-08-26},
	date = {2007},
	langid = {english},
}

@article{abadi_beckman_2016,
	title = {The Beckman Report on Database Research},
	volume = {59},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2845915},
	doi = {10.1145/2845915},
	abstract = {Database researchers paint big data as a defining challenge. To make the most of the enormous opportunities at hand will require focusing on five research areas.},
	pages = {92--99},
	number = {2},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Abadi, Daniel and Agrawal, Rakesh and Ailamaki, Anastasia and Balazinska, Magdalena and Bernstein, Philip A. and Carey, Michael J. and Chaudhuri, Surajit and Dean, Jeffrey and Doan, {AnHai} and Franklin, Michael J. and Gehrke, Johannes and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Jagadish, H. V. and Kossmann, Donald and Madden, Samuel and Mehrotra, Sharad and Milo, Tova and Naughton, Jeffrey F. and Ramakrishnan, Raghu and Markl, Volker and Olston, Christopher and Ooi, Beng Chin and Ré, Christopher and Suciu, Dan and Stonebraker, Michael and Walter, Todd and Widom, Jennifer},
	urldate = {2022-05-18},
	date = {2016-01-25},
	langid = {english},
}

@inproceedings{chawathe_tsimmis_1994,
	location = {Tokyo, Japan},
	title = {The {TSIMMIS} Project: Integration of Heterogenous Information Sources},
	url = {http://ilpubs.stanford.edu:8090/66/},
	abstract = {The goal of the Tsimmis Project is to develop tools that facilitate the rapid integration of heterogeneous information sources that may include both structured and unstructured data. This paper gives an overview of the project, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites. Tsimmis is a joint project between Stanford and the {IBM} Almaden Research Center. 1 Overview A common problem facing many organizations today is that of multiple, disparate information sources and repositories, including databases, object stores, knowledge bases, file systems, digital libraries, information retrieval systems, and electronic mail systems. Decision makers often need information from multiple sources, but are unable to get and fuse the required information in a timely fashion due to the diffculties of accessing the different systems, and due to the fact that the information obtained can be inconsistent and contradictory. Research sponsored by the Wright Laboratory, Aeronautical Systems Center, Air Force Material Command, {USAF}, under Grant Number F33615-93-1-1339. The {US} Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the offcial policies or endorsements, either express or implied, of Wright Laboratory or the {US} Government. This work was also supported by the Reid and Polly Anderson Faculty Scholar Fund, the Center for Integrated Systems at Stanford University, and by Equipment Grants from Digital Equipment Corporation and {IBM} Corporation. The goal of the {TSIMMIS} 1 project is to provide tools for accessing, in an integrated fashion, multiple informati},
	eventtitle = {{IPSJ} '94},
	pages = {12},
	booktitle = {Information Processing Society of Japan},
	author = {Chawathe, Sudarshan and Garcia-Molina, Hector and Hammer, Joachim and Ireland, Kelly and Papakonstantinou, Yannis and Ullman, Jeffrey and Widom, Jennifer},
	date = {1994},
	keywords = {Heterogeneous databases, Object Exchange Model, integration},
}

@inproceedings{wang_myria_2017,
	location = {Chaminade, {CA}},
	title = {The Myria Big Data Management and Analytics System and Cloud Service},
	abstract = {In this paper, we present an overview of the Myria stack for big data management and analytics that we developed in the database group at the University of Washington and that we have been operating as a cloud service aimed at domain scientists around the {UW} campus. We highlight Myria’s key design choices and innovations and report on our experience with using Myria for various data science use-cases.},
	eventtitle = {{CIDR} '17},
	pages = {11},
	booktitle = {8th Biennial Conference on Innovative Data Systems Research},
	author = {Wang, Jingjing and Baker, Tobin and Balazinska, Magdalena and Halperin, Daniel and Haynes, Brandon and Howe, Bill and Hutchison, Dylan and Jain, Shrainik and Maas, Ryan and Mehta, Parmita and Myers, Brandon and Ortiz, Jennifer and Suciu, Dan and Whitaker, Andrew and Xu, Shengliang},
	date = {2017},
	langid = {english},
}

@inproceedings{hagedorn_putting_2021,
	location = {Chaminade, {CA}},
	title = {Putting Pandas in a Box},
	abstract = {Pandas – the Python Data Analysis Library – is a powerful and widely used framework for data analytics. In this work we present our approach to push down the computational part of Pandas scripts into the {DBMS} by using a transpiler. In addition to basic data processing operations, our approach also supports access to external data stored in ﬁles instead of the {DBMS}. Moreover, user-deﬁned Python functions are transformed automatically to {SQL} {UDFs} executed in the {DBMS}. The latter allows the integration of complex computational tasks including machine learning. We show the usage of this feature to implement a so-called model join, i.e. applying pre-trained {ML} models to data in {SQL} tables.},
	eventtitle = {{CIDR} '21},
	pages = {6},
	author = {Hagedorn, Stefan and Kläbe, Steffen and Sattler, Kai-Uwe},
	date = {2021-01-13},
	langid = {english},
}

@book{ilyas_data_2019,
	location = {New York, {NY}, {USA}},
	title = {Data Cleaning},
	isbn = {978-1-4503-7152-0},
	abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.},
	publisher = {Association for Computing Machinery},
	author = {Ilyas, Ihab F. and Chu, Xu},
	date = {2019},
}

@inproceedings{bagdasaryan_how_2020,
	location = {Palermo, Italy},
	title = {How To Backdoor Federated Learning},
	volume = {108},
	url = {https://proceedings.mlr.press/v108/bagdasaryan20a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {Federated models are created by aggregating model updates submittedby participants. To protect confidentiality of the training data,the aggregator by design has no visibility into how these updates aregenerated. We show that this makes federated learning vulnerable to amodel-poisoning attack that is significantly more powerful than poisoningattacks that target only the training data.A single or multiple malicious participants can use modelreplacement to introduce backdoor functionality into the joint model,e.g., modify an image classifier so that it assigns an attacker-chosenlabel to images with certain features, or force a word predictor tocomplete certain sentences with an attacker-chosen word. We evaluatemodel replacement under different assumptions for the standardfederated-learning tasks and show that it greatly outperformstraining-data poisoning.Federated learning employs secure aggregation to protect confidentialityof participants’ local models and thus cannot detect anomalies inparticipants’ contributions to the joint model. To demonstrate thatanomaly detection would not have been effective in any case, we alsodevelop and evaluate a generic constrain-and-scale technique thatincorporates the evasion of defenses into the attacker’s loss functionduring training.},
	eventtitle = {{AISTATS} '20},
	pages = {2938--2948},
	booktitle = {23rd International Conference on Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	date = {2020-08-26},
}

@article{fagin_composing_2005,
	title = {Composing Schema Mappings: Second-Order Dependencies to the Rescue},
	volume = {30},
	issn = {0362-5915},
	url = {https://doi.org/10.1145/1114244.1114249},
	doi = {10.1145/1114244.1114249},
	abstract = {A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). A fundamental problem is composing schema mappings: given two successive schema mappings, derive a schema mapping between the source schema of the first and the target schema of the second that has the same effect as applying successively the two schema mappings.In this article, we give a rigorous semantics to the composition of schema mappings and investigate the definability and computational complexity of the composition of two schema mappings. We first study the important case of schema mappings in which the specification is given by a finite set of source-to-target tuple-generating dependencies (source-to-target tgds). We show that the composition of a finite set of full source-to-target tgds with a finite set of tgds is always definable by a finite set of source-to-target tgds, but the composition of a finite set of source-to-target tgds with a finite set of full source-to-target tgds may not be definable by any set (finite or infinite) of source-to-target tgds; furthermore, it may not be definable by any formula of least fixed-point logic, and the associated composition query may be {NP}-complete. After this, we introduce a class of existential second-order formulas with function symbols and equalities, which we call second-order tgds, and make a case that they are the “right” language for composing schema mappings. Specifically, we show that second-order tgds form the smallest class (up to logical equivalence) that contains every source-to-target tgd and is closed under conjunction and composition. Allowing equalities in second-order tgds turns out to be of the essence, even though the “obvious” way to define second-order tgds does not require equalities. We show that second-order tgds without equalities are not sufficiently expressive to define the composition of finite sets of source-to-target tgds. Finally, we show that second-order tgds possess good properties for data exchange and query answering: the chase procedure can be extended to second-order tgds so that it produces polynomial-time computable universal solutions in data exchange settings specified by second-order tgds.},
	pages = {994--1055},
	number = {4},
	journaltitle = {{ACM} Trans. Database Syst.},
	author = {Fagin, Ronald and Kolaitis, Phokion G. and Popa, Lucian and Tan, Wang-Chiew},
	date = {2005-12},
	note = {Place: New York, {NY}, {USA}
Publisher: Association for Computing Machinery},
	keywords = {Data exchange, certain answers, chase, composition, computational complexity, conjunctive queries, data integration, dependencies, metadata model management, query answering, schema mapping, second-order logic, universal solution},
}

@inproceedings{madhavan_composing_2003,
	location = {Berlin, Germany},
	title = {Composing Mappings Among Data Sources},
	isbn = {0-12-722442-4},
	series = {{VLDB} '03},
	abstract = {Semantic mappings between data sources play a key role in several data sharing architectures. Mappings provide the relationships between data stored in different sources, and therefore enable answering queries that require data from other nodes in a data sharing network. Composing mappings is one of the core problems that lies at the heart of several optimization methods in data sharing networks, such as caching frequently traversed paths and redundancy analysis.This paper investigates the theoretical underpinnings of mapping composition. We study the problem for a rich mapping language, {GLAV}, that combines the advantages of the known mapping formalisms globalas-view and local-as-view. We first show that even when composing two simple {GLAV} mappings, the full composition may be an infinite set of {GLAV} formulas. Second, we show that if we restrict the set of queries to be in {CQk} (a common restriction in practice), then we can always encode the infinite set of {GLAV} formulas using a finite representation. Furthermore, we describe an algorithm that given a query and a finite encoding of an infinite set of {GLAV} formulas, finds all the certain answers to the query. Consequently, we show that for a commonly occuring class of queries it is possible to pre-compose mappings, thereby potentially offering significant savings in query processing.},
	eventtitle = {{VLDB} '03},
	pages = {572--583},
	booktitle = {29th International Conference on Very Large Data Bases},
	publisher = {{VLDB} Endowment},
	author = {Madhavan, Jayant and Halevy, Alon Y.},
	date = {2003},
}

@article{bronstein_geometric_2017,
	title = {Geometric Deep Learning: Going beyond Euclidean data},
	volume = {34},
	issn = {1558-0792},
	doi = {10.1109/MSP.2017.2693418},
	shorttitle = {Geometric Deep Learning},
	abstract = {Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
	pages = {18--42},
	number = {4},
	journaltitle = {{IEEE} Signal Processing Magazine},
	author = {Bronstein, Michael M. and Bruna, Joan and {LeCun}, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	date = {2017-07},
	note = {Conference Name: {IEEE} Signal Processing Magazine},
	keywords = {Computational modeling, Computer architecture, Convolution, Convolutional codes, Euclidean distance, Machine learning, Social network services},
}

@article{jere_taxonomy_2021,
	title = {A Taxonomy of Attacks on Federated Learning},
	volume = {19},
	issn = {1558-4046},
	doi = {10.1109/MSEC.2020.3039941},
	abstract = {Federated learning is a privacy-by-design framework that enables training deep neural networks from decentralized sources of data, but it is fraught with innumerable attack surfaces. We provide a taxonomy of recent attacks on federated learning systems and detail the need for more robust threat modeling in federated learning environments.},
	pages = {20--28},
	number = {2},
	journaltitle = {{IEEE} Security Privacy},
	author = {Jere, Malhar S. and Farnan, Tyler and Koushanfar, Farinaz},
	date = {2021-03},
	note = {Conference Name: {IEEE} Security Privacy},
	keywords = {Collaborative work, Computational modeling, Data models, Data privacy, Security, Servers, Training data},
}

@inproceedings{kong_consensus_2021,
	title = {Consensus Control for Decentralized Deep Learning},
	url = {https://proceedings.mlr.press/v139/kong21a.html},
	abstract = {Decentralized training of deep learning models enables on-device learning over networks, as well as efficient scaling to large compute clusters. Experiments in earlier works reveal that, even in a data-center setup, decentralized training often suffers from the degradation in the quality of the model: the training and test performance of models trained in a decentralized fashion is in general worse than that of models trained in a centralized fashion, and this performance drop is impacted by parameters such as network size, communication topology and data partitioning. We identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training. We show in theory that when the training consensus distance is lower than a critical quantity, decentralized training converges as fast as the centralized counterpart. We empirically validate that the relation between generalization performance and consensus distance is consistent with this theoretical observation. Our empirical insights allow the principled design of better decentralized training schemes that mitigate the performance drop. To this end, we provide practical training guidelines and exemplify its effectiveness on the data-center setup as the important first step.},
	eventtitle = {{ICML} '21},
	pages = {5686--5696},
	booktitle = {38th International Conference on Machine Learning},
	author = {Kong, Lingjing and Lin, Tao and Koloskova, Anastasia and Jaggi, Martin and Stich, Sebastian},
	date = {2021-07-18},
}

@article{sheth_federated_1990,
	title = {Federated Database Systems for Managing Distributed, Heterogeneous, and Autonomous Databases},
	volume = {22},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/96602.96604},
	doi = {10.1145/96602.96604},
	abstract = {A federated database system ({FDBS}) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various {FDBS} architectures can be developed. We then define a methodology for developing one of the popular architectures of an {FDBS}. Finally, we discuss critical issues related to developing and operating an {FDBS}.},
	pages = {183--236},
	number = {3},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Sheth, Amit P. and Larson, James A.},
	urldate = {2022-01-13},
	date = {1990-09},
	langid = {english},
}

@online{european_commision_general_2018,
	title = {General Data Protection Regulation},
	url = {https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=celex%3A32016R0679},
	author = {{European Commision}},
	urldate = {2021-10-21},
	date = {2018-05-25},
	langid = {english},
	note = {Long Title: Regulation ({EU}) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/{EC}},
}

@inproceedings{xian_assisted_2020,
	title = {Assisted Learning: A Framework for Multi-Organization Learning},
	url = {https://proceedings.neurips.cc/paper/2020/file/a7b23e6eefbe6cf04b8e62a6f0915550-Paper.pdf},
	abstract = {In an increasing number of {AI} scenarios, collaborations among different organizations or agents (e.g., human and robots, mobile units) are often essential to accomplish an organization-speciﬁc mission. However, to avoid leaking useful and possibly proprietary information, organizations typically enforce stringent security constraints on sharing modeling algorithms and data, which signiﬁcantly limits collaborations. In this work, we introduce the Assisted Learning framework for organizations to assist each other in supervised learning tasks without revealing any organization’s algorithm, data, or even task. An organization seeks assistance by broadcasting task-speciﬁc but nonsensitive statistics and incorporating others’ feedback in one or more iterations to eventually improve its predictive performance. Theoretical and experimental studies, including real-world medical benchmarks, show that Assisted Learning can often achieve near-oracle learning performance as if data and training processes were centralized.},
	eventtitle = {{NeurIPS} '20},
	pages = {14580--14591},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Xian, Xun and Wang, Xinran and Ding, Jie and Ghanadan, Reza},
	date = {2020},
}

@inproceedings{kairouz_practical_2021,
	title = {Practical and Private (Deep) Learning Without Sampling or Shuffling},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/kairouz21b.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {We consider training models with differential privacy ({DP}) using mini-batch gradients. The existing state-of-the-art, Differentially Private Stochastic Gradient Descent ({DP}-{SGD}), requires {\textbackslash}emphprivacy amplification by sampling or shuffling to obtain the best privacy/accuracy/computation trade-offs. Unfortunately, the precise requirements on exact sampling and shuffling can be hard to obtain in important practical scenarios, particularly federated learning ({FL}). We design and analyze a {DP} variant of Follow-The-Regularized-Leader ({DP}-{FTRL}) that compares favorably (both theoretically and empirically) to amplified {DP}-{SGD}, while allowing for much more flexible data access patterns. {DP}-{FTRL} does not use any form of privacy amplification.},
	pages = {5213--5225},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kairouz, Peter and Mcmahan, Brendan and Song, Shuang and Thakkar, Om and Thakurta, Abhradeep and Xu, Zheng},
	date = {2021-07-18},
}

@inproceedings{balle_privacy_2020,
	title = {Privacy Amplification via Random Check-Ins},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/313f422ac583444ba6045cd122653b0e-Paper.pdf},
	abstract = {Differentially Private Stochastic Gradient Descent ({DP}-{SGD}) forms a fundamental building block in many applications for learning over sensitive data. Two standard approaches, privacy ampliﬁcation by subsampling, and privacy ampliﬁcation by shufﬂing, permit adding lower noise in {DP}-{SGD} than via na¨ıve schemes. A key assumption in both these approaches is that the elements in the data set can be uniformly sampled, or be uniformly permuted — constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion. In this paper, we focus on conducting iterative methods like {DP}-{SGD} in the setting of federated learning ({FL}) wherein the data is distributed among many devices (clients). Our main contribution is the random check-in distributed protocol, which crucially relies only on randomized participation decisions made locally and independently by each client. It has privacy/accuracy trade-offs similar to privacy ampliﬁcation by subsampling/shufﬂing. However, our method does not require server-initiated communication, or even knowledge of the population size. To our knowledge, this is the ﬁrst privacy ampliﬁcation tailored for a distributed learning framework, and it may have broader applicability beyond {FL}. Along the way, we improve the privacy guarantees of ampliﬁcation by shufﬂing and show that, in practical regimes, this improvement allows for similar privacy and utility using data from an order of magnitude fewer users.},
	pages = {4623--4634},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Balle, Borja and Kairouz, Peter and {McMahan}, Brendan and Thakkar, Om and Guha Thakurta, Abhradeep},
	date = {2020},
}

@inproceedings{narayanan_robust_2008,
	title = {Robust De-anonymization of Large Sparse Datasets},
	doi = {10.1109/SP.2008.33},
	abstract = {We present a new class of statistical de- anonymization attacks against high-dimensional micro-data, such as individual preferences, recommendations, transaction records and so on. Our techniques are robust to perturbation in the data and tolerate some mistakes in the adversary's background knowledge. We apply our de-anonymization methodology to the Netflix Prize dataset, which contains anonymous movie ratings of 500,000 subscribers of Netflix, the world's largest online movie rental service. We demonstrate that an adversary who knows only a little bit about an individual subscriber can easily identify this subscriber's record in the dataset. Using the Internet Movie Database as the source of background knowledge, we successfully identified the Netflix records of known users, uncovering their apparent political preferences and other potentially sensitive information.},
	eventtitle = {{SP} '08},
	pages = {111--125},
	booktitle = {2008 {IEEE} Symposium on Security and Privacy},
	author = {Narayanan, Arvind and Shmatikov, Vitaly},
	date = {2008-05},
	note = {{ISSN}: 2375-1207},
	keywords = {Anonymity, Attack, {DVD}, Data mining, Data privacy, Data security, Internet, Motion pictures, Privacy, Probability, Robustness, Tail, Transaction databases},
}

@article{poepsel-lemaitre_land_2021,
	title = {In the land of data streams where synopses are missing, one framework to bring them all},
	volume = {14},
	issn = {2150-8097},
	url = {https://dl.acm.org/doi/10.14778/3467861.3467871},
	doi = {10.14778/3467861.3467871},
	abstract = {In pursuit of real-time data analysis, approximate summarization structures, i.e., synopses, have gained importance over the years. However, existing stream processing systems, such as Flink, Spark, and Storm, do not support synopses as first class citizens, i.e., as pipeline operators. Synopses’ implementation is upon users. This is mainly because of the diversity of synopses, which makes a unified implementation difficult. We present Condor, a framework that supports synopses as first class citizens. Condor facilitates the specification and processing of synopsis-based streaming jobs while hiding all internal processing details. Condor’s key component is its model that represents synopses as a particular case of windowed aggregate functions. An inherent divide and conquer strategy allows Condor to efficiently distribute the computation, allowing for high-performance and linear scalability. Our evaluation shows that Condor outperforms existing approaches by up to a factor of 75x and that it scales linearly with the number of cores.},
	pages = {1818--1831},
	number = {10},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Poepsel-Lemaitre, Rudi and Kiefer, Martin and von Hein, Joscha and Quiané-Ruiz, Jorge-Arnulfo and Markl, Volker},
	urldate = {2022-02-24},
	date = {2021-06},
	langid = {english},
}

@inproceedings{fung_limitations_2020,
	location = {San Sebastian, Spain},
	title = {The Limitations of Federated Learning in Sybil Settings},
	isbn = {978-1-939133-18-2},
	url = {https://www.usenix.org/conference/raid2020/presentation/fung},
	eventtitle = {{RAID} '20},
	pages = {301--316},
	booktitle = {23rd International Symposium on Research in Attacks, Intrusions and Defenses},
	publisher = {{USENIX} Association},
	author = {Fung, Clement and Yoon, Chris J. M. and Beschastnikh, Ivan},
	date = {2020-10},
}

@article{wei_federated_2020,
	title = {Federated Learning With Differential Privacy: Algorithms and Performance Analysis},
	volume = {15},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2020.2988575},
	shorttitle = {Federated Learning With Differential Privacy},
	abstract = {Federated learning ({FL}), as a type of distributed machine learning, is capable of significantly preserving clients' private data from being exposed to adversaries. Nevertheless, private information can still be divulged by analyzing uploaded parameters from clients, e.g., weights trained in deep neural networks. In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of differential privacy ({DP}), in which artificial noise is added to parameters at the clients' side before aggregating, namely, noising before model aggregation {FL} ({NbAFL}). First, we prove that the {NbAFL} can satisfy {DP} under distinct protection levels by properly adapting different variances of artificial noise. Then we develop a theoretical convergence bound on the loss function of the trained {FL} model in the {NbAFL}. Specifically, the theoretical bound reveals the following three key properties: 1) there is a tradeoff between convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) given a fixed privacy protection level, increasing the number N of overall clients participating in {FL} can improve the convergence performance; and 3) there is an optimal number aggregation times (communication rounds) in terms of convergence performance for a given protection level. Furthermore, we propose a K-client random scheduling strategy, where K (1 ≤ K {\textless}; N) clients are randomly selected from the N overall clients to participate in each aggregation. We also develop a corresponding convergence bound for the loss function in this case and the K-client random scheduling strategy also retains the above three properties. Moreover, we find that there is an optimal K that achieves the best convergence performance at a fixed privacy level. Evaluations demonstrate that our theoretical results are consistent with simulations, thereby facilitating the design of various privacy-preserving {FL} algorithms with different tradeoff requirements on convergence performance and privacy levels.},
	pages = {3454--3469},
	journaltitle = {{IEEE} Transactions on Information Forensics and Security},
	author = {Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H. and Farokhi, Farhad and Jin, Shi and Quek, Tony Q. S. and Poor, H. Vincent},
	date = {2020},
	note = {Conference Name: {IEEE} Transactions on Information Forensics and Security},
	keywords = {Analytical models, Convergence, Distributed databases, Federated learning, Privacy, Servers, Training, client selection, convergence performance, differential privacy, information leakage},
}

@inproceedings{kourtellis_flaas_2020,
	location = {Barcelona, Spain},
	title = {{FLaaS}: Federated Learning as a Service},
	isbn = {978-1-4503-8182-6},
	url = {https://dl.acm.org/doi/10.1145/3426745.3431337},
	doi = {10.1145/3426745.3431337},
	shorttitle = {{FLaaS}},
	abstract = {Federated Learning (F L) is emerging as a promising technology to build machine learning models in a decentralized, privacy-preserving fashion. Indeed, F L enables local training on user devices, avoiding user data to be transferred to centralized servers, and can be enhanced with differential privacy mechanisms. Although F L has been recently deployed in real systems, the possibility of collaborative modeling across different 3rd-party applications has not yet been explored. In this paper, we tackle this problem and present Federated Learning as a Service ({FLaaS}), a system enabling different scenarios of 3rd-party application collaborative model building and addressing the consequent challenges of permission and privacy management, usability, and hierarchical model training. {FLaaS} can be deployed in different operational environments. As a proof of concept, we implement it on a mobile phone setting and discuss practical implications of results on simulated and real devices with respect to on-device training {CPU} cost, memory footprint and power consumed per F L model round. Therefore, we demonstrate {FLaaS}’s feasibility in building unique or joint F L models across applications for image object detection in a few hours, across 100 devices.},
	eventtitle = {{CoNEXT} '20},
	pages = {7--13},
	booktitle = {1st Workshop on Distributed Machine Learning},
	publisher = {{ACM}},
	author = {Kourtellis, Nicolas and Katevas, Kleomenis and Perino, Diego},
	urldate = {2022-02-21},
	date = {2020-12},
	langid = {english},
}

@inproceedings{tuor_overcoming_2021,
	title = {Overcoming Noisy and Irrelevant Data in Federated Learning},
	doi = {10.1109/ICPR48806.2021.9412599},
	abstract = {Many image and vision applications require a large amount of data for model training. Collecting all such data at a central location can be challenging due to data privacy and communication bandwidth restrictions. Federated learning is an effective way of training a machine learning model in a distributed manner from local data collected by client devices, which does not require exchanging the raw data among clients. A challenge is that among the large variety of data collected at each client, it is likely that only a subset is relevant for a learning task while the rest of data has a negative impact on model training. Therefore, before starting the learning process, it is important to select the subset of data that is relevant to the given federated learning task. In this paper, we propose a method for distributedly selecting relevant data, where we use a benchmark model trained on a small benchmark dataset that is task-specific, to evaluate the relevance of individual data samples at each client and select the data with sufficiently high relevance. Then, each client only uses the selected subset of its data in the federated learning process. The effectiveness of our proposed approach is evaluated on multiple real-world image datasets in a simulated system with a large number of clients, showing up to 25\% improvement in model accuracy compared to training with all data.},
	eventtitle = {{ICPR} '21},
	pages = {5020--5027},
	booktitle = {25th International Conference on Pattern Recognition},
	author = {Tuor, Tiffany and Wang, Shiqiang and Ko, Bong Jun and Liu, Changchang and Leung, Kin K.},
	date = {2021-01},
	note = {{ISSN}: 1051-4651},
	keywords = {Benchmark testing, Collaborative work, Data filtering, Data models, Data privacy, Distributed databases, Machine learning, Training, distributed machine learning, federated learning, open set noise},
}

@article{jiang_improving_2019,
	title = {Improving Federated Learning Personalization via Model Agnostic Meta Learning},
	url = {http://arxiv.org/abs/1909.12488},
	abstract = {Federated Learning ({FL}) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning ({MAML}), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for {FL}. We present {FL} as a natural source of practical applications for {MAML} algorithms, and make the following observations. 1) The popular {FL} algorithm, Federated Averaging ({McMahan} et al., 2017), can be interpreted as a meta learning algorithm. 2) Careful ﬁne-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the ﬁrst claim. These results raise new questions for {FL}, {MAML}, and broader {ML} research.},
	journaltitle = {{arXiv}:1909.12488},
	author = {Jiang, Yihan and Konečný, Jakub and Rush, Keith and Kannan, Sreeram},
	urldate = {2022-02-18},
	date = {2019-09-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.12488},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kulkarni_survey_2020,
	title = {Survey of Personalization Techniques for Federated Learning},
	doi = {10.1109/WorldS450073.2020.9210355},
	abstract = {Federated learning enables machine learning models to learn from private decentralized data without compromising privacy. The standard formulation of federated learning produces one shared model for all clients. Statistical heterogeneity due to non-{IID} distribution of data across devices often leads to scenarios where, for some clients, the local models trained solely on their private data perform better than the global shared model thus taking away their incentive to participate in the process. Several techniques have been proposed to personalize global models to work better for individual clients. This paper highlights the need for personalization and surveys recent research on this topic.},
	eventtitle = {{WorldS}4 '20},
	pages = {794--797},
	booktitle = {2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability ({WorldS}4)},
	author = {Kulkarni, Viraj and Kulkarni, Milind and Pant, Aniruddha},
	date = {2020-07},
	keywords = {Adaptation models, Computational modeling, Data models, Data privacy, Machine learning, Task analysis, Training, federated learning, machine learning, model personalization},
}

@article{agarwal_federated_2020,
	title = {Federated Residual Learning},
	url = {http://arxiv.org/abs/2003.12880},
	abstract = {We study a new form of federated learning where the clients train personalized local models and make predictions jointly with the server-side shared model. Using this new federated learning framework, the complexity of the central shared model can be minimized while still gaining all the performance beneﬁts that joint training provides. Our framework is robust to data heterogeneity, addressing the slow convergence problem traditional federated learning methods face when the data is non-i.i.d. across clients. We test the theory empirically and ﬁnd substantial performance gains over baselines.},
	journaltitle = {{arXiv}:2003.12880},
	author = {Agarwal, Alekh and Langford, John and Wei, Chen-Yu},
	urldate = {2022-02-18},
	date = {2020-03-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.12880},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{finn_model-agnostic_2017,
	title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
	volume = {70},
	url = {https://proceedings.mlr.press/v70/finn17a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	eventtitle = {{ICML} '17},
	pages = {1126--1135},
	booktitle = {34th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	date = {2017-08-06},
}

@inproceedings{recht_hogwild_2011,
	title = {Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},
	volume = {24},
	url = {https://proceedings.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf},
	abstract = {Stochastic Gradient Descent ({SGD}) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize {SGD}, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that {SGD} can be implemented *without any locking*. We present an update scheme called Hogwild which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then Hogwild achieves a nearly optimal rate of convergence. We demonstrate experimentally that Hogwild outperforms alternative schemes that use locking by an order of magnitude.},
	eventtitle = {{NeurIPS} '11},
	pages = {693--701},
	booktitle = {Advances in Neural Information Processing Systems 24},
	publisher = {Curran Associates, Inc.},
	author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	date = {2011},
}

@article{renz-wieland_nups_2021,
	title = {{NuPS}: A Parameter Server for Machine Learning with Non-Uniform Parameter Access},
	url = {http://arxiv.org/abs/2104.00501},
	shorttitle = {{NuPS}},
	abstract = {Parameter servers ({PSs}) facilitate the implementation of distributed training for large machine learning tasks. In this paper, we argue that existing {PSs} are inefficient for tasks that exhibit non-uniform parameter access; their performance may even fall behind that of single node baselines. We identify two major sources of such nonuniform access: skew and sampling. Existing {PSs} are ill-suited for managing skew because they uniformly apply the same parameter management technique to all parameters. They are inefficient for sampling because the {PS} is oblivious to the associated randomized accesses and cannot exploit locality. To overcome these performance limitations, we introduce {NuPS}, a novel {PS} architecture that (i) integrates multiple management techniques and employs a suitable technique for each parameter and (ii) supports sampling directly via suitable sampling primitives and sampling schemes that allow for a controlled quality–efficiency trade-off. In our experimental study, {NuPS} outperformed existing {PSs} by up to one order of magnitude and provided up to linear scalability across multiple machine learning tasks.},
	journaltitle = {{arXiv}:2104.00501},
	author = {Renz-Wieland, Alexander and Gemulla, Rainer and Kaoudi, Zoi and Markl, Volker},
	urldate = {2022-02-09},
	date = {2021-12-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2104.00501},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@online{nvidia_deep_2021,
	title = {Deep Learning Examples},
	url = {https://github.com/NVIDIA/DeepLearningExamples},
	author = {{NVIDIA}},
	urldate = {2021-03-02},
	date = {2021-03-02},
}

@article{zhou_places_2018,
	title = {Places: A 10 Million Image Database for Scene Recognition},
	volume = {40},
	doi = {10.1109/TPAMI.2017.2723009},
	pages = {1452--1464},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{PAML}},
	author = {Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	date = {2018},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	doi = {10.1007/s11263-015-0816-y},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {{IJCV}},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	date = {2015},
}

@article{pan_survey_2010,
	title = {A Survey on Transfer Learning},
	volume = {22},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	pages = {1345--1359},
	number = {10},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{TKDE}},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	date = {2010-10},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Data mining, Knowledge engineering, Knowledge transfer, Labeling, Learning systems, Machine learning, Machine learning algorithms, Space technology, Testing, Training data, Transfer learning, data mining., machine learning, survey},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient {GPU} implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {{CACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2021-02-08},
	date = {2017-05-24},
	langid = {english},
}

@article{koziarski_impact_2018,
	title = {Impact of Low Resolution on Image Recognition with Deep Neural Networks: An Experimental Study},
	volume = {28},
	url = {https://content.sciendo.com/view/journals/amcs/28/4/article-p735.xml},
	doi = {https://doi.org/10.2478/amcs-2018-0056},
	pages = {735 -- 744},
	number = {4},
	journaltitle = {International Journal of Applied Mathematics and Computer Science},
	shortjournal = {{IJAMCS}},
	author = {Koziarski, Michał and Cyganek, Bogusław},
	date = {2018-12-01},
}

@inproceedings{huang_cost-effective_2018,
	location = {London, United Kingdom},
	title = {Cost-Effective Training of Deep {CNNs} with Active Model Adaptation},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3220026},
	doi = {10.1145/3219819.3220026},
	abstract = {Deep convolutional neural networks have achieved great success in various applications. However, training an effective {DNN} model for a specific task is rather challenging because it requires a prior knowledge or experience to design the network architecture, repeated trial-and-error process to tune the parameters, and a large set of labeled data to train the model. In this paper, we propose to overcome these challenges by actively adapting a pre-trained model to a new task with less labeled examples. Specifically, the pre-trained model is iteratively fine tuned based on the most useful examples. The examples are actively selected based on a novel criterion, which jointly estimates the potential contribution of an instance on optimizing the feature representation as well as improving the classification model for the target task. On one hand, the pre-trained model brings plentiful information from its original task, avoiding redesign of the network architecture or training from scratch; and on the other hand, the labeling cost can be significantly reduced by active label querying. Experiments on multiple datasets and different pre-trained models demonstrate that the proposed approach can achieve cost-effective training of {DNNs}.},
	eventtitle = {{KDD} '18},
	pages = {1580--1588},
	booktitle = {24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Huang, Sheng-Jun and Zhao, Jia-Wei and Liu, Zhao-Yang},
	urldate = {2022-01-31},
	date = {2018-07-19},
	langid = {english},
}

@inproceedings{yang_deep_2018,
	location = {London, United Kingdom},
	title = {Deep Learning for Practical Image Recognition: Case Study on Kaggle Competitions},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3219907},
	doi = {10.1145/3219819.3219907},
	shorttitle = {Deep Learning for Practical Image Recognition},
	abstract = {In past years, deep convolutional neural networks ({DCNN}) have achieved big successes in image classification and object detection, as demonstrated on {ImageNet} in academic field. However, There are some unique practical challenges remain for real-world image recognition applications, e.g., small size of the objects, imbalanced data distributions, limited labeled data samples, etc. In this work, we are making efforts to deal with these challenges through a computational framework by incorporating latest developments in deep learning. In terms of two-stage detection scheme, pseudo labeling, data augmentation, cross-validation and ensemble learning, the proposed framework aims to achieve better performances for practical image recognition applications as compared to using standard deep learning methods. The proposed framework has recently been deployed as the key kernel for several image recognition competitions organized by Kaggle. The performance is promising as our final private scores were ranked 4 out of 2293 teams for fish recognition on the challenge “The Nature Conservancy Fisheries Monitoring” and 3 out of 834 teams for cervix recognition on the challenge “Intel \& {MobileODT} Cervical Cancer Screening”, and several others. We believe that by sharing the solutions, we can further promote the applications of deep learning techniques.},
	eventtitle = {{KDD} '18},
	pages = {923--931},
	booktitle = {24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {{ACM}},
	author = {Yang, Xulei and Zeng, Zeng and Teo, Sin G. and Wang, Li and Chandrasekhar, Vijay and Hoi, Steven},
	urldate = {2022-01-28},
	date = {2018-07-19},
	langid = {english},
}

@inproceedings{baylor_tfx_2017,
	location = {Halifax, {NS}},
	title = {{TFX}: A {TensorFlow}-Based Production-Scale Machine Learning Platform},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098021},
	doi = {10.1145/3097983.3098021},
	shorttitle = {{TFX}},
	abstract = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components—a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and ﬁnally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for speciﬁc use cases, leading to duplicated eﬀort and fragile systems with high technical debt. We present {TensorFlow} Extended ({TFX}), a {TensorFlowbased} general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform conﬁguration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions.},
	eventtitle = {{KDD} '17},
	pages = {1387--1395},
	booktitle = {23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Baylor, Denis and Breck, Eric and Cheng, Heng-Tze and Fiedel, Noah and Foo, Chuan Yu and Haque, Zakaria and Haykal, Salem and Ispir, Mustafa and Jain, Vihan and Koc, Levent and Koo, Chiu Yuen and Lew, Lukasz and Mewald, Clemens and Modi, Akshay Naresh and Polyzotis, Neoklis and Ramesh, Sukriti and Roy, Sudip and Whang, Steven Euijong and Wicke, Martin and Wilkiewicz, Jarek and Zhang, Xin and Zinkevich, Martin},
	urldate = {2022-01-27},
	date = {2017-08-13},
	langid = {english},
}

@article{dong_one_2021,
	title = {One Backward from Ten Forward, Subsampling for Large-Scale Deep Learning},
	url = {http://arxiv.org/abs/2104.13114},
	abstract = {Deep learning models in large-scale machine learning systems are often continuously trained with enormous data from production environments. The sheer volume of streaming training data poses a signiﬁcant challenge to real-time training subsystems and ad-hoc sampling is the standard practice. Our key insight is that these deployed {ML} systems continuously perform forward passes on data instances during inference, but ad-hoc sampling does not take advantage of this substantial computational effort. Therefore, we propose to record a constant amount of information per instance from these forward passes. The extra information measurably improves the selection of which data instances should participate in forward and backward passes. A novel optimization framework is proposed to analyze this problem and we provide an efﬁcient approximation algorithm under the framework of Mini-batch gradient descent as a practical solution. We also demonstrate the effectiveness of our framework and algorithm on several large-scale classiﬁcation and regression tasks, when compared with competitive baselines widely used in industry.},
	journaltitle = {{arXiv}:2104.13114},
	author = {Dong, Chaosheng and Jin, Xiaojie and Gao, Weihao and Wang, Yijia and Zhang, Hongyi and Wu, Xiang and Yang, Jianchao and Liu, Xiaobing},
	urldate = {2022-01-24},
	date = {2021-04-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2104.13114},
	keywords = {Computer Science - Machine Learning},
}

@article{li_survey_2021,
	title = {A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection},
	issn = {1558-2191},
	url = {http://arxiv.org/abs/1907.09693},
	doi = {10.1109/TKDE.2021.3124599},
	shorttitle = {A Survey on Federated Learning Systems},
	abstract = {Federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as {PyTorch} and {TensorFlow} that boost the development of deep learning, federated learning systems ({FLSs}) are equivalently important, and face challenges from various aspects such as effectiveness, efﬁciency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To achieve smooth ﬂow and guide future research, we introduce the deﬁnition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo and Li, Yuan and Liu, Xu and He, Bingsheng},
	urldate = {2022-01-12},
	date = {2021},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.09693},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Databases, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{roy_braintorrent_2019,
	title = {{BrainTorrent}: A Peer-to-Peer Environment for Decentralized Federated Learning},
	url = {http://arxiv.org/abs/1905.06731},
	shorttitle = {{BrainTorrent}},
	abstract = {Access to suﬃcient annotated data is a common challenge in training deep neural networks on medical images. As annotating data is expensive and time-consuming, it is diﬃcult for an individual medical center to reach large enough sample sizes to build their own, personalized models. As an alternative, data from all centers could be pooled to train a centralized model that everyone can use. However, such a strategy is often infeasible due to the privacy-sensitive nature of medical data. Recently, federated learning ({FL}) has been introduced to collaboratively learn a shared prediction model across centers without the need for sharing data. In {FL}, clients are locally training models on site-speciﬁc datasets for a few epochs and then sharing their model weights with a central server, which orchestrates the overall training process. Importantly, the sharing of models does not compromise patient privacy. A disadvantage of {FL} is the dependence on a central server, which requires all clients to agree on one trusted central body, and whose failure would disrupt the training process of all clients. In this paper, we introduce {BrainTorrent}, a new {FL} framework without a central server, particularly targeted towards medical applications. {BrainTorrent} presents a highly dynamic peer-to-peer environment, where all centers directly interact with each other without depending on a central body. We demonstrate the overall eﬀectiveness of {FL} for the challenging task of whole brain segmentation and observe that the proposed server-less {BrainTorrent} approach does not only outperform the traditional server-based one but reaches a similar performance to a model trained on pooled data.},
	journaltitle = {{arXiv}:1905.06731 [cs, stat]},
	author = {Roy, Abhijit Guha and Siddiqui, Shayan and Pölsterl, Sebastian and Navab, Nassir and Wachinger, Christian},
	urldate = {2022-01-15},
	date = {2019-05-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.06731},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{liu_secure_2020,
	title = {A Secure Federated Transfer Learning Framework},
	volume = {35},
	issn = {1941-1294},
	doi = {10.1109/MIS.2020.2988525},
	abstract = {Machine learning relies on the availability of vast amounts of data for training. However, in reality, data are mostly scattered across different organizations and cannot be easily integrated due to many legal and practical constraints. To address this important challenge in the field of machine learning, we introduce a new technique and framework, known as federated transfer learning ({FTL}), to improve statistical modeling under a data federation. {FTL} allows knowledge to be shared without compromising user privacy and enables complementary knowledge to be transferred across domains in a data federation, thereby enabling a target-domain party to build flexible and effective models by leveraging rich labels from a source domain. This framework requires minimal modifications to the existing model structure and provides the same level of accuracy as the nonprivacy-preserving transfer learning. It is flexible and can be effectively adapted to various secure multiparty machine learning tasks.},
	pages = {70--82},
	number = {4},
	journaltitle = {{IEEE} Intelligent Systems},
	author = {Liu, Yang and Kang, Yan and Xing, Chaoping and Chen, Tianjian and Yang, Qiang},
	date = {2020-07},
	keywords = {Adaption models, Collaborative work, Data models, Encryption, Federated Learning, Homomorphic Encryption, Machine learning, Multi-party Computation, Neural networks, Secret Sharing, Training data, Transfer Learning},
}

@inproceedings{hsieh_non-iid_2020,
	title = {The Non-{IID} Data Quagmire of Decentralized Machine Learning},
	abstract = {Many large-scale machine learning ({ML}) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a signiﬁcant challenge to decentralized learning because their different contexts result in signiﬁcant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized {DNN} training on a common type of data skew: skewed distribution of data labels across devices/locations. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing signiﬁcant accuracy loss across many {ML} applications, {DNN} models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for {DNN} models with batch normalization; and (iii) the degree of data skew is a key determinant of the difﬁculty of the problem. Based on these ﬁndings, we present {SkewScout}, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the accuracy loss of batch normalization.},
	eventtitle = {{ICML} '20},
	pages = {4387--4398},
	booktitle = {International Conference on Machine Learning},
	author = {Hsieh, Kevin and Phanishayee, Amar and Mutlu, Onur and Gibbons, Phillip B},
	date = {2020},
	langid = {english},
}

@inproceedings{koloskova_decentralized_2020,
	title = {Decentralized Deep Learning with Arbitrary Communication Compression},
	url = {https://openreview.net/forum?id=SkgGCkrKvH},
	abstract = {Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efﬁcient scaling to large compute clusters. As current approaches are limited by network bandwidth, we propose the use of communication compression in the decentralized training context. We show that {CHOCO}-{SGD} achieves linear speedup in the number of workers for arbitrary high compression ratios on general non-convex functions, and non-{IID} training data. We demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over decentralized user devices, connected by a peer-to-peer network and (ii) in a datacenter.},
	eventtitle = {{ICLR} '20},
	booktitle = {International Conference on Learning Representations},
	author = {Koloskova, Anastasia and Lin, Tao and Stich, Sebastian U. and Jaggi, Martin},
	date = {2020},
}

@article{chen_federated_2019,
	title = {Federated Meta-Learning with Fast Convergence and Efficient Communication},
	url = {http://arxiv.org/abs/1802.07876},
	abstract = {Statistical and systematic challenges in collaboratively training machine learning models across distributed networks of mobile devices have been the bottlenecks in the real-world application of federated learning. In this work, we show that meta-learning is a natural choice to handle these issues, and propose a federated meta-learning framework {FedMeta}, where a parameterized algorithm (or metalearner) is shared, instead of a global model in previous approaches. We conduct an extensive empirical evaluation on {LEAF} datasets and a real-world production dataset, and demonstrate that {FedMeta} achieves a reduction in required communication cost by 2.82-4.33 times with faster convergence, and an increase in accuracy by 3.23\%-14.84\% as compared to Federated Averaging ({FedAvg}) which is a leading optimization algorithm in federated learning. Moreover, {FedMeta} preserves user privacy since only the parameterized algorithm is transmitted between mobile devices and central servers, and no raw data is collected onto the servers.},
	journaltitle = {{arXiv}:1802.07876},
	author = {Chen, Fei and Luo, Mi and Dong, Zhenhua and Li, Zhenguo and He, Xiuqiang},
	urldate = {2022-01-14},
	date = {2019-12-14},
	langid = {english},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@article{wang_blockchain-based_2021,
	title = {Blockchain-Based Federated Learning in Mobile Edge Networks with Application in Internet of Vehicles},
	url = {http://arxiv.org/abs/2103.01116},
	abstract = {The rapid increase of the data scale in Internet of Vehicles ({IoV}) system paradigm, hews out new possibilities in boosting the service quality for the emerging applications through data sharing. Nevertheless, privacy concerns are major bottlenecks for data providers to share private data in traditional {IoV} networks. To this end, federated learning ({FL}) as an emerging learning paradigm, where data providers only send local model updates trained on their local raw data rather than upload any raw data, has been recently proposed to build a privacy-preserving data sharing models. Unfortunately, by analyzing on the differences of uploaded local model updates from data providers, private information can still be divulged, and performance of the system cannot be guaranteed when partial federated nodes executes malicious behavior. Additionally, traditional cloud-based {FL} poses challenges to the communication overhead with the rapid increase of terminal equipment in {IoV} system. All these issues inspire us to propose an autonomous blockchain empowered privacy-preserving {FL} framework in this paper, where the mobile edge computing ({MEC}) technology was naturally integrated in {IoV} system. Speciﬁcally, we introduce differential privacy techniques to prevent privacy concerns while the honesty of participants are ensured by a malicious updates remove algorithm based on self-reliability ﬁlter. Simultaneously, a double aggregation frame is proposed, to guarantee the communication overhead and ensure the quality of model training. We propose to build a federated learning participant management system by employing a blockchain architecture, and ensure the immutability of uploaded model via computing and recording their quality in blockchain. The model quality is quantiﬁed as aggregate weight and used as a criterion for the distribution of federal proﬁts, thus attracting vehicles with high-quality data to join federated learning. Numerical results derived from realworld datasets demonstrate that the proposed {FL} scheme for {IoV} system achieves good accuracy, high robustness, and enhanced security.},
	journaltitle = {{arXiv}:2103.01116},
	author = {Wang, Rui and Li, Heju and Liu, Erwu},
	urldate = {2022-01-13},
	date = {2021-03-01},
	langid = {english},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@inproceedings{marfoq_throughput-optimal_2020,
	location = {Vancouver, Canada},
	title = {Throughput-Optimal Topology Design for Cross-Silo Federated Learning},
	url = {http://arxiv.org/abs/2010.12229},
	abstract = {Federated learning usually employs a server-client architecture where an orchestrator iteratively aggregates model updates from remote clients and pushes them back a reﬁned model. This approach may be inefﬁcient in cross-silo settings, as close-by data silos with high-speed access links may exchange information faster than with the orchestrator, and the orchestrator may become a communication bottleneck. In this paper we deﬁne the problem of topology design for cross-silo federated learning using the theory of max-plus linear systems to compute the system throughput—number of communication rounds per time unit. We also propose practical algorithms that, under the knowledge of measurable network characteristics, ﬁnd a topology with the largest throughput or with provable throughput guarantees. In realistic Internet networks with 10 Gbps access links at silos, our algorithms speed up training by a factor 9 and 1.5 in comparison to the server-client architecture and to state-of-the-art {MATCHA}, respectively. Speedups are even larger with slower access links.},
	eventtitle = {{NeurIPS} '20},
	booktitle = {34th Conference on Neural Information Processing System},
	author = {Marfoq, Othmane and Xu, Chuan and Neglia, Giovanni and Vidal, Richard},
	urldate = {2022-01-13},
	date = {2020-11-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.12229},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Mathematics - Optimization and Control},
}

@article{li_practical_2020,
	title = {Practical Federated Gradient Boosting Decision Trees},
	volume = {34},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5895},
	doi = {10.1609/aaai.v34i04.5895},
	pages = {4642--4649},
	number = {4},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Li, Qinbin and Wen, Zeyi and He, Bingsheng},
	date = {2020-04},
}

@article{wagner_technical_2019,
	title = {Technical Privacy Metrics: A Systematic Survey},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3168389},
	doi = {10.1145/3168389},
	shorttitle = {Technical Privacy Metrics},
	abstract = {The goal of privacy metrics is to measure the degree of privacy enjoyed by users in a system and the amount of protection offered by privacy-enhancing technologies. In this way, privacy metrics contribute to improving user privacy in the digital world. The diversity and complexity of privacy metrics in the literature make an informed choice of metrics challenging. As a result, instead of using existing metrics, new metrics are proposed frequently, and privacy studies are often incomparable. In this survey, we alleviate these problems by structuring the landscape of privacy metrics. To this end, we explain and discuss a selection of over 80 privacy metrics and introduce categorizations based on the aspect of privacy they measure, their required inputs, and the type of data that needs protection. In addition, we present a method on how to choose privacy metrics based on nine questions that help identify the right privacy metrics for a given scenario, and highlight topics where additional work on privacy metrics is needed. Our survey spans multiple privacy domains and can be understood as a general framework for privacy measurement.},
	pages = {1--38},
	number = {3},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Wagner, Isabel and Eckhoff, David},
	urldate = {2022-01-13},
	date = {2019-05-31},
	langid = {english},
}

@article{aledhari_federated_2020,
	title = {Federated Learning: A Survey on Enabling Technologies, Protocols, and Applications},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3013541},
	abstract = {This paper provides a comprehensive study of Federated Learning ({FL}) with an emphasis on enabling software and hardware platforms, protocols, real-life applications and use-cases. {FL} can be applicable to multiple domains but applying it to different industries has its own set of obstacles. {FL} is known as collaborative learning, where algorithm(s) get trained across multiple devices or servers with decentralized data samples without having to exchange the actual data. This approach is radically different from other more established techniques such as getting the data samples uploaded to servers or having data in some form of distributed infrastructure. {FL} on the other hand generates more robust models without sharing data, leading to privacy-preserved solutions with higher security and access privileges to data. This paper starts by providing an overview of {FL}. Then, it gives an overview of technical details that pertain to {FL} enabling technologies, protocols, and applications. Compared to other survey papers in the field, our objective is to provide a more thorough summary of the most relevant protocols, platforms, and real-life use-cases of {FL} to enable data scientists to build better privacy-preserving solutions for industries in critical need of {FL}. We also provide an overview of key challenges presented in the recent literature and provide a summary of related research work. Moreover, we explore both the challenges and advantages of {FL} and present detailed service use-cases to illustrate how different architectures and protocols that use {FL} can fit together to deliver desired results.},
	pages = {140699--140725},
	journaltitle = {{IEEE} Access},
	author = {Aledhari, Mohammed and Razzak, Rehma and Parizi, Reza M. and Saeed, Fahad},
	date = {2020},
	keywords = {Computational modeling, Computer architecture, Data models, Data privacy, Federated learning, Industries, Machine learning, Protocols, collaborative {AI}, decentralized data, machine learning, on-device {AI}, peer-to-peer network, privacy, security},
}

@article{chen_wireless_2020,
	title = {Wireless Communications for Collaborative Federated Learning},
	volume = {58},
	issn = {1558-1896},
	doi = {10.1109/MCOM.001.2000397},
	abstract = {To facilitate the deployment of machine learning in resource and privacy-constrained systems such as the Internet of Things, federated learning ({FL}) has been proposed as a means for enabling edge devices to train a shared learning model while promoting privacy. However, Google's seminal {FL} algorithm requires all devices to be directly connected with a central controller, which limits its applications. In contrast, this article introduces a novel {FL} framework, called collaborative {FL} ({CFL}), which enables edge devices to implement {FL} with less reliance on a central controller. The fundamentals of this framework are developed and a number of communication techniques are proposed so as to improve {CFL} performance. An overview of centralized learning, Google's {FL}, and {CFL} is presented. For each type of learning, the basic architecture as well as its advantages, drawbacks, and operating conditions are introduced. Then four {CFL} performance metrics are presented, and a suite of communication techniques ranging from network formation, device scheduling, mobility management, to coding are introduced to optimize the performance of {CFL}. For each technique, future research opportunities are discussed. In a nutshell, this article showcases how {CFL} can be effectively implemented at the edge of large-scale wireless systems.},
	pages = {48--54},
	number = {12},
	journaltitle = {{IEEE} Communications Magazine},
	author = {Chen, Mingzhe and Poor, H. Vincent and Saad, Walid and Cui, Shuguang},
	date = {2020-12},
	note = {Conference Name: {IEEE} Communications Magazine},
	keywords = {Collaborative work, Object recognition, Performance evaluation, Privacy, Reliability, Wireless communication},
}

@article{wu_ddlpf_2021,
	title = {{DDLPF}: A Practical Decentralized Deep Learning Paradigm for Internet-of-Things Applications},
	volume = {8},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2020.3033482},
	shorttitle = {{DDLPF}},
	abstract = {In recent years, it has been observed the exponential growth of the Internet of Things ({IoT}) in different application fields, such as manufacturing and energy industry. To effectively fuse and process the tremendous amount of {IoT} sensing data timely, there is an urgent need to shift from a conventional centralized computing to a decentralized computing. However, there remain some essential technical challenges to develop effective decentralized computing methods in the context of {IoT} applications, including 1) the timely response, sufficient privacy preservation, and high security are normally required in {IoT}-related applications and 2) the biases and non-independent identically distributed ({IID}) properties potentially presented in the {IoT} sensing data. To address these challenges, in this article, we propose a decentralized deep learning paradigm with privacy-preservation and fast few-shot learning ({DDLPF}) by exploiting federated learning, metalearning, and blockchain techniques. In the simulation section, we evaluate the performance of our proposed {DDLPF} paradigm in different scenarios and compare it with other existing techniques.},
	pages = {9740--9752},
	number = {12},
	journaltitle = {{IEEE} Internet of Things Journal},
	author = {Wu, Yifu and Mendis, Gihan J. and Wei, Jin},
	date = {2021-06},
	keywords = {Blockchain, Computational modeling, Data models, Deep learning, Internet of Things, Internet of Things ({IoT}), Sensors, Servers, Task analysis, decentralized deep learning, few-shot learning, metalearning, privacy preservation},
}

@article{rieke_future_2020,
	title = {The future of digital health with federated learning},
	volume = {3},
	issn = {2398-6352},
	url = {https://pubmed.ncbi.nlm.nih.gov/33015372},
	doi = {10.1038/s41746-020-00323-1},
	abstract = {Data-driven machine learning ({ML}) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by {ML} primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, {ML} will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how federated learning ({FL}) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.},
	pages = {119},
	journaltitle = {{NPJ} digital medicine},
	shortjournal = {{NPJ} Digit Med},
	eprinttype = {pubmed},
	eprint = {33015372},
	author = {Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletarì, Fausto and Roth, Holger R and Albarqouni, Shadi and Bakas, Spyridon and Galtier, Mathieu N and Landman, Bennett A and Maier-Hein, Klaus and Ourselin, Sébastien and Sheller, Micah and Summers, Ronald M and Trask, Andrew and Xu, Daguang and Baust, Maximilian and Cardoso, M Jorge},
	date = {2020-09-14},
	keywords = {Medical imaging, Medical research},
}

@inproceedings{balakrishnan_applying_2015,
	location = {Asilomar, {CA}},
	title = {Applying {WebTables} in Practice},
	abstract = {We started investigating the collection of {HTML} tables on the Web and developed the {WebTables} system a few years ago [4]. Since then, our work has been motivated by applying {WebTables} in a broad set of applications at Google, resulting in several product launches. In this paper, we describe the challenges faced, lessons learned, and new insights that we gained from our eﬀorts.},
	eventtitle = {{CIDR} '15},
	booktitle = {7th Biennial Conference on Innovative Data Systems Research},
	author = {Balakrishnan, Sreeram and Halevy, Alon and Harb, Boulos and Lee, Hongrae and Madhavan, Jayant and Rostamizadeh, Afshin and Shen, Warren and Wilder, Kenneth and Wu, Fei and Yu, Cong},
	date = {2015},
	langid = {english},
}

@article{bonawitz_federated_2021,
	title = {Federated Learning and Privacy: Building Privacy-Preserving Systems for Machine Learning and Data Science on Decentralized Data},
	volume = {19},
	issn = {1542-7730},
	url = {https://doi.org/10.1145/3494834.3500240},
	doi = {10.1145/3494834.3500240},
	abstract = {Centralized data collection can expose individuals to privacy risks and organizations to legal risks if data is not properly managed. Federated learning is a machine learning setting where multiple entities collaborate in solving a machine learning problem, under the coordination of a central server or service provider. Each client's raw data is stored locally and not exchanged or transferred; instead, focused updates intended for immediate aggregation are used to achieve the learning objective. This article provides a brief introduction to key concepts in federated learning and analytics with an emphasis on how privacy technologies may be combined in real-world systems and how their use charts a path toward societal benefit from aggregate statistics in new domains and with minimized risk to individuals and to the organizations who are custodians of the data.},
	pages = {87--114},
	number = {5},
	journaltitle = {Queue},
	author = {Bonawitz, Kallista and Kairouz, Peter and {McMahan}, Brendan and Ramage, Daniel},
	date = {2021-10},
}

@inproceedings{cafarella_uncovering_2008,
	location = {Vancouver, Canada},
	title = {Uncovering the Relational Web},
	url = {https://web.eecs.umich.edu/~michjc/papers/webtables_webdb08.pdf},
	abstract = {The World-Wide Web consists of a huge number of unstructured hypertext documents, but it also contains structured data in the form of {HTML} tables. Many of these tables contain both relational-style data and a small “schema” of labeled and typed columns, making each such table a small structured database. The {WebTables} project is an effort to extract and make use of the huge number of these structured tables on the Web. A clean collection of relational-style tables could be useful for improving web search, schema design, and many other applications.

This paper describes the first stage of the {WebTables} project. First, we give an in-depth study of the Web’s {HTML} table corpus. For example, we extracted 14.1 billion {HTML} tables from a several-billion-page portion of Google’s general-purpose web crawl, and estimate that 154 million of these tables contain high-quality relational-style data. We also describe the crawl’s distribution of table sizes and data types.

Second, we describe a system for performing relation recovery. The Web mixes relational and non-relational tables indiscriminately (often on the same page), so there is no simple way to distinguish the 1.1\% of good relations from the remainder, nor to recover column label and type information. Our mix of hand-written detectors and statistical classifiers takes a raw Web crawl as input, and generates a collection of databases that is five orders of magnitude larger than any other collection we are aware of. Relation recovery achieves precision and recall that are comparable to other domain-independent information extraction systems.},
	eventtitle = {{WebDB} '08},
	booktitle = {11th International Workshop on Web and Databases},
	author = {Cafarella, Michael J. and Halevy, Alon and Wang, Zhe and Wu, Eugene and Zhang, Yang},
	urldate = {2021-11-29},
	date = {2008-06-13},
}

@article{shannon_mathematical_1948,
	title = {A Mathematical Theory of Communication},
	volume = {27},
	issn = {0005-8580},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	abstract = {The recent development of various methods of modulation such as {PCM} and {PPM} which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
	pages = {379--423},
	number = {3},
	journaltitle = {The Bell System Technical Journal},
	author = {Shannon, Claude E.},
	date = {1948-07},
	note = {Conference Name: The Bell System Technical Journal},
}

@online{nvidia_nvidia_2021,
	title = {{NVIDIA} A100 Tensor Core {GPU}},
	url = {https://www.nvidia.com/en-us/data-center/a100},
	abstract = {The fastest data center platform for {AI} and {HPC}.},
	author = {{NVIDIA}},
	urldate = {2021-02-01},
	date = {2021},
	langid = {english},
}

@online{google_optimize_2018,
	title = {Optimize Images: {PageSpeed} Insights},
	url = {https://developers.google.com/speed/docs/insights/OptimizeImages},
	author = {{Google}},
	urldate = {2021-09-10},
	date = {2018},
	langid = {english},
}

@online{google_webp_2010,
	title = {{WebP}: An image format for the Web},
	url = {https://developers.google.com/speed/webp},
	author = {{Google}},
	urldate = {2021-09-14},
	date = {2010},
	langid = {english},
}

@article{begaint_compressai_2020,
	title = {{CompressAI}: A {PyTorch} Library and Evaluation Platform for End-to-End Compression Research},
	url = {http://arxiv.org/abs/2011.03029},
	shorttitle = {{CompressAI}},
	abstract = {This paper presents {CompressAI}, a platform that provides custom operations, layers, models and tools to research, develop and evaluate end-to-end image and video compression codecs. In particular, {CompressAI} includes pre-trained models and evaluation tools to compare learned methods with traditional codecs. Multiple models from the state-of-the-art on learned end-to-end compression have thus been reimplemented in {PyTorch} and trained from scratch. We also report objective comparison results using {PSNR} and {MS}-{SSIM} metrics vs. bit-rate, using the Kodak image dataset as test set. Although this framework currently implements models for still-picture compression, it is intended to be soon extended to the video compression domain.},
	journaltitle = {{arXiv}:2011.03029},
	author = {Bégaint, Jean and Racapé, Fabien and Feltman, Simon and Pushparaja, Akshay},
	urldate = {2021-09-27},
	date = {2020-11-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2011.03029},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{balle_variational_2018,
	location = {Vancouver, Canada},
	title = {Variational Image Compression with a Scale Hyperprior},
	url = {https://arxiv.org/abs/1802.01436},
	abstract = {We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artiﬁcial neural networks ({ANNs}). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular {MS}-{SSIM} index, and yields rate–distortion performance surpassing published {ANN}-based methods when evaluated using a more traditional metric based on squared error ({PSNR}). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics.},
	eventtitle = {{ICLR} '18},
	booktitle = {International Conference on Learning Representations},
	author = {Ballé, Johannes and Minnen, David and Singh, Saurabh and Hwang, Sung Jin and Johnston, Nick},
	urldate = {2021-09-15},
	date = {2018},
	langid = {english},
}

@inproceedings{amodei_deep_2016,
	location = {New York, {NY}},
	title = {Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin},
	url = {https://proceedings.mlr.press/v48/amodei16.html},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of {HPC} techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with {GPUs} in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	eventtitle = {{ICML} '16},
	pages = {173--182},
	booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
	author = {Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and Chen, Jie and Chen, Jingdong and Chen, Zhijie and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Ding, Ke and Du, Niandong and Elsen, Erich and Engel, Jesse and Fang, Weiwei and Fan, Linxi and Fougner, Christopher and Gao, Liang and Gong, Caixia and Hannun, Awni and Han, Tony and Johannes, Lappi and Jiang, Bing and Ju, Cai and Jun, Billy and {LeGresley}, Patrick and Lin, Libby and Liu, Junjie and Liu, Yang and Li, Weigao and Li, Xiangang and Ma, Dongpeng and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Peng, Yiping and Prenger, Ryan and Qian, Sheng and Quan, Zongfeng and Raiman, Jonathan and Rao, Vinay and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Srinet, Kavya and Sriram, Anuroop and Tang, Haiyuan and Tang, Liliang and Wang, Chong and Wang, Jidong and Wang, Kaifu and Wang, Yi and Wang, Zhijian and Wang, Zhiqian and Wu, Shuang and Wei, Likai and Xiao, Bo and Xie, Wen and Xie, Yan and Yogatama, Dani and Yuan, Bin and Zhan, Jun and Zhu, Zhenyao},
	editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
	date = {2016-06-20},
}

@article{brin_anatomy_1998,
	title = {The Anatomy of a Large-Scale Hypertextual Web Search Engine},
	volume = {30},
	issn = {01697552},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016975529800110X},
	doi = {10.1016/S0169-7552(98)00110-X},
	abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
	pages = {107--117},
	number = {1},
	journaltitle = {Computer Networks and {ISDN} Systems},
	shortjournal = {Computer Networks and {ISDN} Systems},
	author = {Brin, Sergey and Page, Lawrence},
	urldate = {2021-11-12},
	date = {1998-04},
	langid = {english},
}

@incollection{brewer_combining_2005,
	location = {Cambrige, {MA}},
	edition = {4th},
	title = {Combining Systems and Databases: A Search Engine Retrospective},
	pages = {14},
	booktitle = {Readings in Database Systems},
	publisher = {{MIT} Press},
	author = {Brewer, Eric A.},
	editor = {Hellerstein, Joseph M. and Stonebraker, Michael},
	date = {2005},
	langid = {english},
}

@article{ratner_mlsys_2019,
	title = {{MLSys}: The New Frontier of Machine Learning Systems},
	url = {http://arxiv.org/abs/1904.03257},
	shorttitle = {{MLSys}},
	abstract = {Machine learning ({ML}) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support {ML} models in real-world deployments remains a signiﬁcant obstacle, in large part due to the radically different development and deployment proﬁle of modern {ML} methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and {ML} communities, focused on topics such as hardware systems for {ML}, software systems for {ML}, and {ML} optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, {MLSys}, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and {ML}, and an explicit focus on topics at the intersection of the two.},
	journaltitle = {{arXiv}:1904.03257},
	author = {Ratner, Alexander and Alistarh, Dan and Alonso, Gustavo and Andersen, David G. and Bailis, Peter and Bird, Sarah and Carlini, Nicholas and Catanzaro, Bryan and Chayes, Jennifer and Chung, Eric and Dally, Bill and Dean, Jeff and Dhillon, Inderjit S. and Dimakis, Alexandros and Dubey, Pradeep and Elkan, Charles and Fursin, Grigori and Ganger, Gregory R. and Getoor, Lise and Gibbons, Phillip B. and Gibson, Garth A. and Gonzalez, Joseph E. and Gottschlich, Justin and Han, Song and Hazelwood, Kim and Huang, Furong and Jaggi, Martin and Jamieson, Kevin and Jordan, Michael I. and Joshi, Gauri and Khalaf, Rania and Knight, Jason and Konečný, Jakub and Kraska, Tim and Kumar, Arun and Kyrillidis, Anastasios and Lakshmiratan, Aparna and Li, Jing and Madden, Samuel and {McMahan}, H. Brendan and Meijer, Erik and Mitliagkas, Ioannis and Monga, Rajat and Murray, Derek and Olukotun, Kunle and Papailiopoulos, Dimitris and Pekhimenko, Gennady and Rekatsinas, Theodoros and Rostamizadeh, Afshin and Ré, Christopher and De Sa, Christopher and Sedghi, Hanie and Sen, Siddhartha and Smith, Virginia and Smola, Alex and Song, Dawn and Sparks, Evan and Stoica, Ion and Sze, Vivienne and Udell, Madeleine and Vanschoren, Joaquin and Venkataraman, Shivaram and Vinayak, Rashmi and Weimer, Markus and Wilson, Andrew Gordon and Xing, Eric and Zaharia, Matei and Zhang, Ce and Talwalkar, Ameet},
	urldate = {2021-11-03},
	date = {2019-12-01},
	langid = {english},
	keywords = {Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Software Engineering, Statistics - Machine Learning},
}

@inproceedings{minnen_joint_2018,
	location = {Montreal, Canada},
	title = {Joint Autoregressive and Hierarchical Priors for Learned Image Compression},
	abstract = {Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate–distortion performance and generates smaller files than existing methods: 15.8\% rate reductions over the baseline hierarchical model and 59.8\%, 35\%, and 8.4\% savings over {JPEG}, {JPEG}2000, and {BPG}, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec ({BPG}) on both the {PSNR} and {MS}-{SSIM} distortion metrics.},
	eventtitle = {{NIPS} '18},
	pages = {10794--10803},
	booktitle = {32nd International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Minnen, David and Ballé, Johannes and Toderici, George},
	date = {2018},
}

@online{mlcommons_association_mlperf_2018,
	title = {{MLPerf} Training v0.5 Results},
	url = {https://mlcommons.org/},
	titleaddon = {{MLCommons}},
	author = {{MLCommons} Association},
	urldate = {2021-11-03},
	date = {2018-12-12},
	langid = {english},
}

@online{mlcommons_association_mlperf_2021,
	title = {{MLPerf} Training v1.0 Results},
	url = {https://mlcommons.org/en/training-normal-10/},
	titleaddon = {{MLCommons}},
	author = {{MLCommons Association}},
	urldate = {2021-11-03},
	date = {2021-06-30},
	langid = {english},
}

@article{hardy_private_2017,
	title = {Private Federated Learning on Vertically Partitioned Data via Entity Resolution and Additively Homomorphic Encryption},
	url = {http://arxiv.org/abs/1711.10677},
	abstract = {Consider two data providers, each maintaining private records of different feature sets about common entities. They aim to learn a linear model jointly in a federated setting, namely, data is local and a shared model is trained from locally computed updates. In contrast with most work on distributed learning, in this scenario (i) data is split vertically, i.e. by features, (ii) only one data provider knows the target variable and (iii) entities are not linked across the data providers. Hence, to the challenge of private learning, we add the potentially negative consequences of mistakes in entity resolution. Our contribution is twofold. First, we describe a three-party end-to-end solution in two phases—privacy-preserving entity resolution and federated logistic regression over messages encrypted with an additively homomorphic scheme—, secure against a honest-but-curious adversary. The system allows learning without either exposing data in the clear or sharing which entities the data providers have in common. Our implementation is as accurate as a naive non-private solution that brings all data in one place, and scales to problems with millions of entities with hundreds of features. Second, we provide what is to our knowledge the ﬁrst formal analysis of the impact of entity resolution’s mistakes on learning, with results on how optimal classiﬁers, empirical losses, margins and generalisation abilities are affected. Our results bring a clear and strong support for federated learning: under reasonable assumptions on the number and magnitude of entity resolution’s mistakes, it can be extremely beneﬁcial to carry out federated learning in the setting where each peer’s data provides a signiﬁcant uplift to the other.},
	journaltitle = {{arXiv}:1711.10677},
	author = {Hardy, Stephen and Henecka, Wilko and Ivey-Law, Hamish and Nock, Richard and Patrini, Giorgio and Smith, Guillaume and Thorne, Brian},
	urldate = {2021-10-22},
	date = {2017-11-28},
	langid = {english},
	keywords = {Computer Science - Machine Learning},
}

@online{kairouz_federated_2020,
	title = {Federated Learning Tutorial},
	url = {https://sites.google.com/view/fl-tutorial/home},
	abstract = {Federated Learning Tutorial at {NeurIPS} 2020},
	author = {Kairouz, Peter and {McMahan}, H. Brendan and Smith, Virginia},
	urldate = {2021-10-25},
	date = {2020},
	langid = {american},
}

@collection{rehman_federated_2021,
	location = {Cham},
	title = {Federated Learning Systems: Towards Next-Generation {AI}},
	volume = {965},
	isbn = {978-3-030-70603-6 978-3-030-70604-3},
	url = {https://link.springer.com/10.1007/978-3-030-70604-3},
	series = {Studies in Computational Intelligence},
	shorttitle = {Federated Learning Systems},
	publisher = {Springer International Publishing},
	editor = {Rehman, Muhammad Habib ur and Gaber, Mohamed Medhat},
	urldate = {2021-10-14},
	date = {2021},
	langid = {english},
}

@article{li_federated_2020,
	title = {Federated Learning: Challenges, Methods, and Future Directions},
	volume = {37},
	issn = {1053-5888, 1558-0792},
	url = {http://arxiv.org/abs/1908.07873},
	doi = {10.1109/MSP.2020.2975749},
	shorttitle = {Federated Learning},
	abstract = {Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.},
	pages = {50--60},
	number = {3},
	journaltitle = {{IEEE} Signal Processing Magazine},
	shortjournal = {{IEEE} Signal Process. Mag.},
	author = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
	urldate = {2021-03-15},
	date = {2020-05},
	langid = {english},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{konecny_federated_2017,
	title = {Federated Learning: Strategies for Improving Communication Efficiency},
	url = {http://arxiv.org/abs/1610.05492},
	shorttitle = {Federated Learning},
	abstract = {Federated Learning is a machine learning setting where the goal is to train a highquality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efﬁciency is of the utmost importance.},
	journaltitle = {{arXiv}:1610.05492},
	author = {Konečný, Jakub and {McMahan}, H. Brendan and Yu, Felix X. and Richtárik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
	urldate = {2021-10-14},
	date = {2017-10-30},
	langid = {english},
	keywords = {Computer Science - Machine Learning},
}

@article{yang_federated_2019,
	title = {Federated Machine Learning: Concept and Applications},
	volume = {10},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3298981},
	doi = {10.1145/3298981},
	abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
	number = {2},
	journaltitle = {{ACM} Trans. Intell. Syst. Technol.},
	author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
	date = {2019-01},
	keywords = {Federated learning, {GDPR}, transfer learning},
}

@article{he_fedml_2020,
	title = {{FedML}: A Research Library and Benchmark for Federated Machine Learning},
	url = {http://arxiv.org/abs/2007.13518},
	shorttitle = {{FedML}},
	abstract = {Federated learning ({FL}) is a rapidly growing research ﬁeld in machine learning. However, existing {FL} libraries cannot adequately support diverse algorithmic development; inconsistent dataset and model usage make fair algorithm comparison challenging. In this work, we introduce {FedML}, an open research library and benchmark to facilitate {FL} algorithm development and fair performance comparison. {FedML} supports three computing paradigms: on-device training for edge devices, distributed computing, and single-machine simulation. {FedML} also promotes diverse algorithmic research with ﬂexible and generic {API} design and comprehensive reference baseline implementations (optimizer, models, and datasets). We hope {FedML} could provide an efﬁcient and reproducible means for developing and evaluating {FL} algorithms that would beneﬁt the {FL} research community. We maintain the source code, documents, and user community at https://fedml.ai.},
	journaltitle = {{arXiv}:2007.13518},
	author = {He, Chaoyang and Li, Songze and So, Jinhyun and Zeng, Xiao and Zhang, Mi and Wang, Hongyi and Wang, Xiaoyang and Vepakomma, Praneeth and Singh, Abhishek and Qiu, Hang and Zhu, Xinghua and Wang, Jianzong and Shen, Li and Zhao, Peilin and Kang, Yan and Liu, Yang and Raskar, Ramesh and Yang, Qiang and Annavaram, Murali and Avestimehr, Salman},
	urldate = {2021-06-21},
	date = {2020-11-08},
	langid = {english},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ludwig_ibm_2020,
	title = {{IBM} Federated Learning: an Enterprise Framework White Paper V0.1},
	url = {http://arxiv.org/abs/2007.10987},
	shorttitle = {{IBM} Federated Learning},
	abstract = {Federated Learning ({FL}) is an approach to conduct machine learning without centralizing training data in a single place, for reasons of privacy, conﬁdentiality or data volume. However, solving federated machine learning problems raises issues above and beyond those of centralized machine learning. These issues include setting up communication infrastructure between parties, coordinating the learning process, integrating party results, understanding the characteristics of the training data sets of diﬀerent participating parties, handling data heterogeneity, and operating with the absence of a veriﬁcation data set.},
	journaltitle = {{arXiv}:2007.10987},
	author = {Ludwig, Heiko and Baracaldo, Nathalie and Thomas, Gegi and Zhou, Yi and Anwar, Ali and Rajamoni, Shashank and Ong, Yuya and Radhakrishnan, Jayaram and Verma, Ashish and Sinn, Mathieu and Purcell, Mark and Rawat, Ambrish and Minh, Tran and Holohan, Naoise and Chakraborty, Supriyo and Whitherspoon, Shalisha and Steuer, Dean and Wynter, Laura and Hassan, Hifaz and Laguna, Sean and Yurochkin, Mikhail and Agarwal, Mayank and Chuba, Ebube and Abay, Annie},
	urldate = {2021-10-20},
	date = {2020-07-22},
	langid = {english},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, I.2.11, I.2.6},
}

@article{beutel_flower_2021,
	title = {Flower: A Friendly Federated Learning Research Framework},
	url = {http://arxiv.org/abs/2007.14390},
	shorttitle = {Flower},
	abstract = {Federated Learning ({FL}) has emerged as a promising technique for edge devices to collaboratively learn a shared prediction model while keeping training data on device, thereby decoupling the ability to do machine learning from the need to store data in the cloud. However, {FL} is difﬁcult to implement and deploy in practice considering the heterogeneity in common edge device settings, e.g., different frameworks, languages, and hardware accelerators. On the systems side, progress has been two-fold: closed large-scale industrial systems running {FL} on diverse device ﬂeets and open source research frameworks primarily used for single-machine simulation. In this paper, we present Flower, a novel {FL} framework which uniﬁes both perspectives. Flower is open source, supports heterogeneous environments including mobile and edge devices, and scales to a large number of distributed clients. Abstractions provided by Flower allow engineers to port existing workloads with little overhead, regardless of {ML} framework used, while also enabling researchers ﬂexibility to experiment with novel approaches to advance the state-of-the-art. We describe the design goals and architecture of Flower and use it to evaluate the impacts of scale and heterogeneity on common {FL} methods in experiments with up to 1000 clients.},
	journaltitle = {{arXiv}:2007.14390},
	author = {Beutel, Daniel J. and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Parcollet, Titouan and de Gusmão, Pedro P. B. and Lane, Nicholas D.},
	urldate = {2021-09-22},
	date = {2021-04-07},
	langid = {english},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{caldas_leaf_2019,
	location = {Vancouver, Canada},
	title = {{LEAF}: A Benchmark for Federated Settings},
	url = {http://arxiv.org/abs/1812.01097},
	shorttitle = {{LEAF}},
	abstract = {Modern federated networks, such as those comprised of wearable devices, mobile phones, or autonomous vehicles, generate massive amounts of data each day. This wealth of data can help to learn models that can improve the user experience on each device. However, the scale and heterogeneity of federated data presents new challenges in research areas such as federated learning, meta-learning, and multi-task learning. As the machine learning community begins to tackle these challenges, we are at a critical time to ensure that developments made in these areas are grounded with realistic benchmarks. To this end, we propose {LEAF}, a modular benchmarking framework for learning in federated settings. {LEAF} includes a suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.},
	eventtitle = {{NeurIPS} '19},
	booktitle = {33rd Conference on Neural Information Processing Systems},
	author = {Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Konečný, Jakub and {McMahan}, H. Brendan and Smith, Virginia and Talwalkar, Ameet},
	urldate = {2021-06-21},
	date = {2019-12-09},
	langid = {english},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{ziller_pysyft_2021,
	location = {Cham},
	title = {{PySyft}: A Library for Easy Federated Learning},
	volume = {965},
	isbn = {978-3-030-70604-3},
	url = {https://doi.org/10.1007/978-3-030-70604-3_5},
	shorttitle = {{PySyft}},
	abstract = {{PySyft} is an open-source multi-language library enabling secure and private machine learning by wrapping and extending popular deep learning frameworks such as {PyTorch} in a transparent, lightweight, and user-friendly manner. Its aim is to both help popularize privacy-preserving techniques in machine learning by making them as accessible as possible via Python bindings and common tools familiar to researchers and data scientists, as well as to be extensible such that new Federated Learning ({FL}), Multi-Party Computation, or Differential Privacy methods can be flexibly and simply implemented and integrated. This chapter will introduce the methods available within the {PySyft} library and describe their implementations. We will then provide a proof-of-concept demonstration of a {FL} workflow using an example of how to train a convolutional neural network. Next, we review the use of {PySyft} in academic literature to date and discuss future use-cases and development plans. Most importantly, we introduce Duet: our tool for easier {FL} for scientists and data owners.},
	pages = {111--139},
	booktitle = {Federated Learning Systems: Towards Next-Generation {AI}},
	publisher = {Springer International Publishing},
	author = {Ziller, Alexander and Trask, Andrew and Lopardo, Antonio and Szymkow, Benjamin and Wagner, Bobby and Bluemke, Emma and Nounahon, Jean-Mickael and Passerat-Palmbach, Jonathan and Prakash, Kritika and Rose, Nick and Ryffel, Théo and Reza, Zarreen Naowal and Kaissis, Georgios},
	editor = {Rehman, Muhammad Habib ur and Gaber, Mohamed Medhat},
	date = {2021},
}

@inproceedings{bonawitz_towards_2019,
	location = {Palo Alto, {CA}},
	title = {Towards Federated Learning at Scale: System Design},
	url = {http://arxiv.org/abs/1902.01046},
	shorttitle = {Towards Federated Learning at Scale},
	abstract = {Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on {TensorFlow}. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.},
	eventtitle = {{SysML} '19},
	booktitle = {2nd {SysML} Conference},
	author = {Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Konečný, Jakub and Mazzocchi, Stefano and {McMahan}, H. Brendan and Van Overveldt, Timon and Petrou, David and Ramage, Daniel and Roselander, Jason},
	urldate = {2021-03-15},
	date = {2019-03-22},
	langid = {english},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{so_turbo-aggregate_2021,
	title = {Turbo-Aggregate: Breaking the Quadratic Aggregation Barrier in Secure Federated Learning},
	volume = {2},
	issn = {2641-8770},
	doi = {10.1109/JSAIT.2021.3054610},
	shorttitle = {Turbo-Aggregate},
	abstract = {Federated learning is a distributed framework for training machine learning models over the data residing at mobile devices, while protecting the privacy of individual users. A major bottleneck in scaling federated learning to a large number of users is the overhead of secure model aggregation across many users. In particular, the overhead of the state-of-the-art protocols for secure model aggregation grows quadratically with the number of users. In this article, we propose the first secure aggregation framework, named Turbo-Aggregate, that in a network with N users achieves a secure aggregation overhead of O({NlogN}), as opposed to O(N2), while tolerating up to a user dropout rate of 50\%. Turbo-Aggregate employs a multi-group circular strategy for efficient model aggregation, and leverages additive secret sharing and novel coding techniques for injecting aggregation redundancy in order to handle user dropouts while guaranteeing user privacy. We experimentally demonstrate that Turbo-Aggregate achieves a total running time that grows almost linear in the number of users, and provides up to 40× speedup over the state-of-the-art protocols with up to N=200 users. Our experiments also demonstrate the impact of model size and bandwidth on the performance of Turbo-Aggregate.},
	pages = {479--489},
	number = {1},
	journaltitle = {{IEEE} Journal on Selected Areas in Information Theory},
	author = {So, Jinhyun and Güler, Başak and Avestimehr, A. Salman},
	date = {2021-03},
	keywords = {Collaborative work, Computational modeling, Data models, Federated learning, Privacy, Protocols, Servers, Training, privacy-preserving machine learning, secure aggregation},
}

@article{learning_technology_standards_committee_ieee_2021,
	title = {{IEEE} Guide for Architectural Framework and Application of Federated Machine Learning},
	doi = {10.1109/IEEESTD.2021.9382202},
	abstract = {Federated machine learning defines a machine learning framework that allows a collective model to be constructed from data that is distributed across repositories owned by different organizations or devices. A blueprint for data usage and model building across organizations and devices while meeting applicable privacy, security and regulatory requirements is provided in this guide. It defines the architectural framework and application guidelines for federated machine learning, including description and definition of federated machine learning; the categories federated machine learning and the application scenarios to which each category applies; performance evaluation of federated machine learning; and associated regulatory requirements.},
	pages = {1--69},
	journaltitle = {{IEEE} Std 3652.1-2020},
	author = {{Learning Technology Standards Committee}},
	date = {2021-03},
	keywords = {Collaborative work, Economics, {IEEE} 3652.1™, {IEEE} Standards, Machine learning, Metasearch, Modeling, Privacy, computation efficiency, economic viability, federated machine learning ({FML}), incentive mechanism, machine learning, model performance, privacy, privacy regulations, security},
}

@inproceedings{dodge_understanding_2016,
	location = {Lisbon, Portugal},
	title = {Understanding How Image Quality Affects Deep Neural Networks},
	doi = {10.1109/QoMEX.2016.7498955},
	abstract = {Image quality is an important practical challenge that is often overlooked in the design of machine vision systems. Commonly, machine vision systems are trained and tested on high quality image datasets, yet in practical applications the input images can not be assumed to be of high quality. Recently, deep neural networks have obtained state-of-the-art performance on many machine vision tasks. In this paper we provide an evaluation of 4 state-of-the-art deep neural network models for image classification under quality distortions. We consider five types of quality distortions: blur, noise, contrast, {JPEG}, and {JPEG}2000 compression. We show that the existing networks are susceptible to these quality distortions, particularly to blur and noise. These results enable future work in developing deep neural networks that are more invariant to quality distortions.},
	eventtitle = {{QoMEX} '16},
	pages = {1--6},
	booktitle = {8th International Conference on Quality of Multimedia Experience},
	author = {Dodge, Samuel and Karam, Lina},
	date = {2016-06},
	keywords = {Computer vision, Distortion, Image coding, Image quality, {JPEG}2000 compression, Neural networks, Neurons, Transform coding, computer vision, data compression, deep neural network model, high-quality image datasets, image classification, image coding, image quality, machine vision systems, neural nets, quality distortion},
}

@inproceedings{alserafi_towards_2016,
	location = {Barcelona, Spain},
	title = {Towards Information Profiling: Data Lake Content Metadata Management},
	isbn = {978-1-5090-5910-2},
	url = {http://ieeexplore.ieee.org/document/7836664/},
	doi = {10.1109/ICDMW.2016.0033},
	shorttitle = {Towards Information Profiling},
	abstract = {There is currently a burst of Big Data ({BD}) processed and stored in huge raw data repositories, commonly called Data Lakes ({DL}). These {BD} require new techniques of data integration and schema alignment in order to make the data usable by its consumers and to discover the relationships linking their content. This can be provided by metadata services which discover and describe their content. However, there is currently a lack of a systematic approach for such kind of metadata discovery and management. Thus, we propose a framework for the proﬁling of informational content stored in the {DL}, which we call information proﬁling. The proﬁles are stored as metadata to support data analysis. We formally deﬁne a metadata management process which identiﬁes the key activities required to effectively handle this. We demonstrate the alternative techniques and performance of our process using a prototype implementation handling a real-life case-study from the {OpenML} {DL}, which showcases the value and feasibility of our approach.},
	eventtitle = {{ICDMW} '16},
	pages = {178--185},
	booktitle = {16th {IEEE} International Conference on Data Mining Workshops},
	publisher = {{IEEE}},
	author = {Alserafi, Ayman and Abello, Alberto and Romero, Oscar and Calders, Toon},
	urldate = {2020-08-31},
	date = {2016-12},
	langid = {english},
}

@inproceedings{zeuch_nebulastream_2020,
	location = {Amsterdam, Netherlands},
	title = {The {NebulaStream} Platform: Data and Application Management for the Internet of Things},
	abstract = {The Internet of Things ({IoT}) presents a novel computing architecture for data management: a distributed, highly dynamic, and heterogeneous environment of massive scale. Applications for the {IoT} introduce new challenges for integrating the concepts of fog and cloud computing as well as sensor networks in one uniﬁed environment. In this paper, we highlight these major challenges and outline how existing systems handle them. To address these challenges, we introduce the {NebulaStream} platform, a general purpose, endto-end data management system for the {IoT}. {NebulaStream} addresses the heterogeneity and distribution of compute and data, supports diverse data and programming models going beyond relational algebra, deals with potentially unreliable communication, and enables constant evolution under continuous operation. In our evaluation, we demonstrate the eﬀectiveness of our approach by providing early results on partial aspects.},
	eventtitle = {{CIDR} '20},
	booktitle = {Conference on Innovative Data Systems Research},
	author = {Zeuch, Steffen and Chaudhary, Ankit and Monte, Bonaventura and Gavriilidis, Haralampos and Giouroukis, Dimitrios and Grulich, Philipp and Breß, Sebastian and Traub, Jonas and Markl, Volker},
	date = {2020},
}

@inproceedings{paola_effect_1995,
	location = {Firenze, Italy},
	title = {The Effect of Lossy Image Compression on Image Classification},
	doi = {10.1109/IGARSS.1995.519665},
	eventtitle = {{IGARSS} '95},
	pages = {118--120},
	booktitle = {International Geoscience and Remote Sensing Symposium},
	author = {Paola, Justin D. and Schowengerdt, Robert A.},
	date = {1995},
}

@inproceedings{deng_data_2017,
	location = {Chaminade, {CA}},
	title = {The Data Civilizer System},
	abstract = {In many organizations, it is often challenging for users to ﬁnd relevant data for speciﬁc tasks, since the data is usually scattered across the enterprise and often inconsistent. In fact, data scientists routinely report that the majority of their effort is spent ﬁnding, cleaning, integrating, and accessing data of interest to a task at hand. In order to decrease the “grunt work” needed to facilitate the analysis of data “in the wild”, we present {DATA} {CIVILIZER}, an end-to-end big data management system. {DATA} {CIVILIZER} has a linkage graph computation module to build a linkage graph for the data and a data discovery module which utilizes the linkage graph to help identify data that is relevant to user tasks. It also uses the linkage graph to discover possible join paths that can then be used in a query. For the actual query execution, we use a polystore {DBMS}, which federates query processing across disparate systems. In addition, {DATA} {CIVILIZER} integrates data cleaning operations into query processing. Because different users need to invoke the above tasks in different orders, {DATA} {CIVILIZER} embeds a workﬂow engine which enables the arbitrary composition of different modules, as well as the handling of data updates. We have deployed our preliminary {DATA} {CIVILIZER} system in two institutions, {MIT} and Merck and describe initial positive experiences that show the system shortens the time and effort required to ﬁnd, prepare, and analyze data.},
	eventtitle = {{CIDR} '17},
	pages = {7},
	booktitle = {8th Biennial Conference on Innovative Data Systems Research},
	author = {Deng, Dong and Fernandez, Raul Castro and Abedjan, Ziawasch and Wang, Sibo and Stonebraker, Michael and Elmagarmid, Ahmed and Ilyas, Ihab F and Madden, Samuel and Ouzzani, Mourad and Tang, Nan},
	date = {2017},
	langid = {english},
}

@inproceedings{kakaraparthy_case_2019,
	location = {Renton, {WA}},
	title = {The Case for Unifying Data Loading in Machine Learning Clusters},
	url = {https://www.usenix.org/conference/hotcloud19/presentation/kakaraparthy},
	eventtitle = {{HotCloud} '19},
	booktitle = {11th {USENIX} Workshop on Hot Topics in Cloud Computing},
	author = {Kakaraparthy, Aarati and Venkatesh, Abhay and Phanishayee, Amar and Venkataraman, Shivaram},
	date = {2019-07},
}

@inproceedings{abadi_tensorflow_2016,
	location = {Savannah, {GA}},
	title = {{TensorFlow}: A System for Large-Scale Machine Learning},
	abstract = {{TensorFlow} is a machine learning system that operates at large scale and in heterogeneous environments. {TensorFlow} uses dataﬂow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataﬂow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore {CPUs}, generalpurpose {GPUs}, and custom-designed {ASICs} known as Tensor Processing Units ({TPUs}). This architecture gives ﬂexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, {TensorFlow} enables developers to experiment with novel optimizations and training algorithms. {TensorFlow} supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use {TensorFlow} in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the {TensorFlow} dataﬂow model and demonstrate the compelling performance that {TensorFlow} achieves for several real-world applications.},
	eventtitle = {{OSDI} '16},
	pages = {265--283},
	booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	date = {2016},
	langid = {english},
}

@inproceedings{boehm_systemds_2020,
	location = {Amsterdam, Netherlands},
	title = {{SystemDS}: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle},
	abstract = {Machine learning ({ML}) applications become increasingly common in many domains. {ML} systems to execute these workloads include numerical computing frameworks and libraries, {ML} algorithm libraries, and specialized systems for deep neural networks and distributed {ML}. These systems focus primarily on eﬃcient model training and scoring. However, the data science process is exploratory, and deals with underspeciﬁed objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual eﬀort, and lacks optimization across the lifecycle. In this paper, we introduce {SystemDS}, an open source {ML} system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated {ML} model training, to debugging and serving. To this end, we aim to provide a stack of declarative language abstractions for the diﬀerent lifecycle tasks, and users with diﬀerent expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache {SystemML}), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.},
	eventtitle = {{CIDR} ‘20},
	pages = {8},
	booktitle = {10th Annual Conference on Innovative Data Systems Research},
	author = {Boehm, Matthias and Antonov, Iulian and Baunsgaard, Sebastian and Dokter, Mark and Ginthör, Robert and Innerebner, Kevin and Klezin, Florijan and Lindstaedt, Stefanie and Phani, Arnab and Rath, Benjamin and Reinwald, Berthold and Siddiqi, Shafaq and Wrede, Sebastian Benjamin},
	date = {2020},
	langid = {english},
}

@inproceedings{das_shield_2018,
	location = {London, United Kingdom},
	title = {{SHIELD}: Fast, Practical Defense and Vaccination for Deep Learning using {JPEG} Compression},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3219910},
	doi = {10.1145/3219819.3219910},
	shorttitle = {{SHIELD}},
	abstract = {The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks ({DNNs}) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place {JPEG} compression at the core of our proposed Shield defense framework, utilizing its capability to effectively “compress away” such pixel manipulation. To immunize a {DNN} model from artifacts introduced by compression, Shield “vaccinates” the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, Shield adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes Shield a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the {ImageNet} dataset, and show that our approaches eliminate up to 98\% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner’s L2 attack and {DeepFool}. Our approaches are fast and work without requiring knowledge about the model.},
	eventtitle = {{KDD} '18},
	pages = {196--204},
	booktitle = {24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	author = {Das, Nilaksh and Shanbhogue, Madhuri and Chen, Shang-Tse and Hohman, Fred and Li, Siwei and Chen, Li and Kounavis, Michael E. and Chau, Duen Horng},
	urldate = {2021-02-17},
	date = {2018-07-19},
	langid = {english},
}

@inproceedings{xie_self-training_2020,
	location = {Virtual Event},
	title = {Self-Training With Noisy Student Improves {ImageNet} Classification},
	doi = {10.1109/CVPR42600.2020.01070},
	abstract = {We present a simple self-training method that achieves 88.4\% top-1 accuracy on {ImageNet}, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves {ImageNet}-A top-1 accuracy from 61.0\% to 83.7\%, reduces {ImageNet}-C mean corruption error from 45.7 to 28.3, and reduces {ImageNet}-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an {EfficientNet} model on labeled {ImageNet} images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger {EfficientNet} as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via {RandAugment} to the student so that the student generalizes better than the teacher.},
	eventtitle = {{CVPR} '20},
	pages = {10684--10695},
	booktitle = {{IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	date = {2020-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Data models, Entropy, Image resolution, Noise measurement, Robustness, Stochastic processes, Training},
}

@inproceedings{chakroun_reviewing_2019,
	location = {Porto, Portugal},
	title = {Reviewing Data Access Patterns and Computational Redundancy for Machine Learning Algorithms},
	isbn = {978-989-8533-92-0},
	url = {http://www.iadisportal.org/digital-library/reviewing-data-access-patterns-and-computational-redundancy-for-machine-learning-algorithms},
	doi = {10.33965/bigdaci2019_201907L004},
	abstract = {Machine learning ({ML}) is probably the first and foremost used technique to deal with the size and complexity of the new generation of data. In this paper, we analyze one of the means to increase the performances of {ML} algorithms which is exploiting data locality. Data locality and access patterns are often at the heart of performance issues in computing systems due to the use of certain hardware techniques to improve performance. Altering the access patterns to increase locality can dramatically increase performance of a given algorithm. Besides, repeated data access can be seen as redundancy in data movement. Similarly, there can also be redundancy in the repetition of calculations. This work also identifies some of the opportunities for avoiding these redundancies by directly reusing computation results. We document the possibilities of such reuse in some selected machine learning algorithms and give initial indicative results from our first experiments on data access improvement and algorithm redesign.},
	eventtitle = {{BIGDACI} '19},
	pages = {31--38},
	booktitle = {International Conferences Big Data Analytics, Data Mining and Computational Intelligence},
	author = {Chakroun, Imen and Vander Aa, Tom and Ashby, Tom},
	urldate = {2020-08-27},
	date = {2019-07-16},
	langid = {english},
}

@inproceedings{rippel_real-time_2017,
	location = {Sydney, Australia},
	title = {Real-Time Adaptive Image Compression},
	abstract = {We present a machine learning-based approach to lossy image compression which outperforms all existing codecs, while running in real-time. Our algorithm typically produces ﬁles 2.5 times smaller than {JPEG} and {JPEG} 2000, 2 times smaller than {WebP}, and 1.7 times smaller than {BPG} on datasets of generic images across all quality levels. At the same time, our codec is designed to be lightweight and deployable: for example, it can encode or decode the Kodak dataset in around 10ms per image on {GPU}. Our architecture is an autoencoder featuring pyramidal analysis, an adaptive coding module, and regularization of the expected codelength. We also supplement our approach with adversarial training specialized towards use in a compression setting: this enables us to produce visually pleasing reconstructions for very low bitrates.},
	eventtitle = {{ICML} '17},
	pages = {2922--2930},
	booktitle = {International Conference on Machine Learning},
	author = {Rippel, Oren and Bourdev, Lubomir},
	date = {2017},
	langid = {english},
}

@inproceedings{erlingsson_rappor_2014,
	location = {Scottsdale, {AZ}},
	title = {{RAPPOR}: Randomized Aggregatable Privacy-Preserving Ordinal Response},
	isbn = {978-1-4503-2957-6},
	url = {https://dl.acm.org/doi/10.1145/2660267.2660348},
	doi = {10.1145/2660267.2660348},
	shorttitle = {{RAPPOR}},
	abstract = {Randomized Aggregatable Privacy-Preserving Ordinal Response, or {RAPPOR}, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, {RAPPORs} allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, {RAPPOR} provides the mechanisms for such collection as well as for eﬃcient, high-utility analysis of the collected data. In particular, {RAPPOR} permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports.},
	eventtitle = {{CCS} '14},
	pages = {1054--1067},
	booktitle = {{ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {{ACM}},
	author = {Erlingsson, Úlfar and Pihur, Vasyl and Korolova, Aleksandra},
	urldate = {2021-10-22},
	date = {2014-11-03},
	langid = {english},
}

@inproceedings{haochen_random_2019,
	location = {Long Beach, {CA}},
	title = {Random Shuffling Beats {SGD} after Finite Epochs},
	abstract = {A long-standing problem in optimization is proving that {RANDOMSHUFFLE}, the withoutreplacement version of {SGD}, converges faster than (the usual) with-replacement {SGD}. Building upon (Gu¨rbu¨zbalaban et al., 2015b), we present the ﬁrst non-asymptotic results for this problem, proving that after a reasonable number of epochs {RANDOMSHUFFLE} converges faster than {SGD}. Speciﬁcally, we prove that for strongly convex, second-order smooth functions, the iterates of {RANDOMSHUFFLE} converge to the optimal solution as O(1/T 2 + n3/T 3), where n is the number of components in the objective, and T is num√ber of iterations. This result implies that after O( n) epochs, {RANDOMSHUFFLE} is strictly better than {SGD} (which converges as O(1/T )). The key step toward showing this better dependence on T is the introduction of n into the bound; and as our analysis shows, in general a dependence on n is unavoidable without further changes. To understand how {RANDOMSHUFFLE} works in practice, we further explore two valuable settings: data sparsity and over-parameterization. For sparse data, {RANDOMSHUFFLE} has the rate O (1/T 2), again strictly better than {SGD}. Under a setting closely related to over-parameterization, {RANDOMSHUFFLE} is shown to converge faster than {SGD} after any arbitrary number of iterations. Finally, we extend the analysis of {RANDOMSHUFFLE} to smooth convex and some non-convex functions.},
	eventtitle = {{ICML} '19},
	pages = {10},
	booktitle = {36th International Conference on Machine Learning},
	author = {{HaoChen}, Jeff and Sra, Suvrit},
	date = {2019},
	langid = {english},
}

@inproceedings{kumar_quiver_2020,
	location = {Santa Clara, {CA}},
	title = {Quiver: An Informed Storage Cache for Deep Learning},
	isbn = {978-1-939133-12-0},
	url = {https://www.usenix.org/conference/fast20/presentation/kumar},
	abstract = {We introduce Quiver, an informed storage cache for deep learning training ({DLT}) jobs in a cluster of {GPUs}. Quiver employs domain-speciﬁc intelligence within the caching layer, to achieve much higher efﬁciency compared to a generic storage cache. First, Quiver uses a secure hash-based addressing to transparently reuse cached data across multiple jobs and even multiple users operating on the same dataset. Second, by codesigning with the deep learning framework (e.g., {PyTorch}), Quiver employs a technique of substitutable cache hits to get more value from the existing contents of the cache, thus avoiding cache thrashing when cache capacity is much smaller than the working set. Third, Quiver dynamically prioritizes cache allocation to jobs that beneﬁt the most from the caching. With a prototype implementation in {PyTorch}, we show that Quiver can signiﬁcantly improve throughput of deep learning workloads.},
	eventtitle = {{FAST} '20},
	pages = {283--296},
	booktitle = {18th {USENIX} Conference on File and Storage Technologies},
	author = {Kumar, Abhishek Vijaya and Sivathanu, Muthian},
	date = {2020-02},
	langid = {english},
}

@article{li_pytorch_2020,
	title = {{PyTorch} Distributed: Experiences on Accelerating Data Parallel Training},
	url = {http://arxiv.org/abs/2006.15704},
	shorttitle = {{PyTorch} Distributed},
	abstract = {This paper presents the design, implementation, and evaluation of the {PyTorch} distributed data parallel module. {PyTorch} is a widely-adopted scientiﬁc computing package used in deep learning research and applications. Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training eﬃciency. As of v1.5, {PyTorch} natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when conﬁgured appropriately, the {PyTorch} distributed data parallel module attains near-linear scalability using 256 {GPUs}.},
	journaltitle = {{arXiv}:2006.15704},
	author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
	urldate = {2020-11-22},
	date = {2020-06-28},
	langid = {english},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@inproceedings{karras_progressive_2018,
	location = {Vancouver, Canada},
	title = {Progressive Growing of {GANs} for Improved Quality, Stability, and Variation},
	url = {https://openreview.net/forum?id=Hk99zCeAb},
	eventtitle = {{ICLR} '18},
	booktitle = {International Conference on Learning Representations},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	date = {2018},
}

@inproceedings{narayanan_pipedream_2019,
	location = {Huntsville, Canada},
	title = {{PipeDream}: generalized pipeline parallelism for {DNN} training},
	isbn = {978-1-4503-6873-5},
	url = {https://dl.acm.org/doi/10.1145/3341301.3359646},
	doi = {10.1145/3341301.3359646},
	shorttitle = {{PipeDream}},
	abstract = {{DNN} training is extremely time-consuming, necessitating efficient multi-accelerator parallelization. Current approaches to parallelizing training primarily use intra-batch parallelization, where a single iteration of training is split over the available workers, but suffer from diminishing returns at higher worker counts. We present {PipeDream}, a system that adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible. Unlike traditional pipelining, {DNN} training is bi-directional, where a forward pass through the computation graph is followed by a backward pass that uses state and intermediate data computed during the forward pass. Naïve pipelining can thus result in mismatches in state versions used in the forward and backward passes, or excessive pipeline flushes and lower hardware efficiency. To address these challenges, {PipeDream} versions model parameters for numerically correct gradient computations, and schedules forward and backward passes of different minibatches concurrently on different workers with minimal pipeline stalls. {PipeDream} also automatically partitions {DNN} layers among workers to balance work and minimize communication. Extensive experimentation with a range of {DNN} tasks, models, and hardware configurations shows that {PipeDream} trains models to high accuracy up to 5.3× faster than commonly used intra-batch parallelism techniques.},
	eventtitle = {{SOSP} '19},
	pages = {1--15},
	booktitle = {27th {ACM} Symposium on Operating Systems Principles},
	publisher = {{ACM}},
	author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
	urldate = {2020-12-02},
	date = {2019-10-27},
	langid = {english},
}

@inproceedings{jain_performance_2019,
	location = {Albuquerque, {NM}},
	title = {Performance Characterization of {DNN} Training using {TensorFlow} and {PyTorch} on Modern Clusters},
	isbn = {978-1-72814-734-5},
	url = {https://ieeexplore.ieee.org/document/8891042/},
	doi = {10.1109/CLUSTER.2019.8891042},
	abstract = {The recent surge of Deep Learning ({DL}) models and applications can be attributed to the rise in computational resources, availability of large-scale datasets, and accessible {DL} frameworks such as {TensorFlow} and {PyTorch}. Because these frameworks have been heavily optimized for {NVIDIA} {GPUs}, several performance characterization studies exist for {GPU}-based Deep Neural Network ({DNN}) training. However, there exist very few research studies that focus on {CPU}-based {DNN} training. In this paper, we provide an in-depth performance characterization of state-of-the-art {DNNs} such as {ResNet}(s) and Inception-v3/v4 on multiple {CPU} architectures including Intel Xeon Broadwell, three variants of the Intel Xeon Skylake, {AMD} {EPYC}, and {NVIDIA} {GPUs} like K80, P100, and V100. We provide three key insights: 1) Multi-process ({MP}) training should be used even for a single-node, because the single-process ({SP}) approach cannot fully exploit all the cores, 2) Performance of both {SP} and {MP} depend on various features such as the number of cores, the processes per node (ppn), and {DNN} architecture, and 3) There is a non-linear and complex relationship between {CPU}/system characteristics (core-count, ppn, hyper-threading, etc) and {DNN} speciﬁcations such as inherent parallelism between layers. We further provide a comparative analysis for {CPU} and {GPU}-based training and proﬁling analysis for Horovod. The fastest Skylake we had access to is up to 2.35× better than a K80 {GPU} but up to 3.32× slower than a V100 {GPU}. For {ResNet}-152 training, we observed that {MP} is up to 1.47× faster than {SP} and achieves 125× speedup on 128 Skylake nodes.},
	eventtitle = {{CLUSTER}},
	pages = {1--11},
	booktitle = {{IEEE} International Conference on Cluster Computing},
	publisher = {{IEEE}},
	author = {Jain, Arpan and Awan, Ammar Ahmad and Anthony, Quentin and Subramoni, Hari and Panda, Dhableswar K. {DK}},
	urldate = {2020-08-25},
	date = {2019-09},
	langid = {english},
}

@inproceedings{gulzar_perception_2019,
	location = {Montreal, Canada},
	title = {Perception and Practices of Differential Testing},
	isbn = {978-1-72811-760-7},
	url = {https://ieeexplore.ieee.org/document/8804465/},
	doi = {10.1109/ICSE-SEIP.2019.00016},
	abstract = {Tens of thousands engineers are contributing to Google’s codebase that spans billions of lines of code. To ensure high code quality, tremendous amount of eﬀort has been made with new testing techniques and frameworks. However, with increasingly complex data structures and software systems, traditional test case based testing strategies cannot scale well to achieve the desired level of test adequacy. Diﬀerential (Diﬀ) testing is one of the new testing techniques adapted to ﬁll this gap. It uses the same input to run two versions of a software system, namely base and test, where base is the veriﬁed/tested version of the system while test is the modiﬁed version. The output of two runs are then thoroughly compared to ﬁnd abnormalities that may lead to possible bugs.},
	eventtitle = {{ICSE}-{SEIP} '19},
	pages = {71--80},
	booktitle = {41st {IEEE}/{ACM} International Conference on Software Engineering: Software Engineering in Practice},
	publisher = {{IEEE}},
	author = {Gulzar, Muhammad Ali and Zhu, Yongkang and Han, Xiaofeng},
	urldate = {2020-09-16},
	date = {2019-05},
	langid = {english},
}

@inproceedings{cong_partial_2020,
	title = {Partial data permutation for training deep neural networks},
	doi = {10.1109/CCGrid49817.2020.00-17},
	abstract = {Random data permutation is considered as a best practice for training deep neural networks. When the input is large, permuting the full dataset is costly and limits scaling on distributed systems. Some practitioners use partial or no permutation that may potentially result in poor convergence. We propose a partitioned data permutation scheme as a low-cost alternative to full data permutation. Analyzing their entropy, we show that the two sampling schemes are asymptotically identical. We also show with minibatch {SGD}, both sampling schemes produce unbiased estimators of the true gradient. In addition, they have the same bound on the second moment of the gradient. Thus they have similar convergence properties. Our experiments confirm that {SGD} has similar training performance in practice with both sampling schemes. We further show that due to inherent randomness such as data augmentation and dropout in the training, even faster sampling schemes than partial permutation such as sequential sampling can achieve good performance. However, if no extra randomness is present in training, sampling schemes with low entropy can indeed degrade performance significantly.},
	eventtitle = {{CCGRID} '20},
	pages = {728--735},
	booktitle = {20th {IEEE}/{ACM} International Symposium on Cluster, Cloud and Internet Computing},
	author = {Cong, Guojing and Zhang, Li and Yang, Chih-Chieh},
	date = {2020-05},
	keywords = {Best practices, Convergence, Entropy, Loading, Machine learning, Neural networks, Training, data augmentation, gradient methods, learning (artificial intelligence), neural nets, partial data permutation, partitioned data permutation scheme, random data permutation, random processes, sampling methods, sampling schemes},
}

@inproceedings{poyser_impact_2021,
	location = {Milan, Italy},
	title = {On the Impact of Lossy Image and Video Compression on the Performance of Deep Convolutional Neural Network Architectures},
	doi = {10.1109/ICPR48806.2021.9412455},
	abstract = {Recent advances in generalized image understanding have seen a surge in the use of deep convolutional neural networks ({CNN}) across a broad range of image-based detection, classification and prediction tasks. Whilst the reported performance of these approaches is impressive, this study investigates the hitherto unapproached question of the impact of commonplace image and video compression techniques on the performance of such deep learning architectures. Focusing on the {JPEG} and H.264 ({MPEG}-4 {AVC}) as a representative proxy for contemporary lossy image/video compression techniques that are in common use within network-connected image/video devices and infrastructure, we examine the impact on performance across five discrete tasks: human pose estimation, semantic segmentation, object detection, action recognition, and monocular depth estimation. As such, within this study we include a variety of network architectures and domains spanning end-to-end convolution, encoder-decoder, region-based {CNN} (R-{CNN}), dual-stream, and generative adversarial networks ({GAN}). Our results show a non-linear and non-uniform relationship between network performance and the level of lossy compression applied. Notably, performance decreases significantly below a {JPEG} quality (quantization) level of 15\% and a H.264 Constant Rate Factor ({CRF}) of 40. However, retraining said architectures on pre-compressed imagery conversely recovers network performance by up to 78.4\% in some cases. Furthermore, there is a correlation between architectures employing an encoder-decoder pipeline and those that demonstrate resilience to lossy image compression. The characteristics of the relationship between input compression to output task performance can be used to inform design decisions within future image/video devices and infrastructure.},
	eventtitle = {{ICPR} '21},
	pages = {2830--2837},
	booktitle = {25th International Conference on Pattern Recognition},
	author = {Poyser, Matt and Atapour-Abarghouei, Amir and Breckon, Toby P.},
	date = {2021-01},
	note = {{ISSN}: 1051-4651},
	keywords = {Image coding, Image segmentation, Network architecture, Performance evaluation, Pose estimation, Transform coding, Video compression},
}

@inproceedings{babcock_models_2002,
	location = {Madison, {WI}},
	title = {Models and Issues in Data Stream Systems},
	isbn = {1-58113-507-6},
	url = {https://doi.org/10.1145/543613.543615},
	doi = {10.1145/543613.543615},
	abstract = {In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues.},
	eventtitle = {{PODS} '02},
	pages = {1--16},
	booktitle = {21st {ACM} Symposium on Principles of Database Systems},
	author = {Babcock, Brian and Babu, Shivnath and Datar, Mayur and Motwani, Rajeev and Widom, Jennifer},
	date = {2002},
}

@inproceedings{mitchell_model_2019,
	location = {Atlanta, {GA}},
	title = {Model Cards for Model Reporting},
	url = {http://arxiv.org/abs/1810.03993},
	doi = {10.1145/3287560.3287596},
	abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
	eventtitle = {{FAT}* '19},
	pages = {220--229},
	booktitle = {Conference on Fairness, Accountability, and Transparency},
	author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
	urldate = {2020-08-31},
	date = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.03993},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{mattson_mlperf_2020,
	location = {Austin, {TX}},
	title = {{MLPerf} Training Benchmark},
	url = {http://arxiv.org/abs/1910.01500},
	abstract = {Machine learning ({ML}) needs industry-standard performance benchmarks to support design and competitive evaluation of the many emerging software and hardware solutions for {ML}. But {ML} training presents three unique benchmarking challenges absent from other domains: optimizations that improve training throughput can increase the time to solution, training is stochastic and time to solution exhibits high variance, and software and hardware systems are so diverse that fair benchmarking with the same binary, code, and even hyperparameters is difﬁcult. We therefore present {MLPerf}, an {ML} benchmark that overcomes these challenges. Our analysis quantitatively evaluates {MLPerf}’s efﬁcacy at driving performance and scalability improvements across two rounds of results from multiple vendors.},
	eventtitle = {{MLSys} '20},
	booktitle = {3rd {MLSys} Conference},
	author = {Mattson, Peter and Cheng, Christine and Coleman, Cody and Diamos, Greg and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and Brooks, David and Chen, Dehao and Dutta, Debojyoti and Gupta, Udit and Hazelwood, Kim and Hock, Andrew and Huang, Xinyuan and Ike, Atsushi and Jia, Bill and Kang, Daniel and Kanter, David and Kumar, Naveen and Liao, Jeffery and Ma, Guokai and Narayanan, Deepak and Oguntebi, Tayo and Pekhimenko, Gennady and Pentecost, Lillian and Reddi, Vijay Janapa and Robie, Taylor and John, Tom St and Tabaru, Tsuguchika and Wu, Carole-Jean and Xu, Lingjie and Yamazaki, Masafumi and Young, Cliff and Zaharia, Matei},
	urldate = {2021-04-23},
	date = {2020-03-02},
	langid = {english},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance, Statistics - Machine Learning},
}

@inproceedings{theis_lossy_2017,
	location = {Toulon, France},
	title = {Lossy Image Compression with Compressive Autoencoders},
	url = {http://arxiv.org/abs/1703.00395},
	abstract = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more ﬂexible than existing codecs. Autoencoders have the potential to address this need, but are difﬁcult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufﬁcient to train deep autoencoders competitive with {JPEG} 2000 and outperforming recently proposed approaches based on {RNNs}. Our network is furthermore computationally efﬁcient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
	eventtitle = {{ICLR} '17},
	booktitle = {International Conference on Learning Representations},
	author = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
	urldate = {2021-09-15},
	date = {2017},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.00395},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@inproceedings{moore_lithography_1995,
	title = {Lithography and the future of Moore's law},
	volume = {2439},
	eventtitle = {Integrated Circuit Metrology, Inspection, and Process Control {IX}},
	pages = {2--17},
	booktitle = {International Society for Optics and Photonics},
	author = {Moore, Gordon E},
	date = {1995},
}

@inproceedings{hashemi_learning_2018,
	location = {Stockholm, Schweden},
	title = {Learning Memory Access Patterns},
	abstract = {The explosion in workload complexity and the recent slow-down in Moore’s law scaling call for new approaches towards efﬁcient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efﬁcient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we ﬁnd that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the ﬁrst step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.},
	eventtitle = {{ICML} '18},
	pages = {10},
	booktitle = {35th International Conference on Machine Learning},
	author = {Hashemi, Milad and Swersky, Kevin and Smith, Jamie A and Ayers, Grant and Litz, Heiner and Chang, Jichuan and Kozyrakis, Christos and Ranganathan, Parthasarathy},
	date = {2018},
	langid = {english},
}

@inproceedings{reddi_mlperf_2020,
	location = {Virtual Event},
	title = {{MLPerf} Inference Benchmark},
	doi = {10.1109/ISCA45697.2020.00045},
	abstract = {Machine-learning ({ML}) hardware and software system demand is burgeoning. Driven by {ML} applications, the number of different {ML} inference systems has exploded. Over 100 organizations are building {ML} inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of {ML} hardware and {ML} software make assessing {ML}-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard {ML} benchmarking and evaluation criteria. {MLPerf} Inference answers that call. In this paper, we present our benchmarking method for evaluating {ML} inference systems. Driven by more than 30 organizations as well as more than 200 {ML} engineers and practitioners, {MLPerf} prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.},
	eventtitle = {{ISCA} '20},
	pages = {446--459},
	booktitle = {47th {ACM}/{IEEE} Annual International Symposium on Computer Architecture},
	author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
	date = {2020-05},
	keywords = {Benchmarking, Inference, Machine Learning},
}

@inproceedings{verma_demystifying_2020,
	location = {Virtual Meeting},
	title = {Demystifying the {MLPerf} Training Benchmark Suite},
	doi = {10.1109/ISPASS48437.2020.00013},
	abstract = {{MLPerf}, an emerging machine learning benchmark suite, strives to cover a broad range of machine learning applications. We present a study on the characteristics of {MLPerf} benchmarks and how they differ from previous deep learning benchmarks such as {DAWNBench} and {DeepBench}. {MLPerf} benchmarks are seen to exhibit moderately high memory transactions per second and moderately high compute rates, while {DAWNBench} creates a high-compute benchmark with low memory transaction rate, and {DeepBench} provides low compute rate benchmarks. We also observe that the various {MLPerf} benchmarks possess unique features that allow unveiling various bottlenecks in systems. We also observe variation in scaling efficiency across the {MLPerf} models. The variation exhibited by the different models highlight the importance of smart scheduling strategies for multi-{GPU} training. Another observation is that dedicated low latency interconnect between {GPUs} in multi-{GPU} systems is crucial for optimal distributed deep learning training. Furthermore, host {CPU} utilization increases with an increase in the number of {GPUs} used for training. Corroborating prior work, we also observe and quantify improvements possible by mixed-precision training using Tensor Cores.},
	eventtitle = {{ISPASS} '20},
	pages = {24--33},
	booktitle = {{IEEE} International Symposium on Performance Analysis of Systems and Software},
	author = {Verma, Snehil and Wu, Qinzhe and Hanindhito, Bagus and Jha, Gunjan and John, Eugene B. and Radhakrishnan, Ramesh and John, Lizy K.},
	date = {2020-08},
	keywords = {Benchmarking, Machine Learning, Training},
}

@inproceedings{cheng_learned_2020,
	location = {Virtual Event},
	title = {Learned Image Compression With Discretized Gaussian Mixture Likelihoods and Attention Modules},
	doi = {10.1109/CVPR42600.2020.00796},
	abstract = {Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used {PSNR} metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding ({VVC}) regarding {PSNR}. More importantly, our approach generates more visually pleasant results when optimized by {MS}-{SSIM}.},
	eventtitle = {{CVPR} '20},
	pages = {7936--7945},
	booktitle = {{IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	author = {Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
	date = {2020-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Entropy, Image coding, Redundancy, Standards, Training, Transform coding, Visualization},
}

@inproceedings{patterson_informed_1995,
	location = {Copper Mountain, {CO}},
	title = {Informed Prefetching and Caching},
	isbn = {0-89791-715-4},
	url = {https://doi.org/10.1145/224056.224064},
	doi = {10.1145/224056.224064},
	eventtitle = {{SOSP} '95},
	pages = {79--95},
	booktitle = {15th {ACM} Symposium on Operating Systems Principles},
	author = {Patterson, R. Hugo and Gibson, Garth A. and Ginting, Eka and Stodolsky, Daniel and Zelenka, Jim},
	date = {1995},
}

@inproceedings{tomkins_informed_1997,
	location = {Seattle, {WA}},
	title = {Informed Multi-Process Prefetching and Caching},
	isbn = {0-89791-909-2},
	url = {https://doi.org/10.1145/258612.258680},
	doi = {10.1145/258612.258680},
	abstract = {Informed prefetching and caching based on application disclosure of future I/O accesses (hints) can dramatically reduce the execution time of I/O-intensive applications. A recent study showed that, in the context of a single hinting application, prefetching and caching algorithms should adapt to the dynamic load on the disks to obtain the best performance. In this paper, we show how to incorporate adaptivity to disk load into the {TIP}2 system, which uses cost-benefit analysis to allocate global resources among multiple processes. We compare the resulting system, which we call {TIPTOE} ({TIP} with Temporal Overload Estimators) to Cao et al's {LRU}-{SP} allocation scheme, also modified to include adaptive prefetching. Using disk-accurate trace-driven simulation we show that, averaged over eleven experiments involving pairs of hinting applications, and with data striped over one to ten disks, {TIPTOE} delivers 7\% lower execution time than {LRU}-{SP}. Where the computation and I/O demands of each experiment are closely matched, in a two-disk array, {TIPTOE} delivers 18\% lower execution time.},
	eventtitle = {{SIGMETRICS} '97},
	pages = {100--114},
	booktitle = {{ACM} {SIGMETRICS} International Conference on Measurement and Modeling of Computer Systems},
	author = {Tomkins, Andrew and Patterson, R. Hugo and Gibson, Garth},
	date = {1997},
}

@inproceedings{toderici_full_2017,
	location = {Honolulu, {HI}},
	title = {Full Resolution Image Compression with Recurrent Neural Networks},
	doi = {10.1109/CVPR.2017.577},
	abstract = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network ({RNN})-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare {RNN} types ({LSTM}, associative {LSTM}) and introduce a new hybrid of {GRU} and {ResNet}. We also study “one-shot” versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3\%-8.8\% {AUC} (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform {JPEG} at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
	eventtitle = {{CVPR} '17},
	pages = {5435--5443},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
	date = {2017-07},
	note = {{ISSN}: 1063-6919},
	keywords = {Additives, Convolutional codes, Decoding, Image coding, Image reconstruction, Kernel, Neural networks},
}

@inproceedings{li_federated_2020,
	location = {Austin, {TX}},
	title = {Federated Optimization in Heterogeneous Networks},
	url = {https://proceedings.mlsys.org/paper/2020/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf},
	eventtitle = {{MLSys} '20},
	booktitle = {3rd {MLSys} Conference},
	author = {Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
	editor = {Dhillon, I. and Papailiopoulos, D. and Sze, V.},
	date = {2020},
}

@inproceedings{adolf_fathom_2016,
	location = {Providence, {RI}},
	title = {Fathom: Reference Workloads for Modern Deep Learning Methods},
	isbn = {978-1-5090-3896-1},
	url = {http://ieeexplore.ieee.org/document/7581275/},
	doi = {10.1109/IISWC.2016.7581275},
	shorttitle = {Fathom},
	abstract = {Deep learning has been popularized by its recent successes on challenging artiﬁcial intelligence problems. One of the reasons for its dominance is also an ongoing challenge: the need for immense amounts of computational power. Hardware architects have responded by proposing a wide array of promising ideas, but to date, the majority of the work has focused on speciﬁc algorithms in somewhat narrow application domains. While their speciﬁcity does not diminish these approaches, there is a clear need for more ﬂexible solutions. We believe the ﬁrst step is to examine the characteristics of cutting edge models from across the deep learning community.},
	eventtitle = {{IISWC} '16},
	pages = {1--10},
	booktitle = {{IEEE} International Symposium on Workload Characterization},
	author = {Adolf, Robert and Rama, Saketh and Reagen, Brandon and Wei, Gu-Yeon and Brooks, David},
	urldate = {2020-08-25},
	date = {2016-09},
	langid = {english},
}

@inproceedings{raasveldt_fair_2018,
	location = {Houston, {TX}},
	title = {Fair Benchmarking Considered Difficult: Common Pitfalls In Database Performance Testing},
	isbn = {978-1-4503-5826-2},
	url = {http://dl.acm.org/citation.cfm?doid=3209950.3209955},
	doi = {10.1145/3209950.3209955},
	shorttitle = {Fair Benchmarking Considered Difficult},
	abstract = {Performance benchmarking is one of the most commonly used methods for comparing different systems or algorithms, both in scientific literature and in industrial publications. While performance measurements might seem objective on the surface, there are many different ways to influence benchmark results to favor one system over the other, either by accident or on purpose. In this paper, we perform a study of the common pitfalls in {DBMS} performance comparisons, and give advice on how they can be spotted and avoided so a fair performance comparison between systems can be made. We illustrate the common pitfalls with a series of mock benchmarks, which show large differences in performance where none should be present.},
	eventtitle = {{DBTest} '18},
	pages = {1--6},
	booktitle = {Workshop on Testing Database Systems},
	publisher = {{ACM} Press},
	author = {Raasveldt, Mark and Holanda, Pedro and Gubner, Tim and Mühleisen, Hannes},
	urldate = {2020-10-12},
	date = {2018},
	langid = {english},
}

@inproceedings{lim_enhanced_2017,
	location = {Honolulu, {HI}},
	title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
	doi = {10.1109/CVPRW.2017.151},
	abstract = {Recent research on super-resolution has progressed with the development of deep convolutional neural networks ({DCNN}). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network ({EDSR}) with performance exceeding those of current state-of-the-art {SR} methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system ({MDSR}) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the {NTIRE}2017 Super-Resolution Challenge[26].},
	eventtitle = {{CVPR} '17},
	pages = {1132--1140},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition Workshops},
	author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
	date = {2017-07},
	note = {{ISSN}: 2160-7516},
	keywords = {Computational modeling, Computer architecture, Convolution, Image reconstruction, Image resolution, Signal resolution, Training},
}

@inproceedings{balle_end--end_2017,
	location = {Toulon, France},
	title = {End-to-end Optimized Image Compression},
	url = {http://arxiv.org/abs/1611.01704},
	abstract = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear ﬁlters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate–distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate–distortion curve, as speciﬁed by a trade-off parameter. Across an independent set of test images, we ﬁnd that the optimized method generally exhibits better rate–distortion performance than the standard {JPEG} and {JPEG} 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using {MS}-{SSIM}.},
	eventtitle = {{ICLR} '17},
	booktitle = {International Conference on Learning Representations},
	author = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
	urldate = {2021-09-15},
	date = {2017},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.01704},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory},
}

@inproceedings{zhang_quick_2018,
	location = {Singapore},
	title = {A Quick Survey on Large Scale Distributed Deep Learning Systems},
	doi = {10.1109/PADSW.2018.8644613},
	eventtitle = {{ICPADS} '18},
	pages = {1052--1056},
	booktitle = {2018 {IEEE} 24th International Conference on Parallel and Distributed Systems},
	author = {Zhang, Zhaoning and Yin, Lujia and Peng, Yuxing and Li, Dongsheng},
	date = {2018},
}

@article{terrizzano_data_2015,
	title = {Data Wrangling: The Challenging Journey from the Wild to the Lake},
	abstract = {Much has been written about the explosion of data, also known as the “data deluge”. Similarly, much of today's research and decision making are based on the de facto acceptance that knowledge and insight can be gained from analyzing and contextualizing the vast (and growing) amount of “open” or “raw” data. The concept that the large number of data sources available today facilitates analyses on combinations of heterogeneous information that would not be achievable via “siloed” data maintained in warehouses is very powerful. The term data lake has been coined to convey the concept of a centralized repository containing virtually inexhaustible amounts of raw (or minimally curated) data that is readily made available anytime to anyone authorized to perform analytical activities. The often unstated premise of a data lake is that it relieves users from dealing with data acquisition and maintenance issues, and guarantees fast access to local, accurate and updated data without incurring development costs (in terms of time and money) typically associated with structured data warehouses. However appealing this premise, practically speaking, it is our experience, and that of our customers, that “raw” data is logistically difficult to obtain, quite challenging to interpret and describe, and tedious to maintain. Furthermore, these challenges multiply as the number of sources grows, thus increasing the need to thoroughly describe and curate the data in order to make it consumable. In this paper, we present and describe some of the challenges inherent in creating, filling, maintaining, and governing a data lake, a set of processes that collectively define the actions of data wrangling, and we propose that what is really needed is a curated data lake, where the lake contents have undergone a curation process that enable its use and deliver the promise of ad-hoc data accessibility to users beyond the enterprise {IT} staff.},
	pages = {9},
	author = {Terrizzano, Ignacio and Schwarz, Peter and Roth, Mary and Colino, John E},
	date = {2015-01-04},
	langid = {english},
}

@inproceedings{wang_diesel_2020,
	location = {Edmonton, Canada},
	title = {{DIESEL}: A Dataset-Based Distributed Storage and Caching System for Large-Scale Deep Learning Training},
	isbn = {978-1-4503-8816-0},
	url = {https://dl.acm.org/doi/10.1145/3404397.3404472},
	doi = {10.1145/3404397.3404472},
	shorttitle = {{DIESEL}},
	abstract = {We observe three problems in existing storage and caching systems for deep-learning training ({DLT}) tasks: (1) accessing a dataset containing a large number of small files takes a long time, (2) global in-memory caching systems are vulnerable to node failures and slow to recover, and (3) repeatedly reading a dataset of files in shuffled orders is inefficient when the dataset is too large to be cached in memory. Therefore, we propose {DIESEL}, a dataset-based distributed storage and caching system for {DLT} tasks. Our approach is via a storage-caching system co-design. Firstly, since accessing small files is a metadata-intensive operation, {DIESEL} decouples the metadata processing from metadata storage, and introduces metadata snapshot mechanisms for each dataset. This approach speeds up metadata access significantly. Secondly, {DIESEL} deploys a task-grained distributed cache across the worker nodes of a {DLT} task. This way node failures are contained within each {DLT} task. Furthermore, the files are grouped into large chunks in storage, so the recovery time of the caching system is reduced greatly. Thirdly, {DIESEL} provides chunk-based shuffle so that the performance of random file access is improved without sacrificing training accuracy. Our experiments show that {DIESEL} achieves a linear speedup on metadata access, and outperforms an existing distributed caching system in both file caching and file reading. In real {DLT} tasks, {DIESEL} halves the data access time of an existing storage system, and reduces the training time by hours without changing any training code.},
	eventtitle = {{ICPP} '20},
	pages = {1--11},
	booktitle = {49th International Conference on Parallel Processing},
	author = {Wang, Lipeng and Ye, Songgao and Yang, Baichen and Lu, Youyou and Zhang, Hequan and Yan, Shengen and Luo, Qiong},
	urldate = {2020-09-21},
	date = {2020-08-17},
	langid = {english},
}

@inproceedings{he_delving_2015,
	location = {Santiago, Chile},
	title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on {ImageNet} Classification},
	isbn = {978-1-4673-8391-2},
	url = {https://doi.org/10.1109/ICCV.2015.123},
	doi = {10.1109/ICCV.2015.123},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit ({PReLU}) that generalizes the traditional rectified unit. {PReLU} improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the {ImageNet} 2012 classification dataset. This is a 26\% relative improvement over the {ILSVRC} 2014 winner ({GoogLeNet}, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
	eventtitle = {{ICCV} '15},
	pages = {1026--1034},
	booktitle = {{IEEE} International Conference on Computer Vision},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	date = {2015},
}

@inproceedings{he_deep_2016,
	location = {Las Vegas, {NV}},
	title = {Deep Residual Learning for Image Recognition},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than {VGG} nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classiﬁcation task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers.},
	eventtitle = {{CVPR} '16},
	pages = {770--778},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2020-09-09},
	date = {2016-06},
	langid = {english},
}

@inproceedings{read_deep_2015,
	location = {Salamanca, Spain},
	title = {Deep Learning in Partially-Labeled Data Streams},
	isbn = {978-1-4503-3196-8},
	url = {https://doi.org/10.1145/2695664.2695871},
	doi = {10.1145/2695664.2695871},
	abstract = {Of the considerable research on data streams, relatively little deals with classification where only some of the instances in the stream are labeled. Most state-of-the-art data-stream algorithms do not have an effective way of dealing with unlabeled instances from the same domain. In this paper we explore deep learning techniques that provide important advantages such as the ability to learn incrementally in constant memory, and from unlabeled examples. We develop two deep learning methods and explore empirically via a series of empirical evaluations the application to several data streams scenarios based on real data. We find that our methods can offer competitive accuracy as compared with existing popular data-stream learners.},
	eventtitle = {{SAC} '15},
	pages = {954--959},
	booktitle = {30th {ACM} Symposium on Applied Computing},
	author = {Read, Jesse and Perez-Cruz, Fernando and Bifet, Albert},
	date = {2015},
}

@inproceedings{coleman_dawnbench_2017,
	location = {Long Beach, {CA}},
	title = {{DAWNBench}: An End-to-End Deep Learning Benchmark and Competition},
	abstract = {Despite considerable research on systems, algorithms and hardware to speed up deep learning workloads, there is no standard means of evaluating end-to-end deep learning performance. Existing benchmarks measure proxy metrics, such as time to process one minibatch of data, that do not indicate whether the system as a whole will produce a high-quality result. In this work, we introduce {DAWNBench}, a benchmark and competition focused on end-to-end training time to achieve a state-of-the-art accuracy level, as well as inference time with that accuracy. Using time to accuracy as a target metric, we explore how different optimizations, including choice of optimizer, stochastic depth, and multi-{GPU} training, affect end-to-end training performance. Our results demonstrate that optimizations can interact in non-trivial ways when used in conjunction, producing lower speed-ups and less accurate models. We believe {DAWNBench} will provide a useful, reproducible means of evaluating the many trade-offs in deep learning systems.},
	eventtitle = {{NIPS} '17},
	booktitle = {Proceedings of the 31st Conference on Neural Information Processing Systems},
	author = {Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and Ré, Chris and Zaharia, Matei},
	date = {2017},
	langid = {english},
}

@inproceedings{meng_convergence_2017,
	location = {Long Beach, {CA}},
	title = {Convergence Analysis of Distributed Stochastic Gradient Descent with Shuffling},
	url = {http://arxiv.org/abs/1709.10432},
	abstract = {When using stochastic gradient descent ({SGD}) to solve large-scale machine learning problems, a common practice of data processing is to shufﬂe the training data, partition the data across multiple threads/machines if needed, and then perform several epochs of training on the re-shufﬂed (either locally or globally) data. The above procedure makes the instances used to compute the gradients no longer independently sampled from the training data set, which contradicts with the basic assumptions of conventional convergence analysis of {SGD}. Then does the distributed {SGD} method have desirable convergence properties in this practical situation? In this paper, we give answers to this question. First, we give a mathematical formulation for the practical data processing procedure in distributed machine learning, which we call (data partition with) global/local shufﬂing. We observe that global shufﬂing is equivalent to without-replacement sampling if the shufﬂing operations are independent. We prove that {SGD} with global shufﬂing has convergence guarantee in both convex and non-convex cases. An interesting ﬁnding is that, the non-convex tasks like deep learning are more suitable to apply shufﬂing comparing to the convex tasks. Second, we conduct the convergence analysis for {SGD} with local shufﬂing. The convergence rate for local shufﬂing is slower than that for global shufﬂing, since it will lose some information if there’s no communication between partitioned data. Finally, we consider the situation when the permutation after shufﬂing is not uniformly distributed (We call it insufﬁcient shufﬂing), and discuss the condition under which this insufﬁciency will not inﬂuence the convergence rate. Our theoretical results provide important insights to large-scale machine learning, especially in the selection of data processing methods in order to achieve faster convergence and good speedup. Our theoretical ﬁndings are veriﬁed by extensive experiments on logistic regression and deep neural networks.},
	eventtitle = {{NIPS} '17},
	booktitle = {31st Conference on Neural Information Processing Systems},
	author = {Meng, Qi and Chen, Wei and Wang, Yue and Ma, Zhi-Ming and Liu, Tie-Yan},
	urldate = {2020-08-31},
	date = {2017-09-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.10432},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{jiang_clock-pro_2005,
	location = {Anaheim, {CA}},
	title = {{CLOCK}-Pro: An Effective Improvement of the {CLOCK} Replacement},
	abstract = {With the ever-growing performance gap between memory systems and disks, and rapidly improving {CPU} performance, virtual memory ({VM}) management becomes increasingly important for overall system performance. However, one of its critical components, the page replacement policy, is still dominated by {CLOCK}, a replacement policy developed almost 40 years ago. While pure {LRU} has an unaffordable cost in {VM}, {CLOCK} simulates the {LRU} replacement algorithm with a low cost acceptable in {VM} management. Over the last three decades, the inability of {LRU} as well as {CLOCK} to handle weak locality accesses has become increasingly serious, and an effective ﬁx becomes increasingly desirable.},
	eventtitle = {{ATC} '05},
	pages = {323--336},
	booktitle = {{USENIX} Annual Technical Conference},
	author = {Jiang, Song and Chen, Feng and Zhang, Xiaodong},
	date = {2005},
	langid = {english},
}

@article{chen_big_2020,
	title = {Big Self-Supervised Models are Strong Semi-Supervised Learners},
	url = {http://arxiv.org/abs/2006.10029},
	abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on {ImageNet}. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big {ResNet} model using {SimCLRv}2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% {ImageNet} top-1 accuracy with just 1\% of the labels (\${\textbackslash}le\$13 labeled images per class) using {ResNet}-50, a \$10{\textbackslash}times\$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, {ResNet}-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
	journaltitle = {{arXiv}:2006.10029},
	author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
	urldate = {2021-09-08},
	date = {2020-10-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.10029},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_benchnn_2012,
	location = {La Jolla, {CA}},
	title = {{BenchNN}: On the Broad Potential Application Scope of Hardware Neural Network Accelerators},
	isbn = {978-1-4673-4532-3 978-1-4673-4531-6},
	url = {http://ieeexplore.ieee.org/document/6402898/},
	doi = {10.1109/IISWC.2012.6402898},
	shorttitle = {{BenchNN}},
	abstract = {Recent technology trends have indicated that, although device sizes will continue to scale as they have in the past, supply voltage scaling has ended. As a result, future chips can no longer rely on simply increasing the operational core count to improve performance without surpassing a reasonable power budget. Alternatively, allocating die area towards accelerators targeting an application, or an application domain, appears quite promising, and this paper makes an argument for a neural network hardware accelerator. After being hyped in the 1990s, then fading away for almost two decades, there is a surge of interest in hardware neural networks because of their energy and fault-tolerance properties. At the same time, the emergence of high-performance applications like Recognition, Mining, and Synthesis ({RMS}) suggest that the potential application scope of a hardware neural network accelerator would be broad. In this paper, we want to highlight that a hardware neural network accelerator is indeed compatible with many of the emerging high-performance workloads, currently accepted as benchmarks for high-performance micro-architectures. For that purpose, we develop and evaluate software neural network implementations of 5 (out of 12) {RMS} applications from the {PARSEC} Benchmark Suite. Our results show that neural network implementations can achieve competitive results, with respect to application-speciﬁc quality metrics, on these 5 {RMS} applications.},
	eventtitle = {{IISWC} '12},
	pages = {36--45},
	booktitle = {{IEEE} International Symposium on Workload Characterization},
	publisher = {{IEEE}},
	author = {Chen, Tianshi and Chen, Yunji and Duranton, Marc and Guo, Qi and Hashmi, Atif and Lipasti, Mikko and Nere, Andrew and Qiu, Shi and Sebag, Michele and Temam, Olivier},
	urldate = {2020-08-25},
	date = {2012-11},
	langid = {english},
}

@inproceedings{duan_benchmarking_2016,
	location = {New York, {NY}},
	title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
	abstract = {Recently, researchers have made signiﬁcant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difﬁcult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel ﬁndings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/ rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
	eventtitle = {{ICML} '16},
	pages = {10},
	booktitle = {33rd International Conference on Machine Learning},
	author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	date = {2016},
	langid = {english},
}

@inproceedings{zhu_benchmarking_2018,
	location = {Raleigh, {NC}},
	title = {Benchmarking and Analyzing Deep Neural Network Training},
	doi = {10.1109/IISWC.2018.8573476},
	abstract = {The recent popularity of deep neural networks ({DNNs}) has generated considerable research interest in performing {DNN}-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference - i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation. Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark suite for {DNN} training, called {TBD}1, which comprises a representative set of eight {DNN} models and covers six major machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) performing an extensive performance analysis of these models on three major deep learning frameworks ({TensorFlow}, {MXNet}, {CNTK}) across different hardware configurations (single-{GPU}, multi-{GPU}, and multi-machine). We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of performance metrics, and methodologies to analyze the results. We also build a new set of tools for memory profiling in three major frameworks. These tools can shed light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in {DNN} training. Using our tools and methodologies, we make several important observations and recommendations on where future {DNN} training research and optimization should be focused.},
	eventtitle = {{IISWC} '18},
	pages = {88--100},
	booktitle = {{IEEE} International Symposium on Workload Characterization},
	author = {Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
	date = {2018-09},
	keywords = {Benchmark testing, Computational modeling, {DNN} models, {DNN} training, {DNN}-related computation, Graphics processing units, Hardware, Speech recognition, {TBD}, Tools, Training, adversarial networks, benchmark suite, data handling, data structures, deep learning frameworks, deep neural network training, deep neural networks, extensive performance analysis, hardware configurations, image classification networks, learning (artificial intelligence), machine learning applications, machine translation, {multiGPU}, myopic view, neural nets, object detection, optimization, performance analysis tools, performance metrics, primary benchmark, reinforcement learning, representative set, speech recognition, trained models},
}

@inproceedings{schelter_automatically_2017,
	location = {Long Beach, {CA}},
	title = {Automatically Tracking Metadata and Provenance of Machine Learning Experiments},
	abstract = {We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning ({ML}) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their {ML} workﬂow, and provides a basis for comparability and repeatability of {ML} experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said {ML} artifacts, is integrated with popular frameworks such as {MXNet}, {SparkML} and scikit-learn, and meets the demands of various production use cases at Amazon.},
	eventtitle = {{NIPS} '17},
	pages = {8},
	booktitle = {Machine Learning Systems Workshop},
	author = {Schelter, Sebastian and Böse, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
	date = {2017},
	langid = {english},
}

@inproceedings{vaswani_attention_2017,
	location = {Long Beach, {CA}},
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	eventtitle = {{NIPS} '17},
	pages = {15},
	booktitle = {31st Conference on Neural Information Processing Systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2021-01-26},
	date = {2017},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{karnin_almost_2013,
	location = {Atlanta, {GA}},
	title = {Almost Optimal Exploration in Multi-Armed Bandits},
	url = {http://proceedings.mlr.press/v28/karnin13.html},
	abstract = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameter-free algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doubly-logarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.},
	eventtitle = {{ICML} '13},
	pages = {1238--1246},
	booktitle = {30th International Conference on Machine Learning},
	author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
	editor = {Dasgupta, Sanjoy and {McAllester}, David},
	date = {2013-06-17},
}

@inproceedings{bergstra_algorithms_2011,
	location = {Granada, Spain},
	title = {Algorithms for Hyper-Parameter Optimization},
	url = {https://hal.inria.fr/hal-00642998},
	eventtitle = {{NIPS} '11},
	booktitle = {25th Conference on Neural Information Processing Systems},
	author = {Bergstra, James and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. and Pereira, F. and Weinberger, K. Q.},
	date = {2011-12},
}

@inproceedings{ignatov_ai_2019,
	title = {{AI} Benchmark: All About Deep Learning on Smartphones in 2019},
	doi = {10.1109/ICCVW.2019.00447},
	shorttitle = {{AI} Benchmark},
	abstract = {The performance of mobile {AI} accelerators has been evolving rapidly in the past two years, nearly doubling with each new generation of {SoCs}. The current 4th generation of mobile {NPUs} is already approaching the results of {CUDA}-compatible Nvidia graphics cards presented not long ago, which together with the increased capabilities of mobile deep learning frameworks makes it possible to run complex and deep {AI} models on mobile devices. In this paper, we evaluate the performance and compare the results of all chipsets from Qualcomm, {HiSilicon}, Samsung, {MediaTek} and Unisoc that are providing hardware acceleration for {AI} inference. We also discuss the recent changes in the Android {ML} pipeline and provide an overview of the deployment of deep learning models on mobile devices. All numerical results provided in this paper can be found and are regularly updated on the official project website.},
	eventtitle = {{ICCVW} '19},
	pages = {3617--3635},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision Workshop},
	author = {Ignatov, Andrey and Timofte, Radu and Kulik, Andrei and Yang, Seungsoo and Wang, Ke and Baum, Felix and Wu, Max and Xu, Lirong and Van Gool, Luc},
	date = {2019-10},
	keywords = {{AI} Benchmark, Acceleration, Android, Androids, Artificial Intelligence, Benchmark, Computer Vision, Deep Learning, Humanoid robots, Machine Learning, Machine learning, Mobile, Mobile handsets, Performance evaluation, Smartphones, {SoCs}, {TensorFlow}},
}

@inproceedings{yang_accelerating_2019,
	location = {Hyderabad, India},
	title = {Accelerating Data Loading in Deep Neural Network Training},
	doi = {10.1109/HiPC.2019.00037},
	abstract = {Data loading can dominate deep neural network training time on large-scale systems. We present a comprehensive study on accelerating data loading performance in large-scale distributed training. We first identify performance and scalability issues in current data loading implementations. We then propose optimizations that utilize {CPU} resources to the data loader design. We use an analytical model to characterize the impact of data loading on the overall training time and establish the performance trend as we scale up distributed training. Our model suggests that I/O rate limits the scalability of distributed training, which inspires us to design a locality-aware data loading method. By utilizing software caches, our method can drastically reduce the data loading communication volume in comparison with the original data loading implementation. Finally, we evaluate the proposed optimizations with various experiments. We achieved more than 30x speedup in data loading using 256 nodes with 1,024 learners.},
	eventtitle = {{HiPC} '19},
	pages = {235--245},
	booktitle = {26th {IEEE} International Conference on High Performance Computing, Data, and Analytics},
	author = {Yang, Chih-Chieh and Cong, Guojing},
	date = {2019-12},
	keywords = {Computational modeling, Distributed databases, Load modeling, Loading, Optimization, Synchronization, Training, cache storage, data loader design, data loading, data loading communication volume, data loading implementation, data loading performance, data locality, deep neural network training time, distributed training, large-scale distributed training, learning (artificial intelligence), locality-aware data loading method, machine learning, neural nets, scalability},
}

@inproceedings{wang_systematic_2020,
	location = {Austin, {TX}},
	title = {A Systematic Methodology for Analysis of Deep Learning Hardware and Software Platforms},
	abstract = {Training deep learning models is compute-intensive and there is an industry-wide trend towards hardware and software specialization to improve performance. To systematically compare deep learning systems, we introduce a methodology comprised of a set of analysis techniques and parameterized end-to-end models for fully connected, convolutional, and recurrent neural networks. This methodology can be applied to analyze various hardware and software systems, and is intended to complement traditional methods. We demonstrate its utility by comparing two generations of specialized platforms (Google’s Cloud {TPU} v2/v3), three heterogeneous platforms (Google {TPU}, Nvidia {GPU}, and Intel {CPU}), and specialized software stacks ({TensorFlow} and {CUDA}).},
	eventtitle = {{MLSys} '20},
	booktitle = {3rd {MLSys} Conference},
	author = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David},
	date = {2020},
	langid = {english},
}

@article{dziugaite_study_2016,
	title = {A study of the effect of {JPG} compression on adversarial images},
	url = {http://arxiv.org/abs/1608.00853},
	abstract = {Neural network image classiﬁers are known to be vulnerable to adversarial images, i.e., natural images which have been modiﬁed by an adversarial perturbation specifically designed to be imperceptible to humans yet fool the classiﬁer. Not only can adversarial images be generated easily, but these images will often be adversarial for networks trained on disjoint subsets of data or with different architectures. Adversarial images represent a potential security risk as well as a serious machine learning challenge—it is clear that vulnerable neural networks perceive images very differently from humans. Noting that virtually every image classiﬁcation data set is composed of {JPG} images, we evaluate the effect of {JPG} compression on the classiﬁcation of adversarial images. For Fast-Gradient-Sign perturbations of small magnitude, we found that {JPG} compression often reverses the drop in classiﬁcation accuracy to a large extent, but not always. As the magnitude of the perturbations increases, {JPG} recompression alone is insufﬁcient to reverse the effect.},
	journaltitle = {{arXiv}:1608.00853},
	author = {Dziugaite, Gintare Karolina and Ghahramani, Zoubin and Roy, Daniel M.},
	urldate = {2021-01-06},
	date = {2016-08-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1608.00853},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{chen_simple_2020,
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents {SimCLR}: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on {ImageNet}. A linear classifier trained on self-supervised representations learned by {SimCLR} achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised {ResNet}-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming {AlexNet} with 100X fewer labels.},
	journaltitle = {{arXiv}:2002.05709},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	urldate = {2021-09-08},
	date = {2020-06-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.05709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{schelter_amnesia_2020,
	location = {Amsterdam, Netherlands},
	title = {“Amnesia” - A Selection of Machine Learning Models That Can Forget User Data Very Fast},
	abstract = {Software systems that learn from user data with machine learning ({ML}) have become ubiquitous over the last years. Recent law requires organisations that process personal data to delete user data upon request (enacting the “right to be forgotten”).},
	eventtitle = {{CIDR} '20},
	pages = {9},
	booktitle = {10th Annual Conference on Innovative Data Systems Research},
	author = {Schelter, Sebastian},
	date = {2020},
	langid = {english},
}

@inproceedings{nguyen_multi-task_2018,
	location = {Turin, Italy},
	title = {A Multi-Task Deep Learning Architecture for Maritime Surveillance Using {AIS} Data Streams},
	doi = {10.1109/DSAA.2018.00044},
	eventtitle = {{DSAA} '18},
	pages = {331--340},
	booktitle = {2018 {IEEE} 5th International Conference on Data Science and Advanced Analytics},
	author = {Nguyen, Duong and Vadaine, Rodolphe and Hajduch, Guillaume and Garello, René and Fablet, Ronan},
	date = {2018},
}

@online{ramage_federated_2020,
	title = {Federated Analytics: Collaborative Data Science without Data Collection},
	url = {http://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html},
	shorttitle = {Federated Analytics},
	titleaddon = {Google {AI} Blog},
	author = {Ramage, Daniel and Mazzocchi, Stefano},
	urldate = {2021-09-23},
	date = {2020},
	langid = {english},
}

@article{kairouz_advances_2021,
	title = {Advances and Open Problems in Federated Learning},
	volume = {14},
	issn = {1935-8237},
	url = {http://dx.doi.org/10.1561/2200000083},
	doi = {10.1561/2200000083},
	abstract = {Federated learning ({FL}) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. {FL} embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in {FL} research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
	pages = {1--210},
	number = {1},
	journaltitle = {Foundations and Trends® in Machine Learning},
	author = {Kairouz, Peter and {McMahan}, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D’Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konecný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Raykova, Mariana and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
	date = {2021},
}

@online{mcmahan_federated_2017,
	title = {Federated Learning: Collaborative Machine Learning without Centralized Training Data},
	url = {http://ai.googleblog.com/2017/04/federated-learning-collaborative.html},
	shorttitle = {Federated Learning},
	titleaddon = {Google {AI} Blog},
	author = {{McMahan}, H. Brendan and Ramage, Daniel},
	urldate = {2021-10-21},
	date = {2017-04-06},
	langid = {english},
}

@online{state_of_california_california_2018,
	title = {California Consumer Privacy Act of 2018},
	url = {https://leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?division=3.&part=4.&lawCode=CIV&title=1.81.5},
	author = {{State of California}},
	urldate = {2021-10-21},
	date = {2018},
}

@online{onnx_contributors_open_2017,
	title = {Open Neural Network Exchange Standard},
	url = {https://onnx.ai/},
	author = {{ONNX contributors}},
	urldate = {2021-10-11},
	date = {2017},
}

@online{nvidia_tensorrt_2016,
	title = {{TensorRT}},
	url = {https://developer.nvidia.com/tensorrt},
	abstract = {{NVIDIA} {TensorRT} {NVIDIA}® {TensorRT}™ is an {SDK} for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. Get Started {TensorRT}-based applications perform up to 40X faster than {CPU}-only platforms during inference. With {TensorRT}, you can optimize neural network models trained in all major frameworks, calibrate for lower precision with high accuracy, and deploy to hyperscale data centers, embedded, or automotive product platforms.},
	author = {{NVIDIA}},
	urldate = {2021-09-28},
	date = {2016-04-05},
	langid = {english},
}

@software{webank_fate_2021,
	title = {{FATE}},
	rights = {Apache-2.0},
	url = {https://github.com/FederatedAI/FATE},
	abstract = {An Industrial Grade Federated Learning Framework},
	publisher = {Federated {AI} Ecosystem},
	author = {{WeBank}},
	urldate = {2021-09-22},
	date = {2021-09-22},
	note = {original-date: 2019-01-24T10:32:43Z},
	keywords = {algorithm, fate, federated-learning, machine-learning, privacy-preserving},
}

@article{yanjun_ma_paddlepaddle_2019,
	title = {{PaddlePaddle}: An Open-Source Deep Learning Platform from Industrial Practice},
	volume = {1},
	url = {http://www.jfdc.cnic.cn/EN/abstract/article_2.shtml},
	doi = {10.11871/jfdc.issn.2096.742X.2019.01.011},
	pages = {105},
	number = {1},
	journaltitle = {Frontiers of Data and Domputing},
	author = {Yanjun Ma, Dianhai Yu, Tian Wu, Haifeng Wang},
	date = {2019},
	note = {Publisher: Frontiers of Data and Domputing},
	keywords = {{PaddlePaddle}, artificial intelligence, deep learning, deep learning framework},
}

@book{goodfellow_deep_2016,
	title = {Deep Learning},
	url = {http://www.deeplearningbook.org},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	urldate = {2021-09-15},
	date = {2016},
}

@online{boutell_png_1997,
	title = {{PNG} (Portable Network Graphics) Specification},
	url = {https://www.hjp.at/doc/rfc/rfc2083.html},
	author = {Boutell, Thomas},
	urldate = {2021-09-14},
	date = {1997},
	note = {Version 1.0},
}

@article{taubman_jpeg2000_2002,
	title = {{JPEG}2000: standard for interactive imaging},
	volume = {90},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2002.800725},
	shorttitle = {{JPEG}2000},
	abstract = {{JPEG}2000 is the latest image compression standard to emerge from the Joint Photographic Experts Group ({JPEG}) working under the auspices of the International Standards Organization. Although the new standard does offer superior compression performance to {JPEG}, {JPEG}2000 provides a whole new way of interacting with compressed imagery in a scalable and interoperable fashion. This paper provides a tutorial-style review of the new standard, explaining the technology on which it is based and drawing comparisons with {JPEG} and other compression standards. The paper also describes new work, exploiting the capabilities of {JPEG}2000 in client-server systems for efficient interactive browsing of images over the Internet.},
	pages = {1336--1357},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Taubman, D.S. and Marcellin, M.W.},
	date = {2002-08},
	note = {Conference Name: Proceedings of the {IEEE}},
	keywords = {Bit rate, {IEC} standards, {ISO} standards, Image coding, Image resolution, Scalability, Standards development, Standards organizations, Streaming media, Transform coding},
}

@book{salomon_concise_2007,
	title = {A Concise Introduction to Data Compression},
	isbn = {978-1-84800-072-8},
	abstract = {Compressing data is an option naturally selected when faced with problems of high costs or restricted space. Written by a renowned expert in the field, this book offers readers a succinct, reader-friendly foundation to the chief approaches, methods and techniques currently employed in the field of data compression. Part I presents the basic approaches to data compression and describes a few popular techniques and methods commonly used to compress data. The reader discovers essential concepts, such as variable-length and prefix codes, statistical distributions and run-length encoding. Part {II} then concentrates on advanced techniques, such as arithmetic coding, orthogonal transforms, subband transforms and the Burrows-Wheeler transform. Features: • Clear overview of the principles underlying this field • Outlines the essentials of the various approaches to compressing data • Contains many learning aids such as: chapter introductions and summaries, chapter-end exercises, comprehensive glossary, etc. • Provides several examples of important compression algorithms • Offers a supplementary author-maintained website, with errata and auxiliary material – www.davidsalomon.name/{DCugAdvertis}/{DCug}.html • An ideal introductory volume to David Salomon’s fourth edition of Data Compression: The Complete Reference Complete and clear, this book is the perfect resource for undergraduates in computer science and requires a minimum of mathematics. It is also ideal for readers with a basic knowledge of computer science wanting to learn about data compression. David Salomon is a professor emeritus of Computer Science at California State University, Northridge. He has authored numerous articles and books, including Coding for Data and Computer Communications, Guide to Data Compression Methods, Data Privacy and Security, Computer Graphics and Geometric Modeling, Foundations of Computer Security, Transformations and Projections in Computer Graphics, and Variable-length Codes for Data Compression.},
	pagetotal = {318},
	publisher = {Springer Science \& Business Media},
	author = {Salomon, David},
	date = {2007-12-18},
	langid = {english},
	note = {Google-Books-{ID}: {mnpeizY}0btYC},
	keywords = {Computers / Information Technology, Computers / Information Theory, Computers / Programming / Algorithms},
}

@article{warnat-herresthal_swarm_2021,
	title = {Swarm Learning for decentralized and confidential clinical machine learning},
	volume = {594},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/s41586-021-03583-3},
	doi = {10.1038/s41586-021-03583-3},
	abstract = {Fast and reliable detection of patients with severe and heterogeneous illnesses is a major goal of precision medicine1,2. Patients with leukaemia can be identified using machine learning on the basis of their blood transcriptomes3. However, there is an increasing divide between what is technically possible and what is allowed, because of privacy legislation4,5. Here, to facilitate the integration of any medical data from any data owner worldwide without violating privacy laws, we introduce Swarm Learning—a decentralized machine-learning approach that unites edge computing, blockchain-based peer-to-peer networking and coordination while maintaining confidentiality without the need for a central coordinator, thereby going beyond federated learning. To illustrate the feasibility of using Swarm Learning to develop disease classifiers using distributed data, we chose four use cases of heterogeneous diseases ({COVID}-19, tuberculosis, leukaemia and lung pathologies). With more than 16,400 blood transcriptomes derived from 127 clinical studies with non-uniform distributions of cases and controls and substantial study biases, as well as more than 95,000 chest X-ray images, we show that Swarm Learning classifiers outperform those developed at individual sites. In addition, Swarm Learning completely fulfils local confidentiality regulations by design. We believe that this approach will notably accelerate the introduction of precision medicine.},
	pages = {265--270},
	number = {7862},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Warnat-Herresthal, Stefanie and Schultze, Hartmut and Shastry, Krishnaprasad Lingadahalli and Manamohan, Sathyanarayanan and Mukherjee, Saikat and Garg, Vishesh and Sarveswara, Ravi and Händler, Kristian and Pickkers, Peter and Aziz, N. Ahmad and Ktena, Sofia and Tran, Florian and Bitzer, Michael and Ossowski, Stephan and Casadei, Nicolas and Herr, Christian and Petersheim, Daniel and Behrends, Uta and Kern, Fabian and Fehlmann, Tobias and Schommers, Philipp and Lehmann, Clara and Augustin, Max and Rybniker, Jan and Altmüller, Janine and Mishra, Neha and Bernardes, Joana P. and Krämer, Benjamin and Bonaguro, Lorenzo and Schulte-Schrepping, Jonas and De Domenico, Elena and Siever, Christian and Kraut, Michael and Desai, Milind and Monnet, Bruno and Saridaki, Maria and Siegel, Charles Martin and Drews, Anna and Nuesch-Germano, Melanie and Theis, Heidi and Heyckendorf, Jan and Schreiber, Stefan and Kim-Hellmuth, Sarah and Balfanz, Paul and Eggermann, Thomas and Boor, Peter and Hausmann, Ralf and Kuhn, Hannah and Isfort, Susanne and Stingl, Julia Carolin and Schmalzing, Günther and Kuhl, Christiane K. and Röhrig, Rainer and Marx, Gernot and Uhlig, Stefan and Dahl, Edgar and Müller-Wieland, Dirk and Dreher, Michael and Marx, Nikolaus and Nattermann, Jacob and Skowasch, Dirk and Kurth, Ingo and Keller, Andreas and Bals, Robert and Nürnberg, Peter and Rieß, Olaf and Rosenstiel, Philip and Netea, Mihai G. and Theis, Fabian and Mukherjee, Sach and Backes, Michael and Aschenbrenner, Anna C. and Ulas, Thomas and Angelov, Angel and Bartholomäus, Alexander and Becker, Anke and Bezdan, Daniela and Blumert, Conny and Bonifacio, Ezio and Bork, Peer and Boyke, Bunk and Blum, Helmut and Clavel, Thomas and Colome-Tatche, Maria and Cornberg, Markus and De La Rosa Velázquez, Inti Alberto and Diefenbach, Andreas and Dilthey, Alexander and Fischer, Nicole and Förstner, Konrad and Franzenburg, Sören and Frick, Julia-Stefanie and Gabernet, Gisela and Gagneur, Julien and Ganzenmueller, Tina and Gauder, Marie and Geißert, Janina and Goesmann, Alexander and Göpel, Siri and Grundhoff, Adam and Grundmann, Hajo and Hain, Torsten and Hanses, Frank and Hehr, Ute and Heimbach, André and Hoeper, Marius and Horn, Friedemann and Hübschmann, Daniel and Hummel, Michael and Iftner, Thomas and Iftner, Angelika and Illig, Thomas and Janssen, Stefan and Kalinowski, Jörn and Kallies, René and Kehr, Birte and Keppler, Oliver T. and Klein, Christoph and Knop, Michael and Kohlbacher, Oliver and Köhrer, Karl and Korbel, Jan and Kremsner, Peter G. and Kühnert, Denise and Landthaler, Markus and Li, Yang and Ludwig, Kerstin U. and Makarewicz, Oliwia and Marz, Manja and McHardy, Alice C. and Mertes, Christian and Münchhoff, Maximilian and Nahnsen, Sven and Nöthen, Markus and Ntoumi, Francine and Overmann, Jörg and Peter, Silke and Pfeffer, Klaus and Pink, Isabell and Poetsch, Anna R. and Protzer, Ulrike and Pühler, Alfred and Rajewsky, Nikolaus and Ralser, Markus and Reiche, Kristin and Ripke, Stephan and da Rocha, Ulisses Nunes and Saliba, Antoine-Emmanuel and Sander, Leif Erik and Sawitzki, Birgit and Scheithauer, Simone and Schiffer, Philipp and Schmid-Burgk, Jonathan and Schneider, Wulf and Schulte, Eva-Christina and Sczyrba, Alexander and Sharaf, Mariam L. and Singh, Yogesh and Sonnabend, Michael and Stegle, Oliver and Stoye, Jens and Vehreschild, Janne and Velavan, Thirumalaisamy P. and Vogel, Jörg and Volland, Sonja and von Kleist, Max and Walker, Andreas and Walter, Jörn and Wieczorek, Dagmar and Winkler, Sylke and Ziebuhr, John and Breteler, Monique M. B. and Giamarellos-Bourboulis, Evangelos J. and Kox, Matthijs and Becker, Matthias and Cheran, Sorin and Woodacre, Michael S. and Goh, Eng Lim and Schultze, Joachim L. and {COVID-19 Aachen Study (COVAS)} and {Deutsche COVID-19 Omics Initiative (DeCOI)}},
	date = {2021-06-01},
}

@online{menage_cgroups_2007,
	title = {cgroups},
	url = {https://kernel.org/doc/Documentation/cgroup-v1},
	abstract = {cgroups is a Linux kernel feature that limits, accounts for, and isolates the resource usage of a collection of processes.},
	author = {Menage, Paul and Seth, Rohit},
	urldate = {2021-03-15},
	date = {2007},
}

@book{montavon_neural_2012,
	location = {Heidelberg},
	title = {Neural Networks: Tricks of the Trade},
	isbn = {978-3-642-35289-8},
	shorttitle = {Neural Networks},
	pagetotal = {769},
	publisher = {Springer},
	author = {Montavon, Grégoire and Orr, Geneviève and Müller, Klaus-Robert},
	date = {2012},
}

@article{lau_effects_2003,
	title = {Effects of {JPEG} Compression on Image Classification},
	volume = {24},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160210142842},
	doi = {10.1080/01431160210142842},
	pages = {1535--1544},
	number = {7},
	journaltitle = {International Journal of Remote Sensing},
	author = {Lau, W.-L. and Li, Z.-L. and Lam, K. W.-K.},
	date = {2003},
}

@online{tange_gnu_2021,
	title = {{GNU} Parallel},
	url = {https://doi.org/10.5281/zenodo.4454976},
	author = {Tange, Ole},
	date = {2021-01},
}

@online{reinforcement_learning_working_group_garage_2021,
	title = {garage},
	rights = {{MIT} License         ,                 {MIT} License},
	url = {https://github.com/rlworkgroup/garage},
	abstract = {A toolkit for reproducible reinforcement learning research.},
	author = {{Reinforcement Learning Working Group}},
	urldate = {2021-04-25},
	date = {2021-04-23},
	keywords = {pytorch, reproducibility, rl-algorithms, tensorflow},
}

@online{nvidia_cuda_2006,
	title = {{CUDA}},
	url = {https://developer.nvidia.com/cuda-toolkit},
	abstract = {{CUDA} Toolkit Develop, Optimize and Deploy {GPU}-Accelerated Apps The {NVIDIA}® {CUDA}® Toolkit provides a development environment for creating high performance {GPU}-accelerated applications. With the {CUDA} Toolkit, you can develop, optimize, and deploy your applications on {GPU}-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and {HPC} supercomputers.},
	author = {{NVIDIA}},
	urldate = {2021-01-22},
	date = {2006},
}

@online{baidu_paddle_2021,
	title = {Paddle},
	rights = {Apache-2.0 License         ,                 Apache-2.0 License},
	url = {https://github.com/PaddlePaddle/Paddle},
	abstract = {{PArallel} Distributed Deep {LEarning}: Machine Learning Framework from Industrial Practice},
	author = {{Baidu} and {Paddle contributors}},
	urldate = {2021-04-24},
	date = {2021-04-24},
	keywords = {deep-learning, distributed-training, efficiency, machine-learning, neural-network, paddlepaddle, python, scalability},
}

@incollection{hao_edge_2019,
	location = {Cham},
	title = {Edge {AIBench}: Towards Comprehensive End-to-End Edge Computing Benchmarking},
	isbn = {978-3-030-32813-9},
	abstract = {In edge computing scenarios, the distribution of data and collaboration of workloads on different layers are serious concerns for performance, privacy, and security issues. So for edge computing benchmarking, we must take an end-to-end view, considering all three layers: client-side devices, edge computing layer, and cloud servers. Unfortunately, the previous work ignores this most important point. This paper presents the {BenchCouncil}'s coordinated effort on edge {AI} benchmarks, named Edge {AIBench}. In total, Edge {AIBench} models four typical application scenarios: {ICU} Patient Monitor, Surveillance Camera, Smart Home, and Autonomous Vehicle with the focus on data distribution and workload collaboration on three layers. Edge {AIBench} is publicly available from http://www.benchcouncil.org/{EdgeAIBench}/index.html. We also build an edge computing testbed with a federated learning framework to resolve performance, privacy, and security issues.},
	pages = {23--30},
	booktitle = {Benchmarking, Measuring, and Optimizing},
	publisher = {Springer International Publishing},
	author = {Hao, Tianshu and Huang, Yunyou and Wen, Xu and Gao, Wanling and Zhang, Fan and Zheng, Chen and Wang, Lei and Ye, Hainan and Hwang, Kai and Ren, Zujie and Zhan, Jianfeng},
	editor = {Zheng, Chen and Zhan, Jianfeng},
	date = {2019},
}

@article{dodge_quality_2017,
	title = {Quality Resilient Deep Neural Networks},
	url = {http://arxiv.org/abs/1703.08119},
	abstract = {We study deep neural networks for classiﬁcation of images with quality distortions. We ﬁrst show that networks ﬁne-tuned on distorted data greatly outperform the original networks when tested on distorted data. However, ﬁnetuned networks perform poorly on quality distortions that they have not been trained for. We propose a mixture of experts ensemble method that is robust to different types of distortions. The “experts” in our model are trained on a particular type of distortion. The output of the model is a weighted sum of the expert models, where the weights are determined by a separate gating network. The gating network is trained to predict optimal weights for a particular distortion type and level. During testing, the network is blind to the distortion level and type, yet can still assign appropriate weights to the expert models. We additionally investigate weight sharing methods for the mixture model and show that improved performance can be achieved with a large reduction in the number of unique network parameters.},
	journaltitle = {{arXiv}:1703.08119},
	author = {Dodge, Samuel and Karam, Lina},
	urldate = {2021-04-27},
	date = {2017-03-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.08119},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{bishop_pattern_2006,
	location = {New York},
	title = {Pattern Recognition and Machine Learning},
	isbn = {978-0-387-31073-2},
	pagetotal = {738},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	date = {2006},
	langid = {english},
	keywords = {Machine learning, Pattern perception},
}

@article{bianco_benchmark_2018,
	title = {Benchmark Analysis of Representative Deep Neural Network Architectures},
	volume = {6},
	doi = {10.1109/ACCESS.2018.2877890},
	pages = {64270--64277},
	journaltitle = {{IEEE} Access},
	author = {Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
	date = {2018},
}

@article{qin_how_2018,
	title = {How Convolutional Neural Networks See the World -- A Survey of Convolutional Neural Network Visualization Methods},
	volume = {1},
	issn = {2577-8838},
	doi = {10.3934/mfc.2018008},
	abstract = {Nowadays, the Convolutional Neural Networks ({CNNs}) have achieved impressive performance on many computer vision related tasks, such as object detection, image recognition, image retrieval, etc. These achievements beneﬁt from the {CNNs}’ outstanding capability to learn the input features with deep layers of neuron structures and iterative training process. However, these learned features are hard to identify and interpret from a human vision perspective, causing a lack of understanding of the {CNNs}’ internal working mechanism. To improve the {CNN} interpretability, the {CNN} visualization is well utilized as a qualitative analysis method, which translates the internal features into visually perceptible patterns. And many {CNN} visualization works have been proposed in the literature to interpret the {CNN} in perspectives of network structure, operation, and semantic concept.},
	pages = {149--180},
	number = {2},
	journaltitle = {Mathematical Foundations of Computing},
	author = {Qin, Zhuwei and Yu, Fuxun and Liu, Chenchen and Chen, Xiang},
	urldate = {2021-03-09},
	date = {2018-05-31},
	langid = {english},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{katakol_distributed_2021,
	title = {Distributed Learning and Inference With Compressed Images},
	volume = {30},
	issn = {1941-0042},
	doi = {10.1109/TIP.2021.3058545},
	abstract = {Modern computer vision requires processing large amounts of data, both while training the model and/or during inference, once the model is deployed. Scenarios where images are captured and processed in physically separated locations are increasingly common (e.g. autonomous vehicles, cloud computing, smartphones). In addition, many devices suffer from limited resources to store or transmit data (e.g. storage space, channel capacity). In these scenarios, lossy image compression plays a crucial role to effectively increase the number of images collected under such constraints. However, lossy compression entails some undesired degradation of the data that may harm the performance of the downstream analysis task at hand, since important semantic information may be lost in the process. Moreover, we may only have compressed images at training time but are able to use original images at inference time (i.e. test), or vice versa, and in such a case, the downstream model suffers from covariate shift. In this paper, we analyze this phenomenon, with a special focus on vision-based perception for autonomous driving as a paradigmatic scenario. We see that loss of semantic information and covariate shift do indeed exist, resulting in a drop in performance that depends on the compression rate. In order to address the problem, we propose dataset restoration, based on image restoration with generative adversarial networks ({GANs}). Our method is agnostic to both the particular image compression method and the downstream task; and has the advantage of not adding additional cost to the deployed models, which is particularly important in resource-limited devices. The presented experiments focus on semantic segmentation as a challenging use case, cover a broad range of compression rates and diverse datasets, and show how our method is able to significantly alleviate the negative effects of compression on the downstream visual task.},
	pages = {3069--3083},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Katakol, Sudeep and Elbarashy, Basem and Herranz, Luis and van de Weijer, Joost and López, Antonio M.},
	date = {2021},
	keywords = {Data models, Degradation, Image coding, Image compression, Image restoration, Semantics, Task analysis, Training, autonomous driving, deep learning, generative adversarial networks, image restoration},
}

@article{cybenko_approximation_1989,
	title = {Approximation by Superpositions of a Sigmoidal Function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	pages = {303--314},
	number = {4},
	journaltitle = {Mathematics of Control, Signals and Systems},
	shortjournal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, George},
	date = {1989-12-01},
}

@report{behr_benchmarking_2020,
	title = {Benchmarking Deep Learning Frameworks Using Parameterized Models},
	abstract = {Nowadays, machine learning is one of the most popular topics in computer science which is why major companies like Google or Facebook have developed their machine learning framework. These frameworks help other developers programming their machine learning models, and most developers stick to one framework even though this framework is not well suited for their given problem. Yet, there is not much research done on comparative analysis between different machine learning frameworks to understand their strength and weaknesses, because a comparative analysis of these frameworks has several challenges like a uniform implementation of the models.},
	institution = {{TU} Berlin},
	author = {Behr, Henriette and Côme, Clément and Maresca, Rocco},
	date = {2020},
	langid = {english},
	note = {Big Data Analytics Project Report},
}

@book{zheng_feature_2018,
	title = {Feature Engineering for Machine Learning Principles and Techniques for Data Scientists},
	isbn = {978-1-4919-5319-8},
	abstract = {Feature engineering is a crucial step in the machine-learning pipeline, yet this topic is rarely examined on its own. With this practical book, you'll learn techniques for extracting and transforming features-the numeric representations of raw data-into formats for machine-learning models. Each chapter guides you through a single data problem, such as how to represent text or image data. Together, these examples illustrate the main principles of feature engineering. Rather than simply teach these principles, authors Alice Zheng and Amanda Casari focus on practical application with exercises throughout the book. The closing chapter brings everything together by tackling a real-world, structured dataset with several feature-engineering techniques. Python packages including numpy, Pandas, Scikit-learn, and Matplotlib are used in code examples. You'll examine: Feature engineering for numeric data: filtering, binning, scaling, log transforms, and power transforms Natural text techniques: bag-of-words, n-grams, and phrase detection Frequency-based filtering and feature scaling for eliminating uninformative features Encoding techniques of categorical variables, including feature hashing and bin-counting Model-based feature engineering with principal component analysis The concept of model stacking, using k-means as a featurization technique Image feature extraction with manual and deep-learning techniques},
	publisher = {O'Reilly},
	author = {Zheng, Alice and Casari, Amanda},
	date = {2018},
}

@incollection{fleet_visualizing_2014,
	location = {Cham},
	title = {Visualizing and Understanding Convolutional Networks},
	isbn = {978-3-319-10589-5},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the {ImageNet} benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the {ImageNet} classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our {ImageNet} model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
	pages = {818--833},
	booktitle = {Computer Vision – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	urldate = {2021-03-09},
	date = {2014},
	langid = {english},
}

@incollection{winkler_vision_2001,
	location = {Boston, {MA}},
	title = {Vision and Video: Models and Applications},
	isbn = {978-1-4757-3411-9},
	url = {https://doi.org/10.1007/978-1-4757-3411-9_10},
	abstract = {While traditional analog systems still form the vast majority of television sets today, production studios, broadcasters and network providers have been installing digital video equipment at an ever-increasing rate. The border line between analog and digital video is moving closer and closer to the consumer. Digital satellite and cable service have been available for a while, and recently terrestrial digital television broadcast has been introduced in a number of locations around the world.},
	pages = {201--229},
	booktitle = {Vision Models and Applications to Image and Video Processing},
	publisher = {Springer},
	author = {Winkler, Stefan and Kunt, Murat and van den Branden Lambrecht, Christian J.},
	editor = {van den Branden Lambrecht, Christian J.},
	date = {2001},
	doi = {10.1007/978-1-4757-3411-9_10},
}

@article{wallace_jpeg_1992,
	title = {The {JPEG} Still Picture Compression Standard},
	volume = {38},
	doi = {10.1109/30.125072},
	pages = {xviii--xxxiv},
	number = {1},
	journaltitle = {{IEEE} Transactions on Consumer Electronics},
	author = {Wallace, Gregory K.},
	date = {1992},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster Level in {StarCraft} {II} Using Multi-Agent Reinforcement Learning},
	volume = {575},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of {StarCraft} has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top {StarCraft} players. We chose to address the challenge of {StarCraft} using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, {AlphaStar}, in the full game of {StarCraft} {II}, through a series of online games against human players. {AlphaStar} was rated at Grandmaster level for all three {StarCraft} races and above 99.8\% of officially ranked human players.},
	pages = {350--354},
	number = {7782},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and {McKinney}, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	date = {2019-11-01},
}

@article{tharwat_classification_2020,
	title = {Classification Assessment Methods},
	volume = {17},
	issn = {2634-1964, 2210-8327},
	url = {https://doi.org/10.1016/j.aci.2018.08.003},
	doi = {10.1016/j.aci.2018.08.003},
	abstract = {Classification techniques have been applied to many applications in various fields of sciences. There are several ways of evaluating classification algorithms. The analysis of such metrics and its significance must be interpreted correctly for evaluating different learning algorithms. Most of these measures are scalar metrics and some of them are graphical methods. This paper introduces a detailed overview of the classification assessment measures with the aim of providing the basics of these measures and to show how it works to serve as a comprehensive source for researchers who are interested in this field. This overview starts by highlighting the definition of the confusion matrix in binary and multi-class classification problems. Many classification measures are also explained in details, and the influence of balanced and imbalanced data on each metric is presented. An illustrative example is introduced to show (1) how to calculate these measures in binary and multi-class classification problems, and (2) the robustness of some measures against balanced and imbalanced data. Moreover, some graphical measures such as Receiver operating characteristics ({ROC}), Precision-Recall, and Detection error trade-off ({DET}) curves are presented with details. Additionally, in a step-by-step approach, different numerical examples are demonstrated to explain the preprocessing steps of plotting {ROC}, {PR}, and {DET} curves.},
	pages = {168--192},
	number = {1},
	journaltitle = {Applied Computing and Informatics},
	author = {Tharwat, Alaa},
	urldate = {2021-05-10},
	date = {2020-01-01},
}

@article{tang_communication-efficient_2020,
	title = {Communication-Efficient Distributed Deep Learning: A Comprehensive Survey},
	url = {http://arxiv.org/abs/2003.06307},
	shorttitle = {Communication-Efficient Distributed Deep Learning},
	abstract = {Distributed deep learning becomes very common to reduce the overall training time by exploiting multiple computing devices (e.g., {GPUs}/{TPUs}) as the size of deep models and data sets increases. However, data communication between computing devices could be a potential bottleneck to limit the system scalability. How to address the communication problem in distributed deep learning is becoming a hot research topic recently. In this paper, we provide a comprehensive survey of the communication-efﬁcient distributed training algorithms in both system-level and algorithmic-level optimizations. In the systemlevel, we demystify the system design and implementation to reduce the communication cost. In algorithmic-level, we compare different algorithms with theoretical convergence bounds and communication complexity. Speciﬁcally, we ﬁrst propose the taxonomy of data-parallel distributed training algorithms, which contains four main dimensions: communication synchronization, system architectures, compression techniques, and parallelism of communication and computing. Then we discuss the studies in addressing the problems of the four dimensions to compare the communication cost. We further compare the convergence rates of different algorithms, which enable us to know how fast the algorithms can converge to the solution in terms of iterations. According to the system-level communication cost analysis and theoretical convergence speed comparison, we provide the readers to understand what algorithms are more efﬁcient under speciﬁc distributed environments and extrapolate potential directions for further optimizations.},
	journaltitle = {{arXiv}:2003.06307},
	author = {Tang, Zhenheng and Shi, Shaohuai and Chu, Xiaowen and Wang, Wei and Li, Bo},
	urldate = {2021-03-24},
	date = {2020-03-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.06307},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@online{summers_nvidias_2020,
	title = {{NVIDIA}'s Latest Desktop Workstation has Four 80GB {GPUs}},
	url = {https://www.engadget.com/nvidia-dgx-station-a100-80gb-tensor-core-gpu-announcement-140027589.html},
	abstract = {Back in May, {NVIDIA} announced a ridiculously powerful {GPU} called the A100.  The card was designed for data center systems, however, such as the company’s own {DGX} A100, rather than anything that the average consumer would run at home.  Today, the company has announced the {DGX} Station A100 which, as the name implies, has the form factor of a desk-bound workstation.},
	titleaddon = {Engadget},
	author = {Summers, Nick},
	urldate = {2021-02-16},
	date = {2020-11-16},
	langid = {american},
}

@article{stonebraker_operating_1981,
	title = {Operating System Support for Database Management},
	volume = {24},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/358699.358703},
	doi = {10.1145/358699.358703},
	abstract = {Several operating system services are examined with a view toward their applicability to support of database management functions. These services include buffer pool management; the file system; scheduling, process management, and interprocess communication; and consistency control.},
	pages = {412--418},
	number = {7},
	journaltitle = {Communications of the {ACM}},
	author = {Stonebraker, Michael},
	date = {1981-07},
	keywords = {buffer management, database management, file systems, interprocess communication, operating systems, scheduling},
}

@article{silver_mastering_2016,
	title = {Mastering the Game of Go with Deep Neural Networks and Tree Search},
	volume = {529},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program {AlphaGo} achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	pages = {484--489},
	number = {7587},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	date = {2016-01-01},
}

@article{rumelhart_learning_1986,
	title = {Learning Representations by Back-Propagating Errors},
	volume = {323},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	date = {1986-10-01},
}

@article{ruder_overview_2017,
	title = {An Overview of Gradient Descent Optimization Algorithms},
	journaltitle = {{arXiv}:1609.04747},
	author = {Ruder, Sebastian},
	date = {2017},
}

@article{roy_effects_2019,
	title = {Effects of Degradations on Deep Neural Network Architectures},
	url = {http://arxiv.org/abs/1807.10108},
	abstract = {Recently, image classiﬁcation methods based on capsules (groups of neurons) and a novel dynamic routing protocol are proposed. The methods show promising performances than the state-of-the-art {CNN}-based models in some of the existing datasets. However, the behavior of capsule-based models and {CNN}-based models are largely unknown in presence of noise. So it is important to study the performance of these models under various noises. In this paper, we demonstrate the effect of image degradations on deep neural network architectures for image classiﬁcation task. We select six widely used {CNN} architectures to analyse their performances for image classiﬁcation task on datasets of various distortions. Our work has three main contributions: 1) we observe the effects of degradations on different {CNN} models; 2) accordingly, we propose a network setup that can enhance the robustness of any {CNN} architecture for certain degradations, and 3) we propose a new capsule network that achieves high recognition accuracy. To the best of our knowledge, this is the ﬁrst study on the performance of {CapsuleNet} ({CapsNet}) and other state-of-the-art {CNN} architectures under different types of image degradations. Also, our datasets and source code are available publicly to the researchers.},
	journaltitle = {{arXiv}:1807.10108},
	author = {Roy, Prasun and Ghosh, Subhankar and Bhattacharya, Saumik and Pal, Umapada},
	urldate = {2021-04-27},
	date = {2019-06-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.10108},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@book{rajaraman_mining_2011,
	title = {Mining of Massive Datasets},
	isbn = {978-1-139-16173-2},
	url = {http://infolab.stanford.edu/~ullman/mmds.html#original},
	publisher = {Cambridge University Press},
	author = {Rajaraman, Anand and Leskovec, Jure and Ullman, Jeffrey D.},
	urldate = {2021-03-24},
	date = {2011},
}

@online{python_software_foundation_python_2021,
	title = {Python Documentation — Time Access and Conversions},
	url = {https://docs.python.org/3/library/time.html#time.time},
	author = {{Python Software Foundation}},
	urldate = {2021-01-22},
	date = {2021},
}

@incollection{ozah_compression_2019,
	location = {Cham},
	title = {Compression Improves Image Classification Accuracy},
	isbn = {978-3-030-18305-9},
	abstract = {We study the relationship between the accuracy of image classification and the level of image compression. Specifically, we look at how various levels of {JPEG} and {SVD} compression affect the score of the correct answer in Inception-v3, a {TensorFlow}-based image classifier trained on the {ImageNet} database.},
	pages = {525--530},
	booktitle = {Advances in Artificial Intelligence},
	publisher = {Springer International Publishing},
	author = {Ozah, Nnamdi and Kolokolova, Antonina},
	editor = {Meurs, Marie-Jean and Rudzicz, Frank},
	date = {2019},
}

@online{nvidia_nvlink_2021,
	title = {{NVLink} \& {NVSwitch} for Advanced Multi-{GPU} Communication},
	url = {https://www.nvidia.com/en-us/data-center/nvlink},
	author = {{NVIDIA}},
	urldate = {2021-03-23},
	date = {2021},
	langid = {english},
}

@online{mlcommons_association_mlperf_2020,
	title = {{MLPerf} Training v0.7 Results},
	url = {https://mlcommons.org/en/training-normal-07},
	titleaddon = {{MLCommons}},
	author = {{MLCommons Association}},
	urldate = {2021-02-04},
	date = {2020},
	langid = {english},
}

@article{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	journaltitle = {{arXiv}:1301.3781},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2021-03-24},
	date = {2013-09-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1301.3781},
	keywords = {Computer Science - Computation and Language},
}

@article{mattson_mlperf_2020,
	title = {{MLPerf}: An Industry Standard Benchmark Suite for Machine Learning Performance},
	volume = {40},
	issn = {1937-4143},
	doi = {10.1109/MM.2020.2974843},
	shorttitle = {{MLPerf}},
	abstract = {In this article, we describe the design choices behind {MLPerf}, a machine learning performance benchmark that has become an industry standard. The first two rounds of the {MLPerf} Training benchmark helped drive improvements to software-stack performance and scalability, showing a 1.3× speedup in the top 16-chip results despite higher quality targets and a 5.5× increase in system scale. The first round of {MLPerf} Inference received over 500 benchmark results from 14 different organizations, showing growing adoption.},
	pages = {8--16},
	number = {2},
	journaltitle = {{IEEE} Micro},
	author = {Mattson, Peter and Reddi, Vijay Janapa and Cheng, Christine and Coleman, Cody and Diamos, G. and Kanter, David and Micikevicius, Paulius and Patterson, David and Schmuelling, Guenther and Tang, Hanlin and Wei, Gu-Yeon and Wu, Carole-Jean},
	date = {2020-03},
	keywords = {Benchmark testing, Computational modeling, {MLPerf} Inference, {MLPerf} Training benchmark, Machine learning, Measurement, Numerical models, Training, industry standard benchmark suite, inference mechanisms, learning (artificial intelligence), machine learning performance benchmark, software-stack performance},
}

@article{liu_survey_2017,
	title = {A Survey of Deep Neural Network Architectures and Their Applications},
	volume = {234},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231216315533},
	doi = {https://doi.org/10.1016/j.neucom.2016.12.038},
	abstract = {Since the proposal of a fast learning algorithm for deep belief networks in 2006, the deep learning techniques have drawn ever-increasing research interests because of their inherent capability of overcoming the drawback of traditional algorithms dependent on hand-designed features. Deep learning approaches have also been found to be suitable for big data analysis with successful applications to computer vision, pattern recognition, speech recognition, natural language processing, and recommendation systems. In this paper, we discuss some widely-used deep learning architectures and their practical applications. An up-to-date overview is provided on four deep learning architectures, namely, autoencoder, convolutional neural network, deep belief network, and restricted Boltzmann machine. Different types of deep neural networks are surveyed and recent progresses are summarized. Applications of deep learning techniques on some selected areas (speech recognition, pattern recognition and computer vision) are highlighted. A list of future research topics are finally given with clear justifications.},
	pages = {11--26},
	journaltitle = {Neurocomputing},
	author = {Liu, Weibo and Wang, Zidong and Liu, Xiaohui and Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E.},
	date = {2017},
	keywords = {Autoencoder, Convolutional neural network, Deep belief network, Deep learning, Restricted Boltzmann machine},
}

@article{li_learning_2018,
	title = {Learning {IoT} in Edge: Deep Learning for the Internet of Things with Edge Computing},
	volume = {32},
	issn = {1558-156X},
	doi = {10.1109/MNET.2018.1700202},
	shorttitle = {Learning {IoT} in Edge},
	abstract = {Deep learning is a promising approach for extracting accurate information from raw sensor data from {IoT} devices deployed in complex environments. Because of its multilayer structure, deep learning is also appropriate for the edge computing environment. Therefore, in this article, we first introduce deep learning for {IoTs} into the edge computing environment. Since existing edge nodes have limited processing capability, we also design a novel offloading strategy to optimize the performance of {IoT} deep learning applications with edge computing. In the performance evaluation, we test the performance of executing multiple deep learning tasks in an edge computing environment with our strategy. The evaluation results show that our method outperforms other optimization solutions on deep learning for {IoT}.},
	pages = {96--101},
	number = {1},
	journaltitle = {{IEEE} Network},
	author = {Li, He and Ota, Kaoru and Dong, Mianxiong},
	date = {2018-01},
	keywords = {Cloud computing, Computational modeling, Edge computing, Feature extraction, Internet of Things, {IoT} deep learning applications, Machine learning, Servers, Task analysis, cloud computing, deep learning tasks, edge computing environment, learning (artificial intelligence)},
}

@article{krizhevsky_one_2014,
	title = {One weird trick for parallelizing convolutional neural networks},
	url = {http://arxiv.org/abs/1404.5997},
	abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple {GPUs}. The method scales signiﬁcantly better than all alternatives when applied to modern convolutional neural networks.},
	journaltitle = {{arXiv}:1404.5997},
	author = {Krizhevsky, Alex},
	urldate = {2021-01-27},
	date = {2014-04-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1404.5997},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{khan_survey_2020,
	title = {A Survey of the Recent Architectures of Deep Convolutional Neural Networks},
	volume = {53},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-020-09825-6},
	doi = {10.1007/s10462-020-09825-6},
	abstract = {Deep Convolutional Neural Network ({CNN}) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of {CNN} include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep {CNN} is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in {CNNs}, and recently interesting deep {CNN} architectures have been reported. Several inspiring ideas to bring advancements in {CNNs} have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep {CNN} is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep {CNN} architectures and, consequently, classifies the recent innovations in {CNN} architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of {CNN} components, current challenges, and applications of {CNN} are also provided.},
	pages = {5455--5516},
	number = {8},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
	urldate = {2021-03-09},
	date = {2020-12},
	langid = {english},
}

@online{kanter_supercomputing_2019,
	title = {{SuperComputing} 19: {HPC} Meets Machine Learning},
	url = {https://www.realworldtech.com/sc19-hpc-meets-machine-learning},
	abstract = {For me, {SC}19 was about the fusion of machine learning and scientific computing. I learned about new technologies from Nvidia, Graphcore, and Cerebras Systems and spoke on a panel about the role of {MLPerf} in benchmarking {HPC} systems for machine learning and the many lessons learned.},
	author = {Kanter, David},
	urldate = {2021-04-24},
	date = {2019},
	langid = {american},
}

@book{jain_art_1991,
	location = {New York, {NY}},
	title = {The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling},
	isbn = {978-0-471-50336-1},
	publisher = {Wiley Computer Publishing},
	author = {Jain, Raj K.},
	date = {1991-04},
}

@online{intel_intel_2021,
	title = {Intel {oneAPI} Math Kernel Library},
	url = {https://www.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html},
	abstract = {Design scientific, engineering, or financial applications using enhanced math routines. Optimize code for current and future Intel® {CPUs} and {GPUs}.},
	author = {{Intel}},
	urldate = {2021-04-24},
	date = {2021},
	langid = {english},
}

@article{hornik_approximation_1991,
	title = {Approximation Capabilities of Multilayer Feedforward Networks},
	volume = {4},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/089360809190009T},
	doi = {https://doi.org/10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	pages = {251--257},
	number = {2},
	journaltitle = {Neural Networks},
	author = {Hornik, Kurt},
	date = {1991},
	keywords = {() approximation, Activation function, Input environment measure, Multilayer feedforward networks, Smooth approximation, Sobolev spaces, Uniform approximation, Universal approximation capabilities},
}

@book{haykin_neural_2009,
	location = {New York},
	title = {Neural Networks and Learning Machines},
	isbn = {978-0-13-147139-9},
	pagetotal = {906},
	publisher = {Prentice Hall},
	author = {Haykin, Simon S.},
	date = {2009},
	keywords = {Adaptive filters, Neural networks (Computer science)},
}

@book{gorman_understanding_2004,
	title = {Understanding the Linux Virtual Memory Manager},
	pagetotal = {177},
	publisher = {Prentice Hall Upper Saddle River},
	author = {Gorman, Mel},
	date = {2004},
}

@book{feitelson_workload_2015,
	title = {Workload Modeling for Computer Systems Performance Evaluation},
	isbn = {978-1-107-07823-9},
	url = {https://books.google.de/books?id=_vkGBwAAQBAJ},
	publisher = {Cambridge University Press},
	author = {Feitelson, Dror G.},
	date = {2015},
}

@online{embedded_microprocessor_benchmark_consortium_introducing_2021,
	title = {Introducing the {EEMBC} {MLMark} Benchmark},
	url = {https://www.eembc.org/mlmark},
	author = {{Embedded Microprocessor Benchmark Consortium}},
	urldate = {2021-04-25},
	date = {2021},
	langid = {english},
}

@article{chetlur_cudnn_2014,
	title = {{cuDNN}: Efficient Primitives for Deep Learning},
	url = {http://arxiv.org/abs/1410.0759},
	shorttitle = {{cuDNN}},
	abstract = {We present a library of efﬁcient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difﬁcult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difﬁcult over time. Similar issues have long been addressed in the {HPC} community by libraries such as the Basic Linear Algebra Subroutines ({BLAS}) [2]. However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to {BLAS}, with optimized routines for deep learning workloads. Our implementation contains routines for {GPUs}, although similarly to the {BLAS} library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating {cuDNN} into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
	journaltitle = {{arXiv}:1410.0759},
	author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
	urldate = {2021-04-24},
	date = {2014-12-17},
	langid = {english},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing},
}

@article{bengio_representation_2013,
	title = {Representation Learning: A Review and New Perspectives},
	volume = {35},
	doi = {10.1109/TPAMI.2013.50},
	pages = {1798--1828},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	date = {2013},
}

@online{facebook_pytorch_2021,
	title = {{PyTorch} Hub},
	url = {https://www.pytorch.org/hub},
	author = {{Facebook} and {PyTorch contributors}},
	urldate = {2021-03-08},
	date = {2021},
	langid = {english},
}

@online{facebook_pytorch_2021-1,
	title = {{PyTorch} Examples},
	url = {https://github.com/pytorch/examples},
	abstract = {A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.},
	author = {{Facebook} and {PyTorch contributors}},
	urldate = {2021-03-23},
	date = {2021-03-23},
}

@article{chen_mxnet_2015,
	title = {{MXNet}: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems},
	url = {http://arxiv.org/abs/1512.01274},
	shorttitle = {{MXNet}},
	abstract = {{MXNet} is a multi-language machine learning ({ML}) library to ease the development of {ML} algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. {MXNet} is computation and memory efﬁcient and runs on various heterogeneous systems, ranging from mobile devices to distributed {GPU} clusters.},
	journaltitle = {{arXiv}:1512.01274},
	author = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
	urldate = {2020-09-18},
	date = {2015-12-03},
	langid = {english},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Mathematical Software, Computer Science - Neural and Evolutionary Computing},
}

@article{dixit_spec_1991,
	title = {The {SPEC} Benchmarks},
	volume = {17},
	issn = {0167-8191},
	url = {https://doi.org/10.1016/S0167-8191(05)80033-X},
	doi = {10.1016/S0167-8191(05)80033-X},
	abstract = {This paper characterizes problems with popular benchmarks used to measure system performance and describes the history, structure, mission, future, and workings of an evolving standard in characterizing systems performance. Systems Performance Evaluation Cooperative ({SPEC}) represents an effort of major computer companies to create a yardstick to measure the performance of computer systems. This paper compares results of three architectures with traditional performance metrics and metrics developed by {SPEC}. It discusses strengths and weaknesses of {SPEC} and warns users about the dangers of single number performance characterization.},
	pages = {1195--1209},
	number = {10},
	journaltitle = {Parallel Computing},
	author = {Dixit, Kaivalya M.},
	date = {1991-12},
	keywords = {Benchmarks, {SPEC}, performance metric, performance results, system performance},
}

@incollection{heath_matrix_1986,
	location = {Philadelphia},
	title = {Matrix Computation on Distributed Memory Multiprocessors},
	isbn = {978-0-89871-209-4},
	booktitle = {Hypercube Multiprocessors},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Moler, Cleve},
	editor = {Heath, Michael T.},
	date = {1986},
	keywords = {Congresses, Hypercube networks (Computer networks), Multiprocessors},
}

@incollection{feurer_hyperparameter_2019,
	location = {Cham},
	title = {Hyperparameter Optimization},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_1},
	abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning ({AutoML}) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization ({HPO}). In this chapter, we give an overview of the most prominent approaches for {HPO}. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
	pages = {3--33},
	booktitle = {Automated Machine Learning: Methods, Systems, Challenges},
	publisher = {Springer International Publishing},
	author = {Feurer, Matthias and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	date = {2019},
	doi = {10.1007/978-3-030-05318-5_1},
}

@incollection{alippi_learning_2014,
	location = {Cham},
	title = {Learning in Nonstationary and Evolving Environments},
	isbn = {978-3-319-05278-6},
	url = {https://doi.org/10.1007/978-3-319-05278-6_9},
	abstract = {Previous chapters have developed methods and methodologies for solving specific aspects involving intelligent processing on embedded systems and presented techniques for their performance assessment. However, if we look carefully at those methods, we can observe that we have commonly assumed that the process generating the data acquired by our sensors was not changing with time (stationarity or time invariance assumption).},
	pages = {211--247},
	booktitle = {Intelligence for Embedded Systems: A Methodological Approach},
	publisher = {Springer International Publishing},
	author = {Alippi, Cesare},
	date = {2014},
}

@report{nvidia_nvidia_2017,
	title = {{NVIDIA} Tesla V100 {GPU} Architecture},
	url = {https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/volta-architecture-whitepaper.pdf},
	author = {{NVIDIA}},
	urldate = {2021-03-23},
	date = {2017-08},
}

@book{fortier_computer_2003,
	title = {Computer Systems Performance Evaluation and Prediction},
	isbn = {978-0-08-050260-1},
	url = {https://books.google.de/books?id=1vQmZY-7JLIC},
	publisher = {Elsevier Science},
	author = {Fortier, Paul and Michel, Howard},
	date = {2003},
}

@book{lilja_measuring_2005,
	title = {Measuring Computer Performance: A Practitioner's Guide},
	isbn = {978-0-521-64670-3},
	url = {https://books.google.de/books?id=R8RLniX5DNQC},
	publisher = {Cambridge University Press},
	author = {Lilja, David J.},
	date = {2005},
}

@article{ota_deep_2017,
	title = {Deep Learning for Mobile Multimedia: A Survey},
	volume = {13},
	issn = {1551-6857, 1551-6865},
	url = {https://dl.acm.org/doi/10.1145/3092831},
	doi = {10.1145/3092831},
	shorttitle = {Deep Learning for Mobile Multimedia},
	abstract = {Deep Learning ({DL}) has become a crucial technology for multimedia computing. It offers a powerful instrument to automatically produce high-level abstractions of complex multimedia data, which can be exploited in a number of applications, including object detection and recognition, speech-to- text, media retrieval, multimodal data analysis, and so on. The availability of affordable large-scale parallel processing architectures, and the sharing of effective open-source codes implementing the basic learning algorithms, caused a rapid diffusion of {DL} methodologies, bringing a number of new technologies and applications that outperform, in most cases, traditional machine learning technologies. In recent years, the possibility of implementing {DL} technologies on mobile devices has attracted significant attention. Thanks to this technology, portable devices may become smart objects capable of learning and acting. The path toward these exciting future scenarios, however, entangles a number of important research challenges. {DL} architectures and algorithms are hardly adapted to the storage and computation resources of a mobile device. Therefore, there is a need for new generations of mobile processors and chipsets, small footprint learning and inference algorithms, new models of collaborative and distributed processing, and a number of other fundamental building blocks. This survey reports the state of the art in this exciting research area, looking back to the evolution of neural networks, and arriving to the most recent results in terms of methodologies, technologies, and applications for mobile environments.},
	pages = {1--22},
	number = {3},
	journaltitle = {{ACM} Transactions on Multimedia Computing, Communications, and Applications},
	shortjournal = {{ACM} Trans. Multimedia Comput. Commun. Appl.},
	author = {Ota, Kaoru and Dao, Minh Son and Mezaris, Vasileios and Natale, Francesco G. B. De},
	urldate = {2021-03-15},
	date = {2017-08-10},
	langid = {english},
}

@book{zobel_writing_2014,
	location = {London},
	title = {Writing for Computer Science},
	isbn = {978-1-4471-6638-2 978-1-4471-6639-9},
	url = {http://link.springer.com/10.1007/978-1-4471-6639-9},
	publisher = {Springer London},
	author = {Zobel, Justin},
	urldate = {2021-03-09},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-1-4471-6639-9},
}

@online{levin_how_2014,
	title = {How (and How Not) to Write a Good Systems Paper},
	url = {https://www.usenix.org/conferences/author-resources/how-and-how-not-write-good-systems-paper},
	titleaddon = {{USENIX}},
	author = {Levin, Roy and Redell, David D.},
	urldate = {2021-02-24},
	date = {2014-11-11},
	langid = {english},
}

@online{google_google_2020,
	title = {Google Cloud {TPU}},
	url = {https://cloud.google.com/tpu},
	abstract = {Custom-built for machine learning workloads, Cloud {TPUs} accelerate training and inference at scale.},
	author = {{Google}},
	urldate = {2020-09-18},
	date = {2020},
	langid = {english},
}

@incollection{bose_power_2011,
	location = {Boston, {MA}},
	title = {Power Wall},
	isbn = {978-0-387-09766-4},
	url = {https://doi.org/10.1007/978-0-387-09766-4_499},
	pages = {1593--1608},
	booktitle = {Encyclopedia of Parallel Computing},
	publisher = {Springer {US}},
	author = {Bose, Pradip},
	editor = {Padua, David},
	date = {2011},
	doi = {10.1007/978-0-387-09766-4_499},
}

@collection{brodie_making_2018,
	title = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
	volume = {22},
	isbn = {978-1-947487-19-2},
	abstract = {At the {ACM} Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, {ACM} announced the launch of the {ACM} A.M. Turing Book Series, a sub-series of {ACM} Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the "Nobel Prize" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing."Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker," the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker—researcher, professor, {CTO}, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource ("The Economist," May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.},
	publisher = {Association for Computing Machinery and Morgan \& Claypool},
	editor = {Brodie, Michael L.},
	date = {2018},
}

@article{park_hetpipe_nodate,
	title = {{HetPipe}: Enabling Large {DNN} Training on (Whimpy) Heterogeneous {GPU} Clusters through Integration of Pipelined Model Parallelism and Data Parallelism},
	abstract = {Deep Neural Network ({DNN}) models have continuously been growing in size in order to improve the accuracy and quality of the models. Moreover, for training of large {DNN} models, the use of heterogeneous {GPUs} is inevitable due to the short release cycle of new {GPU} architectures. In this paper, we investigate how to enable training of large {DNN} models on a heterogeneous {GPU} cluster that possibly includes whimpy {GPUs} that, as a standalone, could not be used for training. We present a {DNN} training system, {HetPipe} (Heterogeneous Pipeline), that integrates pipelined model parallelism ({PMP}) with data parallelism ({DP}). In {HetPipe}, a group of multiple {GPUs}, called a virtual worker, processes minibatches in a pipelined manner, and multiple such virtual workers employ data parallelism for higher performance. We also propose a novel parameter synchronization model, which we refer to as Wave Synchronous Parallel ({WSP}) to accommodate both {PMP} and {DP} for virtual workers, and provide convergence proof of {WSP}. Our experimental results on a given heterogeneous setting show that with {HetPipe}, {DNN} models converge up to 49\% faster compared to the state-of-the-art {DP} technique.},
	pages = {16},
	author = {Park, Jay H and Yun, Gyeongchan and Yi, Chang M and Nguyen, Nguyen T and Lee, Seungmin and Choi, Jaesik and Noh, Sam H and Choi, Young-ri},
	langid = {english},
}

@article{nakandala_taming_2020,
	title = {Taming Model Serving Complexity, Performance and Cost: A Compilation to Tensor Computations Approach},
	abstract = {Machine Learning ({ML}) adoption in the enterprise requires simpler and more efﬁcient software infrastructure—the bespoke solutions typical in large web companies are simply untenable. Model scoring, the process of obtaining prediction from a trained model over new data, is a primary contributor to infrastructure complexity and cost, as models are trained once but used many times.},
	pages = {16},
	author = {Nakandala, Supun and Saur, Karla and Yu, Gyeong-In and Karanasos, Konstantinos and Curino, Carlo and Weimer, Markus and Interlandi, Matteo},
	date = {2020},
	langid = {english},
}

@report{crankshaw_design_2019,
	title = {The Design and Implementation of Low-Latency Prediction Serving Systems},
	pages = {102},
	type = {Dissertation},
	author = {Crankshaw, Daniel},
	date = {2019},
	langid = {english},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	pages = {8024--8035},
	booktitle = {Advances in Neural Information Processing Systems 32},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and {DeVito}, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. and Fox, E. and Garnett, R.},
	date = {2019},
}

@online{kaiser_too_2020,
	title = {Too big to deploy: How {GPT}-2 is breaking production},
	url = {https://towardsdatascience.com/too-big-to-deploy-how-gpt-2-is-breaking-production-63ab29f0897c},
	shorttitle = {Too big to deploy},
	abstract = {A look at the bottleneck around deploying massive models to production},
	author = {Kaiser, Caleb},
	urldate = {2020-09-25},
	date = {2020-02-06},
	langid = {english},
}

@incollection{nambiar_end--end_2020,
	title = {End-to-End Benchmarking of Deep Learning Platforms},
	isbn = {978-3-030-55024-0},
	url = {http://link.springer.com/10.1007/978-3-030-55024-0_8},
	abstract = {With their capability to recognise complex patterns in data, deep learning models are rapidly becoming the most prominent set of tools for a broad range of data science tasks from image classiﬁcation to natural language processing. This trend is supplemented by the availability of deep learning software platforms and modern hardware environments. We propose a declarative benchmarking framework to evaluate the performance of diﬀerent software and hardware systems. We further use our framework to analyse the performance of three diﬀerent software frameworks on diﬀerent hardware setups for a representative set of deep learning workloads and corresponding neural network architectures3.},
	pages = {116--132},
	booktitle = {Performance Evaluation and Benchmarking for the Era of Cloud(s)},
	author = {Deuschle, Vincent and Alexandrov, Alexander and Januschowski, Tim and Markl, Volker},
	editor = {Nambiar, Raghunath and Poess, Meikel},
	urldate = {2020-08-25},
	date = {2020},
	langid = {english},
	keywords = {Rysia},
}

@article{publio_ml-schema_2018,
	title = {{ML}-Schema: Exposing the Semantics of Machine Learning with Schemas and Ontologies},
	url = {http://arxiv.org/abs/1807.05351},
	shorttitle = {{ML}-Schema},
	abstract = {The {ML}-Schema, proposed by the W3C Machine Learning Schema Community Group, is a top-level ontology that provides a set of classes, properties, and restrictions for representing and interchanging information on machine learning algorithms, datasets, and experiments. It can be easily extended and specialized and it is also mapped to other more domain-speciﬁc ontologies developed in the area of machine learning and data mining. In this paper we overview existing state-of-the-art machine learning interchange formats and present the ﬁrst release of {ML}-Schema, a canonical format resulted of more than seven years of experience among different research institutions. We argue that exposing semantics of machine learning algorithms, models, and experiments through a canonical format may pave the way to better interpretability and to realistically achieve the full interoperability of experiments regardless of platform or adopted workﬂow solution.},
	journaltitle = {{arXiv}:1807.05351},
	author = {Publio, Gustavo Correa and Esteves, Diego and Ławrynowicz, Agnieszka and Panov, Panče and Soldatova, Larisa and Soru, Tommaso and Vanschoren, Joaquin and Zafar, Hamid},
	urldate = {2020-08-31},
	date = {2018-07-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.05351},
	keywords = {Computer Science - Databases, Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bacher_mergeshuffle_2015,
	title = {{MergeShuffle}: A Very Fast, Parallel Random Permutation Algorithm},
	url = {http://arxiv.org/abs/1508.03167},
	shorttitle = {{MergeShuffle}},
	abstract = {This article introduces an algorithm, {MERGESHUFFLE}, which is an extremely efﬁcient algorithm to generate random permutations (or to randomly permute an existing array). It is easy to implement, runs in nlog2n + O(1) time, is in-place, uses nlog2n + Θ(n) random bits, and can be parallelized accross any number of processes, in a shared-memory {PRAM} model. Finally, our preliminary simulations using {OpenMP}1 suggest it is more efﬁcient than the Rao-Sandelius algorithm, one of the fastest existing random permutation algorithms.},
	journaltitle = {{arXiv}:1508.03167},
	author = {Bacher, Axel and Bodini, Olivier and Hollender, Alexandros and Lumbroso, Jérémie},
	urldate = {2020-09-04},
	date = {2015-08-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1508.03167},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics},
}

@article{arnold_factsheets_2019,
	title = {{FactSheets}: Increasing Trust in {AI} Services through Supplier's Declarations of Conformity},
	url = {http://arxiv.org/abs/1808.07261},
	shorttitle = {{FactSheets}},
	abstract = {The accuracy and reliability of machine learning algorithms are an important concern for suppliers of artiﬁcial intelligence ({AI}) services, but considerations beyond accuracy, such as safety, security, and provenance, are also critical elements to engender consumers’ trust in a service. In this paper, we propose a supplier’s declaration of conformity ({SDoC}) for {AI} services to help increase trust in {AI} services. An {SDoC} is a transparent, standardized, but often not legally required, document used in many industries and sectors to describe the lineage of a product along with the safety and performance testing it has undergone. We envision an {SDoC} for {AI} services to contain purpose, performance, safety, security, and provenance information to be completed and voluntarily released by {AI} service providers for examination by consumers. Importantly, it conveys productlevel rather than component-level functional testing. We suggest a set of declaration items tailored to {AI} and provide examples for two ﬁctitious {AI} services.},
	journaltitle = {{arXiv}:1808.07261},
	author = {Arnold, Matthew and Bellamy, Rachel K. E. and Hind, Michael and Houde, Stephanie and Mehta, Sameep and Mojsilovic, Aleksandra and Nair, Ravi and Ramamurthy, Karthikeyan Natesan and Reimer, Darrell and Olteanu, Alexandra and Piorkowski, David and Tsay, Jason and Varshney, Kush R.},
	urldate = {2020-08-31},
	date = {2019-02-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.07261},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@article{wang_exploiting_2020,
	title = {Exploiting Parallelism Opportunities with Deep Learning Frameworks},
	url = {http://arxiv.org/abs/1908.04705},
	abstract = {State-of-the-art machine learning frameworks support a wide variety of design features to enable a flexible machine learning programming interface and to ease the programmability burden on machine learning developers. Identifying and using a performance-optimal setting in feature-rich frameworks, however, involves a non-trivial amount of performance profiling efforts and often relies on domain-specific knowledge. This paper takes a deep dive into analyzing the performance impact of key design features in a machine learning framework and quantifies the role of parallelism. The observations and insights distill into a simple set of guidelines that one can use to achieve much higher training and inference speedup. Across a diverse set of real-world deep learning models, the evaluation results show that the proposed performance tuning guidelines outperform the Intel and {TensorFlow} recommended settings by 1.29x and 1.34x, respectively.},
	journaltitle = {{arXiv}:1908.04705},
	author = {Wang, Yu Emma and Wu, Carole-Jean and Wang, Xiaodong and Hazelwood, Kim and Brooks, David},
	urldate = {2020-09-09},
	date = {2020-06-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1908.04705},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Performance, Statistics - Machine Learning},
}

@article{traub_agora_2020,
	title = {Agora: A Unified Asset Ecosystem Going Beyond Marketplaces and Cloud Services},
	url = {http://arxiv.org/abs/1909.03026},
	shorttitle = {Agora},
	abstract = {Data, algorithms, and compute/storage infrastructure are key assets that drive data science and artiﬁcial intelligence applications. As providing all these assets requires a huge investment, data science and artiﬁcial intelligence technologies are currently dominated by a small number of providers who can afford these investments. This leads to lock-in effects and hinders features that require a ﬂexible exchange of assets among users.},
	pages = {17},
	journaltitle = {{arXiv}:1909.03026},
	author = {Traub, Jonas and Kaoudi, Zoi and Kaustubh, Beedkar and Redyuk, Sergey and Rosenfeld, Viktor and Quiané-Ruiz, Jorge-Arnulfo and Markl, Volker},
	urldate = {2020-08-31},
	date = {2020-07-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.03026},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Electrical Engineering and Systems Science - Systems and Control},
}

@online{baidu_research_deepbench_2017,
	title = {{DeepBench}},
	rights = {Apache-2.0 License},
	url = {https://github.com/baidu-research/DeepBench},
	abstract = {Benchmarking Deep Learning operations on different hardware},
	author = {{Baidu Research}},
	urldate = {2020-09-03},
	date = {2017},
}

@online{nvidia_nvidia_2019,
	title = {{NVIDIA} Data Loading Library ({DALI})},
	url = {https://developer.nvidia.com/DALI},
	author = {{NVIDIA}},
	urldate = {2020-09-11},
	date = {2019-03-05},
}

@article{bacher_generating_2016,
	title = {Generating random permutations by coin-tossing: classical algorithms, new analysis and modern implementation},
	abstract = {Several simple, classical, little-known algorithms in the statistical literature for generating random permutations by coin-tossing are examined, analyzed and implemented. These algorithms are either asymptotically optimal or close to being so in terms of the expected number of times the random bits are generated. In addition to asymptotic approximations to the expected complexity, we also clarify the corresponding variances, as well as the asymptotic distributions. A brief comparative discussion with numerical computations in a multicore system is also given.},
	pages = {47},
	author = {Bacher, Axel and Bodini, Olivier and Hwang, Hsien-Kuei and Tsai, Tsung-Hsi},
	date = {2016-03-10},
	langid = {english},
}

@report{mcdowell_storage-optimized_2018,
	title = {Storage-Optimized Machine Learning},
	abstract = {Organizations run at the speed of their data. While Artificial Intelligence ({AI}) and Machine Learning ({ML}) have a continuing history of solving traditional problems in pattern recognition, {AI} and {ML} techniques are rapidly finding their place in business analytics, where the patterns being determined might be less obvious. The efficiency of these learning systems can define an organization’s competitive advantage.},
	pages = {11},
	institution = {Moor Insights \& Strategy},
	author = {{McDowell}, Steve},
	date = {2018},
	langid = {english},
}

@book{bovet_understanding_2005,
	title = {Understanding the Linux Kernel},
	isbn = {978-0-596-00565-8},
	url = {https://doc.lagout.org/operating%20system%20/linux/Understanding%20Linux%20Kernel.pdf},
	publisher = {O'Reilly Media, Inc.},
	author = {Bovet, Daniel P. and Cesati, Marco},
	urldate = {2020-09-02},
	date = {2005},
}

@incollection{hallinan_dataset_2020,
	title = {The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards},
	isbn = {978-1-5099-3274-0 978-1-5099-3276-4 978-1-5099-3275-7 978-1-5099-3277-1},
	url = {http://www.bloomsburycollections.com/book/data-protection-and-privacy-data-protection-and-democracy},
	abstract = {Artificial intelligence ({AI}) systems built on incomplete or biased data will often exhibit problematic outcomes. Current methods of data analysis, particularly before model development, are costly and not standardized. The Dataset Nutrition Label1 (the Label) is a diagnostic framework that lowers the barrier to standardized data analysis by providing a distilled yet comprehensive overview of dataset “ingredients” before {AI} model development. Building a Label that can be applied across domains and data types requires that the framework itself be flexible and adaptable; as such, the Label is comprised of diverse qualitative and quantitative modules generated through multiple statistical and probabilistic modelling backends, but displayed in a standardized format. To demonstrate and advance this concept, we generated and published an open source prototype2 with seven sample modules on the {ProPublica} Dollars for Docs dataset. The benefits of the Label are manyfold. For data specialists, the Label will drive more robust data analysis practices, provide an efficient way to select the best dataset for their purposes, and increase the overall quality of {AI} models as a result of more robust training datasets and the ability to check for issues at the time of model development. For those building and publishing datasets, the Label creates an expectation of explanation, which will drive better data collection practices. We also explore the limitations of the Label, including the challenges of generalizing across diverse datasets, and the risk of using “ground truth” data as a comparison dataset. We discuss ways to move forward given the limitations identified. Lastly, we lay out future directions for the Dataset Nutrition Label project, including research and public policy agendas to further advance consideration of the concept.},
	booktitle = {Data Protection and Privacy: Data Protection and Democracy},
	publisher = {Hart Publishing},
	author = {Holland, Sarah and Hosny, Ahmed and Newman, Sarah and Joseph, Joshua and Chmielinski, Kasia},
	editor = {Hallinan, Dara and Leenes, Ronald and Gutwirth, Serge and De Hert, Paul},
	urldate = {2020-08-31},
	date = {2020},
	langid = {english},
	doi = {10.5040/9781509932771},
}

@article{bender_data_2018,
	title = {Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science},
	volume = {6},
	issn = {2307-387X},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00041},
	doi = {10.1162/tacl_a_00041},
	shorttitle = {Data Statements for Natural Language Processing},
	abstract = {In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the ﬁeld can begin to address critical scientiﬁc and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.},
	pages = {587--604},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {{TACL}},
	author = {Bender, Emily M. and Friedman, Batya},
	urldate = {2020-08-31},
	date = {2018-12},
	langid = {english},
}

@online{zhou_understanding_2019,
	title = {Understanding Access Patterns in Machine Learning},
	url = {https://blog.westerndigital.com/understanding-access-patterns-machine-learning/},
	abstract = {For organizations to build optimized data infrastructure for machine learning, they need to understand how their data is served, accessed and processed.},
	titleaddon = {Western Digital Corporate Blog},
	author = {Zhou, Linda},
	urldate = {2020-08-27},
	date = {2019-03-12},
	langid = {american},
}

@article{williams_roofline_2009,
	title = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
	volume = {52},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/1498765.1498785},
	doi = {10.1145/1498765.1498785},
	shorttitle = {Roofline},
	pages = {65--76},
	number = {4},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	urldate = {2020-08-25},
	date = {2009-04},
	langid = {english},
}

@article{chakroun_guidelines_2019,
	title = {Guidelines for Enhancing Data Locality in Selected Machine Learning Algorithms},
	volume = {23},
	issn = {1088467X, 15714128},
	url = {http://arxiv.org/abs/2001.03000},
	doi = {10.3233/IDA-184287},
	abstract = {To deal with the complexity of the new bigger and more complex generation of data, machine learning ({ML}) techniques are probably the ﬁrst and foremost used. For {ML} algorithms to produce results in a reasonable amount of time, they need to be implemented eﬃciently. In this paper, we analyze one of the means to increase the performances of machine learning algorithms which is exploiting data locality. Data locality and access patterns are often at the heart of performance issues in computing systems due to the use of certain hardware techniques to improve performance. Altering the access patterns to increase locality can dramatically increase performance of a given algorithm. Besides, repeated data access can be seen as redundancy in data movement. Similarly, there can also be redundancy in the repetition of calculations. This work also identiﬁes some of the opportunities for avoiding these redundancies by directly reusing computation results. We start by motivating why and how a more eﬃcient implementation can be achieved by exploiting reuse in the memory hierarchy of modern instruction set processors. Next we document the possibilities of such reuse in some selected machine learning algorithms.},
	pages = {1003--1020},
	number = {5},
	journaltitle = {Intelligent Data Analysis},
	shortjournal = {{IDA}},
	author = {Chakroun, Imen and Aa, Tom Vander and Ashby, Thomas J.},
	urldate = {2020-08-27},
	date = {2019-10-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2001.03000},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mckinley_improving_1996,
	title = {Improving Data Locality With Loop Transformations},
	volume = {18},
	issn = {0164-0925, 1558-4593},
	url = {http://dl.acm.org/doi/10.1145/233561.233564},
	doi = {10.1145/233561.233564},
	pages = {424--453},
	number = {4},
	journaltitle = {{ACM} Transactions on Programming Languages and Systems ({TOPLAS})},
	shortjournal = {{ACM} Trans. Program. Lang. Syst.},
	author = {{McKinley}, Kathryn S. and Carr, Steve and Tseng, Chau-Wen},
	urldate = {2020-08-27},
	date = {1996-07},
	langid = {english},
}

@book{bailis_readings_2015,
	edition = {5th},
	title = {Readings in Database Systems},
	isbn = {978-0-262-69314-1},
	series = {{MIT} Press},
	author = {Bailis, Peter and Hellerstein, Joseph M. and Stonebraker, Michael},
	date = {2015},
}

@article{tao_benchip_2018,
	title = {{BenchIP}: Benchmarking Intelligence Processors},
	volume = {33},
	issn = {1000-9000, 1860-4749},
	url = {http://link.springer.com/10.1007/s11390-018-1805-8},
	doi = {10.1007/s11390-018-1805-8},
	shorttitle = {{BenchIP}},
	abstract = {The increasing attention on deep learning has tremendously spurred the design of intelligence processing hardware. The variety of emerging intelligence processors requires standard benchmarks for fair comparison and system optimization (in both software and hardware). However, existing benchmarks are unsuitable for benchmarking intelligence processors due to their non-diversity and nonrepresentativeness. Also, the lack of a standard benchmarking methodology further exacerbates this problem. In this paper, we propose {BenchIP}, a benchmark suite and benchmarking methodology for intelligence processors. The benchmark suite in {BenchIP} consists of two sets of benchmarks: microbenchmarks and macrobenchmarks. The microbenchmarks consist of single-layer networks. They are mainly designed for bottleneck analysis and system optimization. The macrobenchmarks contain state-of-the-art industrial networks, so as to oﬀer a realistic comparison of diﬀerent platforms. We also propose a standard benchmarking methodology built upon an industrial software stack and evaluation metrics that comprehensively reﬂect various characteristics of the evaluated intelligence processors. {BenchIP} is utilized for evaluating various hardware platforms, including {CPUs}, {GPUs}, and accelerators. {BenchIP} will be open-sourced soon.},
	pages = {1--23},
	number = {1},
	journaltitle = {Journal of Computer Science and Technology},
	shortjournal = {J. Comput. Sci. Technol.},
	author = {Tao, Jin-Hua and Du, Zi-Dong and Guo, Qi and Lan, Hui-Ying and Zhang, Lei and Zhou, Sheng-Yuan and Xu, Ling-Jie and Liu, Cong and Liu, Hai-Feng and Tang, Shan and Rush, Allen and Chen, Willian and Liu, Shao-Li and Chen, Yun-Ji and Chen, Tian-Shi},
	urldate = {2020-08-25},
	date = {2018-01},
	langid = {english},
}
